{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pyts.image import RecurrencePlot, MarkovTransitionField, GramianAngularField\n",
        "from statsmodels.datasets import co2\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "from statsmodels.tsa.seasonal import STL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjD_WSAS-0fR"
      },
      "outputs": [],
      "source": [
        "import crunch\n",
        "\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKHXgvjN-0fS"
      },
      "outputs": [],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0oRCTnOb-0fS",
        "outputId": "2d0663ba-76b2-4937-d7fc-e6c314784242"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lengths = X_train.groupby('id').size()\n",
        "print('时间序列长度描述性统计\\n', lengths.describe())\n",
        "print('时间序列长度分布')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(lengths, bins=30, kde=True)\n",
        "plt.xlabel('Length of Each Time Series')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Time Series Lengths per ID')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 统计每个 id 在 period==0 和 period==1 下的长度\n",
        "period_lengths = X_train.groupby(['id', 'period']).size().unstack(fill_value=0)\n",
        "period0_lengths = period_lengths[0]\n",
        "period1_lengths = period_lengths[1]\n",
        "\n",
        "print('Period 0 长度分布描述性统计:\\n', period0_lengths.describe())\n",
        "print('Period 1 长度分布描述性统计:\\n', period1_lengths.describe())\n",
        "\n",
        "# 绘制两个子图\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# period==0\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(period0_lengths, bins=30, kde=True, color='skyblue')\n",
        "plt.xlabel('Length of Period 0')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Period 0 Lengths per ID')\n",
        "plt.grid(True)\n",
        "\n",
        "# period==1\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(period1_lengths, bins=30, kde=True, color='salmon')\n",
        "plt.xlabel('Length of Period 1')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Period 1 Lengths per ID')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 统计每个 id 在 period==0 和 period==1 下的长度\n",
        "period_lengths = X_train.groupby(['id', 'period']).size().unstack(fill_value=0)\n",
        "period0_lengths = period_lengths[0]\n",
        "period1_lengths = period_lengths[1]\n",
        "total_lengths = period0_lengths + period1_lengths\n",
        "\n",
        "# 计算比例\n",
        "period1_ratio = period1_lengths / total_lengths\n",
        "\n",
        "print('Period1 占比描述性统计:\\n', period1_ratio.describe())\n",
        "\n",
        "# 绘制比例分布图\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(period1_ratio, bins=30, kde=True, color='purple')\n",
        "plt.xlabel('Proportion of Period 1 Length')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Period 1 Length Proportion per ID')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.loc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_moving_average_decomposition(X_train, y_train, id_=0, window=100):\n",
        "    \"\"\"\n",
        "    使用滑动平均进行趋势与残差分解，并绘图（风格与 STL 相同）\n",
        "\n",
        "    参数:\n",
        "    - X_train: 包含 'value' 和 'period' 的 DataFrame，MultiIndex\n",
        "    - y_train: 标签\n",
        "    - id_: 指定的样本 id\n",
        "    - window: 滑动平均窗口大小\n",
        "\n",
        "    返回:\n",
        "    - trend: 平滑后的趋势\n",
        "    - residual: 残差\n",
        "    \"\"\"\n",
        "    X_df = X_train.loc[id_]\n",
        "    value_series = X_df['value'].reset_index(drop=True)\n",
        "    period_series = X_df['period'].reset_index(drop=True).values\n",
        "\n",
        "    trend = value_series.rolling(window=window, center=True, min_periods=1).mean()\n",
        "    residual = value_series - trend\n",
        "\n",
        "    # 绘图，3 行 1 列，与 STL 样式一致\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(6, 4), sharex=True)\n",
        "    fig.suptitle(f'Moving Average Decomposition: ID {id_} Break={y_train.loc[id_]}', fontsize=16)\n",
        "\n",
        "    transition_points = np.where(np.diff(period_series) != 0)[0]\n",
        "\n",
        "    # 原始序列\n",
        "    axes[0].plot(value_series, label='Observed', color='black')\n",
        "    for point in transition_points:\n",
        "        axes[0].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # 趋势\n",
        "    axes[1].plot(trend, label=f'Trend (MA window={window})', color='blue')\n",
        "    for point in transition_points:\n",
        "        axes[1].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[1].legend()\n",
        "\n",
        "    # 残差\n",
        "    axes[2].plot(residual, label='Residual', color='orange')\n",
        "    for point in transition_points:\n",
        "        axes[2].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return trend, residual\n",
        "\n",
        "for i in range(15):\n",
        "    trend, residual = plot_moving_average_decomposition(X_train, y_train, id_=i, window=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_stl_decomposition(X_train, y_train, id_=0, period=100, seasonal=15, robust=True):\n",
        "    \"\"\"\n",
        "    对输入的 DataFrame (单一 id 的时间序列) 进行 STL 分解并绘图\n",
        "\n",
        "    参数:\n",
        "    - X_df: DataFrame，含有 'value' 列，index 是 time\n",
        "    - period: 分解周期，建议设为与趋势相关的窗口大小\n",
        "    - seasonal: 季节性平滑窗口（奇数）\n",
        "    - robust: 是否使用鲁棒分解以减少异常值影响\n",
        "\n",
        "    返回:\n",
        "    - STL 分解结果对象\n",
        "    \"\"\"\n",
        "    X_df = X_train.loc[id_]\n",
        "    value_series = X_df['value'].reset_index(drop=True)\n",
        "    period_series = X_df['period'].reset_index(drop=True).values\n",
        "\n",
        "    stl = STL(value_series, period=period, seasonal=seasonal, robust=robust)\n",
        "    result = stl.fit()\n",
        "\n",
        "    fig = result.plot()\n",
        "    fig.suptitle(f'STL: ID {id_} Break={y_train.loc[id_]}', fontsize=16)\n",
        "\n",
        "    transition_points = np.where(np.diff(period_series) != 0)[0]\n",
        "\n",
        "    for ax in fig.axes:\n",
        "        for point in transition_points:\n",
        "            ax.axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return result\n",
        "\n",
        "for i in range(15):\n",
        "    result = plot_stl_decomposition(X_train, y_train, id_=i, period=100, seasonal=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Feb 20 19:24:58 2019\n",
        "\n",
        "@author: Vinícius Rezende Carvalho\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def  VMD(f, alpha, tau, K, DC, init, tol):\n",
        "    \"\"\"\n",
        "    u,u_hat,omega = VMD(f, alpha, tau, K, DC, init, tol)\n",
        "    Variational mode decomposition\n",
        "    Python implementation by Vinícius Rezende Carvalho - vrcarva@gmail.com\n",
        "    code based on Dominique Zosso's MATLAB code, available at:\n",
        "    https://www.mathworks.com/matlabcentral/fileexchange/44765-variational-mode-decomposition\n",
        "    Original paper:\n",
        "    Dragomiretskiy, K. and Zosso, D. (2014) ‘Variational Mode Decomposition’, \n",
        "    IEEE Transactions on Signal Processing, 62(3), pp. 531–544. doi: 10.1109/TSP.2013.2288675.\n",
        "    \n",
        "    \n",
        "    Input and Parameters:\n",
        "    ---------------------\n",
        "    f       - the time domain signal (1D) to be decomposed\n",
        "    alpha   - the balancing parameter of the data-fidelity constraint\n",
        "    tau     - time-step of the dual ascent ( pick 0 for noise-slack )\n",
        "    K       - the number of modes to be recovered\n",
        "    DC      - true if the first mode is put and kept at DC (0-freq)\n",
        "    init    - 0 = all omegas start at 0\n",
        "                       1 = all omegas start uniformly distributed\n",
        "                      2 = all omegas initialized randomly\n",
        "    tol     - tolerance of convergence criterion; typically around 1e-6\n",
        "\n",
        "    Output:\n",
        "    -------\n",
        "    u       - the collection of decomposed modes\n",
        "    u_hat   - spectra of the modes\n",
        "    omega   - estimated mode center-frequencies\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(f)%2:\n",
        "       f = f[:-1]\n",
        "\n",
        "    # Period and sampling frequency of input signal\n",
        "    fs = 1./len(f)\n",
        "    \n",
        "    ltemp = len(f)//2 \n",
        "    fMirr =  np.append(np.flip(f[:ltemp],axis = 0),f)  \n",
        "    fMirr = np.append(fMirr,np.flip(f[-ltemp:],axis = 0))\n",
        "\n",
        "    # Time Domain 0 to T (of mirrored signal)\n",
        "    T = len(fMirr)\n",
        "    t = np.arange(1,T+1)/T  \n",
        "    \n",
        "    # Spectral Domain discretization\n",
        "    freqs = t-0.5-(1/T)\n",
        "\n",
        "    # Maximum number of iterations (if not converged yet, then it won't anyway)\n",
        "    Niter = 500\n",
        "    # For future generalizations: individual alpha for each mode\n",
        "    Alpha = alpha*np.ones(K)\n",
        "    \n",
        "    # Construct and center f_hat\n",
        "    f_hat = np.fft.fftshift((np.fft.fft(fMirr)))\n",
        "    f_hat_plus = np.copy(f_hat) #copy f_hat\n",
        "    f_hat_plus[:T//2] = 0\n",
        "\n",
        "    # Initialization of omega_k\n",
        "    omega_plus = np.zeros([Niter, K])\n",
        "\n",
        "\n",
        "    if init == 1:\n",
        "        for i in range(K):\n",
        "            omega_plus[0,i] = (0.5/K)*(i)\n",
        "    elif init == 2:\n",
        "        omega_plus[0,:] = np.sort(np.exp(np.log(fs) + (np.log(0.5)-np.log(fs))*np.random.rand(1,K)))\n",
        "    else:\n",
        "        omega_plus[0,:] = 0\n",
        "            \n",
        "    # if DC mode imposed, set its omega to 0\n",
        "    if DC:\n",
        "        omega_plus[0,0] = 0\n",
        "    \n",
        "    # start with empty dual variables\n",
        "    lambda_hat = np.zeros([Niter, len(freqs)], dtype = complex)\n",
        "    \n",
        "    # other inits\n",
        "    uDiff = tol+np.spacing(1) # update step\n",
        "    n = 0 # loop counter\n",
        "    sum_uk = 0 # accumulator\n",
        "    # matrix keeping track of every iterant // could be discarded for mem\n",
        "    u_hat_plus = np.zeros([Niter, len(freqs), K],dtype=complex)    \n",
        "\n",
        "    #*** Main loop for iterative updates***\n",
        "\n",
        "    while ( uDiff > tol and  n < Niter-1 ): # not converged and below iterations limit\n",
        "        # update first mode accumulator\n",
        "        k = 0\n",
        "        sum_uk = u_hat_plus[n,:,K-1] + sum_uk - u_hat_plus[n,:,0]\n",
        "        \n",
        "        # update spectrum of first mode through Wiener filter of residuals\n",
        "        u_hat_plus[n+1,:,k] = (f_hat_plus - sum_uk - lambda_hat[n,:]/2)/(1.+Alpha[k]*(freqs - omega_plus[n,k])**2)\n",
        "        \n",
        "        # update first omega if not held at 0\n",
        "        if not(DC):\n",
        "            omega_plus[n+1,k] = np.dot(freqs[T//2:T],(abs(u_hat_plus[n+1, T//2:T, k])**2))/np.sum(abs(u_hat_plus[n+1,T//2:T,k])**2)\n",
        "\n",
        "        # update of any other mode\n",
        "        for k in np.arange(1,K):\n",
        "            #accumulator\n",
        "            sum_uk = u_hat_plus[n+1,:,k-1] + sum_uk - u_hat_plus[n,:,k]\n",
        "            # mode spectrum\n",
        "            u_hat_plus[n+1,:,k] = (f_hat_plus - sum_uk - lambda_hat[n,:]/2)/(1+Alpha[k]*(freqs - omega_plus[n,k])**2)\n",
        "            # center frequencies\n",
        "            omega_plus[n+1,k] = np.dot(freqs[T//2:T],(abs(u_hat_plus[n+1, T//2:T, k])**2))/np.sum(abs(u_hat_plus[n+1,T//2:T,k])**2)\n",
        "            \n",
        "        # Dual ascent\n",
        "        lambda_hat[n+1,:] = lambda_hat[n,:] + tau*(np.sum(u_hat_plus[n+1,:,:],axis = 1) - f_hat_plus)\n",
        "        \n",
        "        # loop counter\n",
        "        n = n+1\n",
        "        \n",
        "        # converged yet?\n",
        "        uDiff = np.spacing(1)\n",
        "        for i in range(K):\n",
        "            uDiff = uDiff + (1/T)*np.dot((u_hat_plus[n,:,i]-u_hat_plus[n-1,:,i]),np.conj((u_hat_plus[n,:,i]-u_hat_plus[n-1,:,i])))\n",
        "\n",
        "        uDiff = np.abs(uDiff)        \n",
        "            \n",
        "    #Postprocessing and cleanup\n",
        "    \n",
        "    #discard empty space if converged early\n",
        "    Niter = np.min([Niter,n])\n",
        "    omega = omega_plus[:Niter,:]\n",
        "    \n",
        "    idxs = np.flip(np.arange(1,T//2+1),axis = 0)\n",
        "    # Signal reconstruction\n",
        "    u_hat = np.zeros([T, K],dtype = complex)\n",
        "    u_hat[T//2:T,:] = u_hat_plus[Niter-1,T//2:T,:]\n",
        "    u_hat[idxs,:] = np.conj(u_hat_plus[Niter-1,T//2:T,:])\n",
        "    u_hat[0,:] = np.conj(u_hat[-1,:])    \n",
        "    \n",
        "    u = np.zeros([K,len(t)])\n",
        "    for k in range(K):\n",
        "        u[k,:] = np.real(np.fft.ifft(np.fft.ifftshift(u_hat[:,k])))\n",
        "        \n",
        "    # remove mirror part\n",
        "    u = u[:,T//4:3*T//4]\n",
        "\n",
        "    # recompute spectrum\n",
        "    u_hat = np.zeros([u.shape[1],K],dtype = complex)\n",
        "    for k in range(K):\n",
        "        u_hat[:,k]=np.fft.fftshift(np.fft.fft(u[k,:]))\n",
        "\n",
        "    return u, u_hat, omega"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_vmd_decomposition(X_train, y_train, id_=0, alpha=2000, tau=0, K=5, DC=0, init=1, tol=1e-7):\n",
        "    \"\"\"\n",
        "    对输入的 DataFrame (单一 id 的时间序列) 进行 VMD 分解并绘图\n",
        "\n",
        "    参数:\n",
        "    - X_train: DataFrame，包含 'value' 和 'period'，multiindex\n",
        "    - y_train: DataFrame，标签\n",
        "    - id_: 指定分解的样本 id\n",
        "    - alpha, tau, K, DC, init, tol: VMD 参数\n",
        "\n",
        "    返回:\n",
        "    - u: 分解后的模态\n",
        "    - omega: 每个模态的中心频率\n",
        "    \"\"\"\n",
        "\n",
        "    X_df = X_train.loc[id_]\n",
        "    value_series = X_df['value'].reset_index(drop=True).values\n",
        "    period_series = X_df['period'].reset_index(drop=True).values\n",
        "\n",
        "    u, u_hat, omega = VMD(value_series, alpha=alpha, tau=tau, K=K, DC=DC, init=init, tol=tol)\n",
        "\n",
        "    fig, axes = plt.subplots(K + 1, 1, figsize=(6, 1.5 * (K + 1)), sharex=True)\n",
        "    fig.suptitle(f'VMD: ID {id_} Break={y_train.loc[id_]}', fontsize=16)\n",
        "\n",
        "    transition_points = np.where(np.diff(period_series) != 0)[0]\n",
        "\n",
        "    # 原始信号\n",
        "    axes[0].plot(value_series, label='Original Signal', color='black')\n",
        "    for point in transition_points:\n",
        "        axes[0].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # VMD 分解结果\n",
        "    for k in range(K):\n",
        "        axes[k + 1].plot(u[k, :], label=f'Mode {k + 1}')\n",
        "        for point in transition_points:\n",
        "            axes[k + 1].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "        axes[k + 1].legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    return u, omega\n",
        "\n",
        "for i in range(15):\n",
        "    u, omega = plot_vmd_decomposition(X_train, y_train, id_=i, K=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "dPsQPdIj-0fT",
        "outputId": "acd28eab-afd8-44c7-d229-d7e414f2e3c4"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot by image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TS2RP(data, args):\n",
        "    dimension = args.get('dimension', 1)\n",
        "    percentage = args.get('percentage', 1)\n",
        "    rp = RecurrencePlot(dimension=dimension, percentage=percentage)\n",
        "    data_reshaped = data.reshape(1, -1)\n",
        "    rp_image = rp.fit_transform(data_reshaped)\n",
        "    return rp_image\n",
        "\n",
        "def TS2MTF(data, args):\n",
        "    n_bins = args.get('n_bins', 10)\n",
        "    image_size = args.get('image_size', 1.0)\n",
        "    mtf = MarkovTransitionField(image_size=image_size, n_bins=n_bins)\n",
        "    data_reshaped = data.reshape(1, -1)\n",
        "    mtf_image = mtf.fit_transform(data_reshaped)\n",
        "    return mtf_image\n",
        "\n",
        "def TS2GAF(data, args):\n",
        "    method = args.get('method', 'summation')\n",
        "    gaf = GramianAngularField(method=method)\n",
        "    data_reshaped = data.reshape(1, -1)\n",
        "    gaf_image = gaf.fit_transform(data_reshaped)\n",
        "    return gaf_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_structure_break(img, df, break_flag, ax=None, title='Time Series Image'):\n",
        "    # img shape: (1, H, W) 或 (H, W)\n",
        "    if img.ndim == 3:\n",
        "        image = img[0]\n",
        "    else:\n",
        "        image = img\n",
        "\n",
        "    # 如果没有传入 ax，就新建一个图表\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    else:\n",
        "        fig = ax.figure  # 获取子图所属的整个图对象\n",
        "\n",
        "    cax = ax.imshow(image, cmap='viridis', aspect='auto')\n",
        "    fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # 找到 period 发生变化的边界索引\n",
        "    period = df['period'].values\n",
        "    boundary_indices = np.where(np.diff(period) != 0)[0] + 1\n",
        "\n",
        "    # 绘制虚线\n",
        "    for idx in boundary_indices:\n",
        "        color = 'red' if break_flag else 'gray'\n",
        "        label = 'Structure Break' if (break_flag and idx == boundary_indices[0]) else \\\n",
        "                'No Structure Break' if (not break_flag and idx == boundary_indices[0]) else None\n",
        "        ax.axvline(x=idx, color=color, linestyle='--', linewidth=2, label=label)\n",
        "        ax.axhline(y=idx, color=color, linestyle='--', linewidth=2)\n",
        "\n",
        "    if boundary_indices.size > 0:\n",
        "        ax.legend(loc='upper right')\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Time Steps')\n",
        "    ax.set_ylabel('Time Steps')\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_time_series_with_breaks(df_idx):\n",
        "    target_length = 3000\n",
        "    df = X_train.loc[df_idx]\n",
        "    flag = y_train.loc[df_idx]\n",
        "    # print(df)\n",
        "    data = df['value'].values\n",
        "    print(f\"📈 Raw Timeseries loaded, shape: {data.shape}\")\n",
        "\n",
        "    if len(data) < target_length:\n",
        "        print(f\"⏳ Padding with {target_length - len(data)} zeros to reach target length {target_length}\")\n",
        "        data = np.pad(data, (0, target_length - len(data)), mode='constant')\n",
        "\n",
        "    rp_img = TS2RP(data, {'dimension': 1, 'percentage': 0.1})\n",
        "    print(f\"📊 Recurrence Plot image shape: {rp_img.shape}\")\n",
        "    mtf_img = TS2MTF(data, {'n_bins': 8})\n",
        "    print(f\"📊 Markov Transition Field image shape: {mtf_img.shape}\")\n",
        "    gaf_img = TS2GAF(data, {'method': 'summation'})\n",
        "    print(f\"📊 Gramian Angular Field image shape: {gaf_img.shape}\")\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    plot_structure_break(rp_img, df, break_flag=flag, ax=axs[0], title='Recurrence Plot')\n",
        "    plot_structure_break(mtf_img, df, break_flag=flag, ax=axs[1], title='Markov Transition Field')\n",
        "    plot_structure_break(gaf_img, df, break_flag=flag, ax=axs[2], title='Gramian Angular Field')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_time_series_with_breaks(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_time_series_with_breaks(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_time_series_with_breaks(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "M_dTYXms-0fT",
        "outputId": "b3ee6375-995f-47f6-f6e5-7f08d9838820"
      },
      "outputs": [],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lengths = [len(df) for df in X_test]\n",
        "lengths = pd.Series(lengths)\n",
        "print('时间序列长度描述性统计\\n', lengths.describe())\n",
        "print('时间序列长度分布')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(lengths, bins=30, kde=True)\n",
        "plt.xlabel('Length of Each Time Series')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Time Series Lengths per ID')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    model = None\n",
        "\n",
        "    # You could enhance this by training an actual model, for example:\n",
        "    # 1. Extract features from before/after segments of each time series\n",
        "    # 2. Train a classifier using these features and y_train labels\n",
        "    # 3. Save the trained model\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        # Baseline approach: Compute t-test between values before and after boundary point\n",
        "        # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "        # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "        # suggesting a structural break\n",
        "        def t_test(u: pd.DataFrame):\n",
        "            return -scipy.stats.ttest_ind(\n",
        "                u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "                u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "            ).pvalue\n",
        "\n",
        "        prediction = t_test(dataset)\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "        # Note: This baseline approach uses a t-test to compare the distributions\n",
        "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "        # suggests stronger evidence that the distributions are different,\n",
        "        # indicating a potential structural break."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZeP-4--0fU"
      },
      "outputs": [],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "adia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
