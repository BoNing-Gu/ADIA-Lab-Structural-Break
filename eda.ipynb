{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T09:52:21.302334Z",
          "start_time": "2024-11-18T09:52:18.268241Z"
        },
        "id": "MKqz-6Zw-0fR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pyts.image import RecurrencePlot, MarkovTransitionField, GramianAngularField\n",
        "from statsmodels.datasets import co2\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "from statsmodels.tsa.seasonal import STL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjD_WSAS-0fR"
      },
      "outputs": [],
      "source": [
        "import crunch\n",
        "\n",
        "crunch = crunch.load_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKHXgvjN-0fS"
      },
      "outputs": [],
      "source": [
        "# Load the data simply\n",
        "X_train, y_train, X_test = crunch.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T_JmgMq-0fS"
      },
      "source": [
        "### Understanding `X_train`\n",
        "\n",
        "The training data is structured as a pandas DataFrame with a MultiIndex:\n",
        "\n",
        "**Index Levels:**\n",
        "- `id`: Identifies the unique time series\n",
        "- `time`: The timestep within each time series\n",
        "\n",
        "**Columns:**\n",
        "- `value`: The actual time series value at each timestep\n",
        "- `period`: A binary indicator where `0` represents the **period before** the boundary point, and `1` represents the **period after** the boundary point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "0oRCTnOb-0fS",
        "outputId": "2d0663ba-76b2-4937-d7fc-e6c314784242"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lengths = X_train.groupby('id').size()\n",
        "print('æ—¶é—´åºåˆ—é•¿åº¦æè¿°æ€§ç»Ÿè®¡\\n', lengths.describe())\n",
        "print('æ—¶é—´åºåˆ—é•¿åº¦åˆ†å¸ƒ')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(lengths, bins=30, kde=True)\n",
        "plt.xlabel('Length of Each Time Series')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Time Series Lengths per ID')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»Ÿè®¡æ¯ä¸ª id åœ¨ period==0 å’Œ period==1 ä¸‹çš„é•¿åº¦\n",
        "period_lengths = X_train.groupby(['id', 'period']).size().unstack(fill_value=0)\n",
        "period0_lengths = period_lengths[0]\n",
        "period1_lengths = period_lengths[1]\n",
        "\n",
        "print('Period 0 é•¿åº¦åˆ†å¸ƒæè¿°æ€§ç»Ÿè®¡:\\n', period0_lengths.describe())\n",
        "print('Period 1 é•¿åº¦åˆ†å¸ƒæè¿°æ€§ç»Ÿè®¡:\\n', period1_lengths.describe())\n",
        "\n",
        "# ç»˜åˆ¶ä¸¤ä¸ªå­å›¾\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# period==0\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(period0_lengths, bins=30, kde=True, color='skyblue')\n",
        "plt.xlabel('Length of Period 0')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Period 0 Lengths per ID')\n",
        "plt.grid(True)\n",
        "\n",
        "# period==1\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(period1_lengths, bins=30, kde=True, color='salmon')\n",
        "plt.xlabel('Length of Period 1')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Period 1 Lengths per ID')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ç»Ÿè®¡æ¯ä¸ª id åœ¨ period==0 å’Œ period==1 ä¸‹çš„é•¿åº¦\n",
        "period_lengths = X_train.groupby(['id', 'period']).size().unstack(fill_value=0)\n",
        "period0_lengths = period_lengths[0]\n",
        "period1_lengths = period_lengths[1]\n",
        "total_lengths = period0_lengths + period1_lengths\n",
        "\n",
        "# è®¡ç®—æ¯”ä¾‹\n",
        "period1_ratio = period1_lengths / total_lengths\n",
        "\n",
        "print('Period1 å æ¯”æè¿°æ€§ç»Ÿè®¡:\\n', period1_ratio.describe())\n",
        "\n",
        "# ç»˜åˆ¶æ¯”ä¾‹åˆ†å¸ƒå›¾\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(period1_ratio, bins=30, kde=True, color='purple')\n",
        "plt.xlabel('Proportion of Period 1 Length')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Period 1 Length Proportion per ID')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train.loc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_moving_average_decomposition(X_train, y_train, id_=0, window=100):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨æ»‘åŠ¨å¹³å‡è¿›è¡Œè¶‹åŠ¿ä¸Žæ®‹å·®åˆ†è§£ï¼Œå¹¶ç»˜å›¾ï¼ˆé£Žæ ¼ä¸Ž STL ç›¸åŒï¼‰\n",
        "\n",
        "    å‚æ•°:\n",
        "    - X_train: åŒ…å« 'value' å’Œ 'period' çš„ DataFrameï¼ŒMultiIndex\n",
        "    - y_train: æ ‡ç­¾\n",
        "    - id_: æŒ‡å®šçš„æ ·æœ¬ id\n",
        "    - window: æ»‘åŠ¨å¹³å‡çª—å£å¤§å°\n",
        "\n",
        "    è¿”å›ž:\n",
        "    - trend: å¹³æ»‘åŽçš„è¶‹åŠ¿\n",
        "    - residual: æ®‹å·®\n",
        "    \"\"\"\n",
        "    X_df = X_train.loc[id_]\n",
        "    value_series = X_df['value'].reset_index(drop=True)\n",
        "    period_series = X_df['period'].reset_index(drop=True).values\n",
        "\n",
        "    trend = value_series.rolling(window=window, center=True, min_periods=1).mean()\n",
        "    residual = value_series - trend\n",
        "\n",
        "    # ç»˜å›¾ï¼Œ3 è¡Œ 1 åˆ—ï¼Œä¸Ž STL æ ·å¼ä¸€è‡´\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(6, 4), sharex=True)\n",
        "    fig.suptitle(f'Moving Average Decomposition: ID {id_} Break={y_train.loc[id_]}', fontsize=16)\n",
        "\n",
        "    transition_points = np.where(np.diff(period_series) != 0)[0]\n",
        "\n",
        "    # åŽŸå§‹åºåˆ—\n",
        "    axes[0].plot(value_series, label='Observed', color='black')\n",
        "    for point in transition_points:\n",
        "        axes[0].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # è¶‹åŠ¿\n",
        "    axes[1].plot(trend, label=f'Trend (MA window={window})', color='blue')\n",
        "    for point in transition_points:\n",
        "        axes[1].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[1].legend()\n",
        "\n",
        "    # æ®‹å·®\n",
        "    axes[2].plot(residual, label='Residual', color='orange')\n",
        "    for point in transition_points:\n",
        "        axes[2].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return trend, residual\n",
        "\n",
        "for i in range(15):\n",
        "    trend, residual = plot_moving_average_decomposition(X_train, y_train, id_=i, window=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_stl_decomposition(X_train, y_train, id_=0, period=100, seasonal=15, robust=True):\n",
        "    \"\"\"\n",
        "    å¯¹è¾“å…¥çš„ DataFrame (å•ä¸€ id çš„æ—¶é—´åºåˆ—) è¿›è¡Œ STL åˆ†è§£å¹¶ç»˜å›¾\n",
        "\n",
        "    å‚æ•°:\n",
        "    - X_df: DataFrameï¼Œå«æœ‰ 'value' åˆ—ï¼Œindex æ˜¯ time\n",
        "    - period: åˆ†è§£å‘¨æœŸï¼Œå»ºè®®è®¾ä¸ºä¸Žè¶‹åŠ¿ç›¸å…³çš„çª—å£å¤§å°\n",
        "    - seasonal: å­£èŠ‚æ€§å¹³æ»‘çª—å£ï¼ˆå¥‡æ•°ï¼‰\n",
        "    - robust: æ˜¯å¦ä½¿ç”¨é²æ£’åˆ†è§£ä»¥å‡å°‘å¼‚å¸¸å€¼å½±å“\n",
        "\n",
        "    è¿”å›ž:\n",
        "    - STL åˆ†è§£ç»“æžœå¯¹è±¡\n",
        "    \"\"\"\n",
        "    X_df = X_train.loc[id_]\n",
        "    value_series = X_df['value'].reset_index(drop=True)\n",
        "    period_series = X_df['period'].reset_index(drop=True).values\n",
        "\n",
        "    stl = STL(value_series, period=period, seasonal=seasonal, robust=robust)\n",
        "    result = stl.fit()\n",
        "\n",
        "    fig = result.plot()\n",
        "    fig.suptitle(f'STL: ID {id_} Break={y_train.loc[id_]}', fontsize=16)\n",
        "\n",
        "    transition_points = np.where(np.diff(period_series) != 0)[0]\n",
        "\n",
        "    for ax in fig.axes:\n",
        "        for point in transition_points:\n",
        "            ax.axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return result\n",
        "\n",
        "for i in range(15):\n",
        "    result = plot_stl_decomposition(X_train, y_train, id_=i, period=100, seasonal=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Feb 20 19:24:58 2019\n",
        "\n",
        "@author: VinÃ­cius Rezende Carvalho\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "def  VMD(f, alpha, tau, K, DC, init, tol):\n",
        "    \"\"\"\n",
        "    u,u_hat,omega = VMD(f, alpha, tau, K, DC, init, tol)\n",
        "    Variational mode decomposition\n",
        "    Python implementation by VinÃ­cius Rezende Carvalho - vrcarva@gmail.com\n",
        "    code based on Dominique Zosso's MATLAB code, available at:\n",
        "    https://www.mathworks.com/matlabcentral/fileexchange/44765-variational-mode-decomposition\n",
        "    Original paper:\n",
        "    Dragomiretskiy, K. and Zosso, D. (2014) â€˜Variational Mode Decompositionâ€™, \n",
        "    IEEE Transactions on Signal Processing, 62(3), pp. 531â€“544. doi: 10.1109/TSP.2013.2288675.\n",
        "    \n",
        "    \n",
        "    Input and Parameters:\n",
        "    ---------------------\n",
        "    f       - the time domain signal (1D) to be decomposed\n",
        "    alpha   - the balancing parameter of the data-fidelity constraint\n",
        "    tau     - time-step of the dual ascent ( pick 0 for noise-slack )\n",
        "    K       - the number of modes to be recovered\n",
        "    DC      - true if the first mode is put and kept at DC (0-freq)\n",
        "    init    - 0 = all omegas start at 0\n",
        "                       1 = all omegas start uniformly distributed\n",
        "                      2 = all omegas initialized randomly\n",
        "    tol     - tolerance of convergence criterion; typically around 1e-6\n",
        "\n",
        "    Output:\n",
        "    -------\n",
        "    u       - the collection of decomposed modes\n",
        "    u_hat   - spectra of the modes\n",
        "    omega   - estimated mode center-frequencies\n",
        "    \"\"\"\n",
        "    \n",
        "    if len(f)%2:\n",
        "       f = f[:-1]\n",
        "\n",
        "    # Period and sampling frequency of input signal\n",
        "    fs = 1./len(f)\n",
        "    \n",
        "    ltemp = len(f)//2 \n",
        "    fMirr =  np.append(np.flip(f[:ltemp],axis = 0),f)  \n",
        "    fMirr = np.append(fMirr,np.flip(f[-ltemp:],axis = 0))\n",
        "\n",
        "    # Time Domain 0 to T (of mirrored signal)\n",
        "    T = len(fMirr)\n",
        "    t = np.arange(1,T+1)/T  \n",
        "    \n",
        "    # Spectral Domain discretization\n",
        "    freqs = t-0.5-(1/T)\n",
        "\n",
        "    # Maximum number of iterations (if not converged yet, then it won't anyway)\n",
        "    Niter = 500\n",
        "    # For future generalizations: individual alpha for each mode\n",
        "    Alpha = alpha*np.ones(K)\n",
        "    \n",
        "    # Construct and center f_hat\n",
        "    f_hat = np.fft.fftshift((np.fft.fft(fMirr)))\n",
        "    f_hat_plus = np.copy(f_hat) #copy f_hat\n",
        "    f_hat_plus[:T//2] = 0\n",
        "\n",
        "    # Initialization of omega_k\n",
        "    omega_plus = np.zeros([Niter, K])\n",
        "\n",
        "\n",
        "    if init == 1:\n",
        "        for i in range(K):\n",
        "            omega_plus[0,i] = (0.5/K)*(i)\n",
        "    elif init == 2:\n",
        "        omega_plus[0,:] = np.sort(np.exp(np.log(fs) + (np.log(0.5)-np.log(fs))*np.random.rand(1,K)))\n",
        "    else:\n",
        "        omega_plus[0,:] = 0\n",
        "            \n",
        "    # if DC mode imposed, set its omega to 0\n",
        "    if DC:\n",
        "        omega_plus[0,0] = 0\n",
        "    \n",
        "    # start with empty dual variables\n",
        "    lambda_hat = np.zeros([Niter, len(freqs)], dtype = complex)\n",
        "    \n",
        "    # other inits\n",
        "    uDiff = tol+np.spacing(1) # update step\n",
        "    n = 0 # loop counter\n",
        "    sum_uk = 0 # accumulator\n",
        "    # matrix keeping track of every iterant // could be discarded for mem\n",
        "    u_hat_plus = np.zeros([Niter, len(freqs), K],dtype=complex)    \n",
        "\n",
        "    #*** Main loop for iterative updates***\n",
        "\n",
        "    while ( uDiff > tol and  n < Niter-1 ): # not converged and below iterations limit\n",
        "        # update first mode accumulator\n",
        "        k = 0\n",
        "        sum_uk = u_hat_plus[n,:,K-1] + sum_uk - u_hat_plus[n,:,0]\n",
        "        \n",
        "        # update spectrum of first mode through Wiener filter of residuals\n",
        "        u_hat_plus[n+1,:,k] = (f_hat_plus - sum_uk - lambda_hat[n,:]/2)/(1.+Alpha[k]*(freqs - omega_plus[n,k])**2)\n",
        "        \n",
        "        # update first omega if not held at 0\n",
        "        if not(DC):\n",
        "            omega_plus[n+1,k] = np.dot(freqs[T//2:T],(abs(u_hat_plus[n+1, T//2:T, k])**2))/np.sum(abs(u_hat_plus[n+1,T//2:T,k])**2)\n",
        "\n",
        "        # update of any other mode\n",
        "        for k in np.arange(1,K):\n",
        "            #accumulator\n",
        "            sum_uk = u_hat_plus[n+1,:,k-1] + sum_uk - u_hat_plus[n,:,k]\n",
        "            # mode spectrum\n",
        "            u_hat_plus[n+1,:,k] = (f_hat_plus - sum_uk - lambda_hat[n,:]/2)/(1+Alpha[k]*(freqs - omega_plus[n,k])**2)\n",
        "            # center frequencies\n",
        "            omega_plus[n+1,k] = np.dot(freqs[T//2:T],(abs(u_hat_plus[n+1, T//2:T, k])**2))/np.sum(abs(u_hat_plus[n+1,T//2:T,k])**2)\n",
        "            \n",
        "        # Dual ascent\n",
        "        lambda_hat[n+1,:] = lambda_hat[n,:] + tau*(np.sum(u_hat_plus[n+1,:,:],axis = 1) - f_hat_plus)\n",
        "        \n",
        "        # loop counter\n",
        "        n = n+1\n",
        "        \n",
        "        # converged yet?\n",
        "        uDiff = np.spacing(1)\n",
        "        for i in range(K):\n",
        "            uDiff = uDiff + (1/T)*np.dot((u_hat_plus[n,:,i]-u_hat_plus[n-1,:,i]),np.conj((u_hat_plus[n,:,i]-u_hat_plus[n-1,:,i])))\n",
        "\n",
        "        uDiff = np.abs(uDiff)        \n",
        "            \n",
        "    #Postprocessing and cleanup\n",
        "    \n",
        "    #discard empty space if converged early\n",
        "    Niter = np.min([Niter,n])\n",
        "    omega = omega_plus[:Niter,:]\n",
        "    \n",
        "    idxs = np.flip(np.arange(1,T//2+1),axis = 0)\n",
        "    # Signal reconstruction\n",
        "    u_hat = np.zeros([T, K],dtype = complex)\n",
        "    u_hat[T//2:T,:] = u_hat_plus[Niter-1,T//2:T,:]\n",
        "    u_hat[idxs,:] = np.conj(u_hat_plus[Niter-1,T//2:T,:])\n",
        "    u_hat[0,:] = np.conj(u_hat[-1,:])    \n",
        "    \n",
        "    u = np.zeros([K,len(t)])\n",
        "    for k in range(K):\n",
        "        u[k,:] = np.real(np.fft.ifft(np.fft.ifftshift(u_hat[:,k])))\n",
        "        \n",
        "    # remove mirror part\n",
        "    u = u[:,T//4:3*T//4]\n",
        "\n",
        "    # recompute spectrum\n",
        "    u_hat = np.zeros([u.shape[1],K],dtype = complex)\n",
        "    for k in range(K):\n",
        "        u_hat[:,k]=np.fft.fftshift(np.fft.fft(u[k,:]))\n",
        "\n",
        "    return u, u_hat, omega"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_vmd_decomposition(X_train, y_train, id_=0, alpha=2000, tau=0, K=5, DC=0, init=1, tol=1e-7):\n",
        "    \"\"\"\n",
        "    å¯¹è¾“å…¥çš„ DataFrame (å•ä¸€ id çš„æ—¶é—´åºåˆ—) è¿›è¡Œ VMD åˆ†è§£å¹¶ç»˜å›¾\n",
        "\n",
        "    å‚æ•°:\n",
        "    - X_train: DataFrameï¼ŒåŒ…å« 'value' å’Œ 'period'ï¼Œmultiindex\n",
        "    - y_train: DataFrameï¼Œæ ‡ç­¾\n",
        "    - id_: æŒ‡å®šåˆ†è§£çš„æ ·æœ¬ id\n",
        "    - alpha, tau, K, DC, init, tol: VMD å‚æ•°\n",
        "\n",
        "    è¿”å›ž:\n",
        "    - u: åˆ†è§£åŽçš„æ¨¡æ€\n",
        "    - omega: æ¯ä¸ªæ¨¡æ€çš„ä¸­å¿ƒé¢‘çŽ‡\n",
        "    \"\"\"\n",
        "\n",
        "    X_df = X_train.loc[id_]\n",
        "    value_series = X_df['value'].reset_index(drop=True).values\n",
        "    period_series = X_df['period'].reset_index(drop=True).values\n",
        "\n",
        "    u, u_hat, omega = VMD(value_series, alpha=alpha, tau=tau, K=K, DC=DC, init=init, tol=tol)\n",
        "\n",
        "    fig, axes = plt.subplots(K + 1, 1, figsize=(6, 1.5 * (K + 1)), sharex=True)\n",
        "    fig.suptitle(f'VMD: ID {id_} Break={y_train.loc[id_]}', fontsize=16)\n",
        "\n",
        "    transition_points = np.where(np.diff(period_series) != 0)[0]\n",
        "\n",
        "    # åŽŸå§‹ä¿¡å·\n",
        "    axes[0].plot(value_series, label='Original Signal', color='black')\n",
        "    for point in transition_points:\n",
        "        axes[0].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # VMD åˆ†è§£ç»“æžœ\n",
        "    for k in range(K):\n",
        "        axes[k + 1].plot(u[k, :], label=f'Mode {k + 1}')\n",
        "        for point in transition_points:\n",
        "            axes[k + 1].axvline(point, color='red', linestyle='--', linewidth=1)\n",
        "        axes[k + 1].legend()\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    return u, omega\n",
        "\n",
        "for i in range(15):\n",
        "    u, omega = plot_vmd_decomposition(X_train, y_train, id_=i, K=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WP39dgx-0fS"
      },
      "source": [
        "### Understanding `y_train`\n",
        "\n",
        "This is a simple `pandas.Series` that tells if a dataset id has a structural breakpoint or not.\n",
        "\n",
        "**Index:**\n",
        "- `id`: the ID of the dataset\n",
        "\n",
        "**Value:**\n",
        "- `structural_breakpoint`: Boolean indicating whether a structural break occurred (`True`) or not (`False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "dPsQPdIj-0fT",
        "outputId": "acd28eab-afd8-44c7-d229-d7e414f2e3c4"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot by image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TS2RP(data, args):\n",
        "    dimension = args.get('dimension', 1)\n",
        "    percentage = args.get('percentage', 1)\n",
        "    rp = RecurrencePlot(dimension=dimension, percentage=percentage)\n",
        "    data_reshaped = data.reshape(1, -1)\n",
        "    rp_image = rp.fit_transform(data_reshaped)\n",
        "    return rp_image\n",
        "\n",
        "def TS2MTF(data, args):\n",
        "    n_bins = args.get('n_bins', 10)\n",
        "    image_size = args.get('image_size', 1.0)\n",
        "    mtf = MarkovTransitionField(image_size=image_size, n_bins=n_bins)\n",
        "    data_reshaped = data.reshape(1, -1)\n",
        "    mtf_image = mtf.fit_transform(data_reshaped)\n",
        "    return mtf_image\n",
        "\n",
        "def TS2GAF(data, args):\n",
        "    method = args.get('method', 'summation')\n",
        "    gaf = GramianAngularField(method=method)\n",
        "    data_reshaped = data.reshape(1, -1)\n",
        "    gaf_image = gaf.fit_transform(data_reshaped)\n",
        "    return gaf_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_structure_break(img, df, break_flag, ax=None, title='Time Series Image'):\n",
        "    # img shape: (1, H, W) æˆ– (H, W)\n",
        "    if img.ndim == 3:\n",
        "        image = img[0]\n",
        "    else:\n",
        "        image = img\n",
        "\n",
        "    # å¦‚æžœæ²¡æœ‰ä¼ å…¥ axï¼Œå°±æ–°å»ºä¸€ä¸ªå›¾è¡¨\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    else:\n",
        "        fig = ax.figure  # èŽ·å–å­å›¾æ‰€å±žçš„æ•´ä¸ªå›¾å¯¹è±¡\n",
        "\n",
        "    cax = ax.imshow(image, cmap='viridis', aspect='auto')\n",
        "    fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # æ‰¾åˆ° period å‘ç”Ÿå˜åŒ–çš„è¾¹ç•Œç´¢å¼•\n",
        "    period = df['period'].values\n",
        "    boundary_indices = np.where(np.diff(period) != 0)[0] + 1\n",
        "\n",
        "    # ç»˜åˆ¶è™šçº¿\n",
        "    for idx in boundary_indices:\n",
        "        color = 'red' if break_flag else 'gray'\n",
        "        label = 'Structure Break' if (break_flag and idx == boundary_indices[0]) else \\\n",
        "                'No Structure Break' if (not break_flag and idx == boundary_indices[0]) else None\n",
        "        ax.axvline(x=idx, color=color, linestyle='--', linewidth=2, label=label)\n",
        "        ax.axhline(y=idx, color=color, linestyle='--', linewidth=2)\n",
        "\n",
        "    if boundary_indices.size > 0:\n",
        "        ax.legend(loc='upper right')\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Time Steps')\n",
        "    ax.set_ylabel('Time Steps')\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_time_series_with_breaks(df_idx):\n",
        "    target_length = 3000\n",
        "    df = X_train.loc[df_idx]\n",
        "    flag = y_train.loc[df_idx]\n",
        "    # print(df)\n",
        "    data = df['value'].values\n",
        "    print(f\"ðŸ“ˆ Raw Timeseries loaded, shape: {data.shape}\")\n",
        "\n",
        "    if len(data) < target_length:\n",
        "        print(f\"â³ Padding with {target_length - len(data)} zeros to reach target length {target_length}\")\n",
        "        data = np.pad(data, (0, target_length - len(data)), mode='constant')\n",
        "\n",
        "    rp_img = TS2RP(data, {'dimension': 1, 'percentage': 0.1})\n",
        "    print(f\"ðŸ“Š Recurrence Plot image shape: {rp_img.shape}\")\n",
        "    mtf_img = TS2MTF(data, {'n_bins': 8})\n",
        "    print(f\"ðŸ“Š Markov Transition Field image shape: {mtf_img.shape}\")\n",
        "    gaf_img = TS2GAF(data, {'method': 'summation'})\n",
        "    print(f\"ðŸ“Š Gramian Angular Field image shape: {gaf_img.shape}\")\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    plot_structure_break(rp_img, df, break_flag=flag, ax=axs[0], title='Recurrence Plot')\n",
        "    plot_structure_break(mtf_img, df, break_flag=flag, ax=axs[1], title='Markov Transition Field')\n",
        "    plot_structure_break(gaf_img, df, break_flag=flag, ax=axs[2], title='Gramian Angular Field')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_time_series_with_breaks(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_time_series_with_breaks(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_time_series_with_breaks(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oSS08Ks-0fT"
      },
      "source": [
        "### Understanding `X_test`\n",
        "\n",
        "The test data is provided as a **`list` of `pandas.DataFrame`s** with the same format as [`X_train`](#understanding-X_test).\n",
        "\n",
        "It is structured as a list to encourage processing records one by one, which will be mandatory in the `infer()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "M_dTYXms-0fT",
        "outputId": "b3ee6375-995f-47f6-f6e5-7f08d9838820"
      },
      "outputs": [],
      "source": [
        "X_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lengths = [len(df) for df in X_test]\n",
        "lengths = pd.Series(lengths)\n",
        "print('æ—¶é—´åºåˆ—é•¿åº¦æè¿°æ€§ç»Ÿè®¡\\n', lengths.describe())\n",
        "print('æ—¶é—´åºåˆ—é•¿åº¦åˆ†å¸ƒ')\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(lengths, bins=30, kde=True)\n",
        "plt.xlabel('Length of Each Time Series')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Time Series Lengths per ID')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgulFOGX-0fT"
      },
      "source": [
        "## Strategy Implementation\n",
        "\n",
        "There are multiple approaches you can take to detect structural breaks:\n",
        "\n",
        "1. **Statistical Tests**: Compare distributions before and after the boundary point;\n",
        "2. **Feature Engineering**: Extract features from both segments for comparison;\n",
        "3. **Time Series Modeling**: Detect deviations from expected patterns;\n",
        "4. **Machine Learning**: Train models to recognize break patterns from labeled examples.\n",
        "\n",
        "The baseline implementation below uses a simple statistical approach: a t-test to compare the distributions before and after the boundary point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLfYIXlz-0fT"
      },
      "source": [
        "### The `train()` Function\n",
        "\n",
        "In this function, you build and train your model for making inferences on the test data. Your model must be stored in the `model_directory_path`.\n",
        "\n",
        "The baseline implementation below doesn't require a pre-trained model, as it uses a statistical test that will be computed at inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:04:00.459399Z",
          "start_time": "2024-11-18T10:04:00.455716Z"
        },
        "id": "xQwWDC6M-0fT"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    X_train: pd.DataFrame,\n",
        "    y_train: pd.Series,\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    # For our baseline t-test approach, we don't need to train a model\n",
        "    # This is essentially an unsupervised approach calculated at inference time\n",
        "    model = None\n",
        "\n",
        "    # You could enhance this by training an actual model, for example:\n",
        "    # 1. Extract features from before/after segments of each time series\n",
        "    # 2. Train a classifier using these features and y_train labels\n",
        "    # 3. Save the trained model\n",
        "\n",
        "    joblib.dump(model, os.path.join(model_directory_path, 'model.joblib'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n-jboJH-0fU"
      },
      "source": [
        "### The `infer()` Function\n",
        "\n",
        "In the inference function, your trained model (if any) is loaded and used to make predictions on test data.\n",
        "\n",
        "**Important workflow:**\n",
        "1. Load your model;\n",
        "2. Use the `yield` statement to signal readiness to the runner;\n",
        "3. Process each dataset one by one within the for loop;\n",
        "4. For each dataset, use `yield prediction` to return your prediction.\n",
        "\n",
        "**Note:** The datasets can only be iterated once!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-11-18T10:03:59.120294Z",
          "start_time": "2024-11-18T10:03:59.114830Z"
        },
        "id": "r1b7hRkl-0fU"
      },
      "outputs": [],
      "source": [
        "def infer(\n",
        "    X_test: typing.Iterable[pd.DataFrame],\n",
        "    model_directory_path: str,\n",
        "):\n",
        "    model = joblib.load(os.path.join(model_directory_path, 'model.joblib'))\n",
        "\n",
        "    yield  # Mark as ready\n",
        "\n",
        "    # X_test can only be iterated once.\n",
        "    # Before getting the next dataset, you must predict the current one.\n",
        "    for dataset in X_test:\n",
        "        # Baseline approach: Compute t-test between values before and after boundary point\n",
        "        # The negative p-value is used as our score - smaller p-values (larger negative numbers)\n",
        "        # indicate more evidence against the null hypothesis that distributions are the same,\n",
        "        # suggesting a structural break\n",
        "        def t_test(u: pd.DataFrame):\n",
        "            return -scipy.stats.ttest_ind(\n",
        "                u[\"value\"][u[\"period\"] == 0],  # Values before boundary point\n",
        "                u[\"value\"][u[\"period\"] == 1],  # Values after boundary point\n",
        "            ).pvalue\n",
        "\n",
        "        prediction = t_test(dataset)\n",
        "        yield prediction  # Send the prediction for the current dataset\n",
        "\n",
        "        # Note: This baseline approach uses a t-test to compare the distributions\n",
        "        # before and after the boundary point. A smaller p-value (larger negative number)\n",
        "        # suggests stronger evidence that the distributions are different,\n",
        "        # indicating a potential structural break."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W0Kl9CA-0fU"
      },
      "source": [
        "## Local testing\n",
        "\n",
        "To make sure your `train()` and `infer()` function are working properly, you can call the `crunch.test()` function that will reproduce the cloud environment locally. <br />\n",
        "Even if it is not perfect, it should give you a quick idea if your model is working properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDZeP-4--0fU"
      },
      "outputs": [],
      "source": [
        "crunch.test(\n",
        "    # Uncomment to disable the train\n",
        "    # force_first_train=False,\n",
        "\n",
        "    # Uncomment to disable the determinism check\n",
        "    # no_determinism_check=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV_5CKs--0fU"
      },
      "source": [
        "## Results\n",
        "\n",
        "Once the local tester is done, you can preview the result stored in `data/prediction.parquet`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly5q68sA-0fU"
      },
      "outputs": [],
      "source": [
        "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
        "prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oP-NLGh-0fU"
      },
      "source": [
        "### Local scoring\n",
        "\n",
        "You can call the function that the system uses to estimate your score locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyCrjpzv-0fU"
      },
      "outputs": [],
      "source": [
        "# Load the targets\n",
        "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
        "\n",
        "# Call the scoring function\n",
        "sklearn.metrics.roc_auc_score(\n",
        "    target,\n",
        "    prediction,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "adia",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
