{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e1d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import statsmodels as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "import antropy\n",
    "import ruptures as rpt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. read dataset\n",
    "print(\"Loading data...\")\n",
    "X_train = pd.read_parquet('./data/X_train.parquet')\n",
    "y_train = pd.read_parquet('./data/y_train.parquet')\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. feature engineering\n",
    "FEATURE_FUNCTIONS = []\n",
    "\n",
    "# def feature(func):\n",
    "#     \"\"\"一个用于注册特征函数的装饰器\"\"\"\n",
    "#     FEATURE_FUNCTIONS.append(func)\n",
    "#     return func\n",
    "\n",
    "# --- 1. 分布统计特征 ---\n",
    "# @feature\n",
    "def distributional_stats(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期的分布（均值、标准差、偏度、峰度）以及KS检验、t检验。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    feats = {}\n",
    "    \n",
    "    mean1, mean2 = s1.mean(), s2.mean()\n",
    "    feats['mean_diff'] = mean2 - mean1\n",
    "    \n",
    "    std1, std2 = s1.std(), s2.std()\n",
    "    feats['std_diff'] = std2 - std1\n",
    "    if std1 > 1e-6:\n",
    "        feats['std_ratio'] = std2 / std1\n",
    "    else:\n",
    "        feats['std_ratio'] = 1.0 if std2 < 1e-6 else 1e6\n",
    "    \n",
    "    feats['skew_diff'] = s2.skew() - s1.skew()\n",
    "    feats['kurt_diff'] = s2.kurt() - s1.kurt()\n",
    "    \n",
    "    if len(s1) > 1 and len(s2) > 1:\n",
    "        ks_stat, ks_pvalue = scipy.stats.ks_2samp(s1, s2)\n",
    "        feats['ks_stat'] = ks_stat\n",
    "        feats['ks_pvalue'] = -ks_pvalue\n",
    "    else:\n",
    "        feats['ks_stat'] = 0\n",
    "        feats['ks_pvalue'] = 0\n",
    "\n",
    "    ttest_stat, ttest_pvalue = scipy.stats.ttest_ind(s1, s2, equal_var=False)\n",
    "    # feats['ttest_stat'] = ttest_stat\n",
    "    feats['ttest_pvalue'] = -ttest_pvalue if not np.isnan(ttest_pvalue) else 0\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 2. 累积和特征 ---\n",
    "# @feature\n",
    "def cumulative_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期累积和的差异。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    feats = {}\n",
    "    \n",
    "    feats['sum_diff'] = s2.sum() - s1.sum()\n",
    "    \n",
    "    if not s1.empty and not s2.empty:\n",
    "        feats['cumsum_max_diff'] = s2.cumsum().max() - s1.cumsum().max()\n",
    "    else:\n",
    "        feats['cumsum_max_diff'] = 0\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 3. 振荡特征 ---\n",
    "# @feature\n",
    "def oscillation_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期的振荡特性（过零点、自相关、差分方差）。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].reset_index(drop=True)\n",
    "    s2 = u['value'][u['period'] == 1].reset_index(drop=True)\n",
    "    feats = {}\n",
    "\n",
    "    def count_zero_crossings(series: pd.Series):\n",
    "        if len(series) < 2: return 0\n",
    "        centered_series = series - series.mean()\n",
    "        # Handle case where all values are the same\n",
    "        if centered_series.eq(0).all(): return 0\n",
    "        return np.sum(np.diff(np.sign(centered_series)) != 0)\n",
    "\n",
    "    feats['zero_cross_diff'] = count_zero_crossings(s2) - count_zero_crossings(s1)\n",
    "    \n",
    "    def autocorr_lag1(s):\n",
    "        if len(s) < 2: return 0.0\n",
    "        ac = s.autocorr(lag=1)\n",
    "        return ac if not np.isnan(ac) else 0.0\n",
    "        \n",
    "    feats['autocorr_lag1_diff'] = autocorr_lag1(s2) - autocorr_lag1(s1)\n",
    "    \n",
    "    feats['diff_var_diff'] = s2.diff().var() - s1.diff().var()\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 4. 周期性特征 ---\n",
    "# @feature\n",
    "def cyclic_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：基于FFT分析两个周期的周期性差异。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    feats = {}\n",
    "\n",
    "    def get_fft_props(series):\n",
    "        if len(series) < 2: return 0.0, 0.0\n",
    "        \n",
    "        N = len(series)\n",
    "        yf = np.fft.fft(series.values)\n",
    "        # Get power spectrum (amplitude squared) & frequencies\n",
    "        power = np.abs(yf[1:N//2])**2\n",
    "        xf = np.fft.fftfreq(N, 1)[1:N//2]\n",
    "        \n",
    "        if len(power) == 0: return 0.0, 0.0\n",
    "            \n",
    "        dominant_freq = xf[np.argmax(power)]\n",
    "        max_power = np.max(power)\n",
    "        return dominant_freq, max_power\n",
    "\n",
    "    freq1, power1 = get_fft_props(s1)\n",
    "    freq2, power2 = get_fft_props(s2)\n",
    "    \n",
    "    feats['dominant_freq_diff'] = freq2 - freq1\n",
    "    feats['max_power_diff'] = power2 - power1\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 5. 振幅特征 ---\n",
    "# @feature\n",
    "def amplitude_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期的振幅（峰峰值、四分位距）。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    feats = {}\n",
    "    \n",
    "    if not s1.empty and not s2.empty:\n",
    "        feats['ptp_diff'] = np.ptp(s2) - np.ptp(s1)\n",
    "        feats['iqr_diff'] = scipy.stats.iqr(s2) - scipy.stats.iqr(s1)\n",
    "    else:\n",
    "        feats['ptp_diff'] = 0\n",
    "        feats['iqr_diff'] = 0\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 6. 高阶统计量与非线性趋势变化特征---\n",
    "# @feature\n",
    "def higher_order_stats_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期的非线性趋势变化（变异系数、滚动标准差均值差异、Theil-Sen 斜率估计）。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].reset_index(drop=True)\n",
    "    s2 = u['value'][u['period'] == 1].reset_index(drop=True)\n",
    "    feats = {}\n",
    "\n",
    "    # --- 1. 变异系数（标准差 / 均值）\n",
    "    def safe_cv(s):\n",
    "        m = s.mean()\n",
    "        std = s.std()\n",
    "        return std / m if abs(m) > 1e-6 else 0.0\n",
    "\n",
    "    feats['cv_diff'] = safe_cv(s2) - safe_cv(s1)\n",
    "\n",
    "    # --- 2. 滚动标准差的均值差异（窗口大小可调）\n",
    "    def rolling_std_mean(s, window=5):\n",
    "        if len(s) < window:\n",
    "            return 0.0\n",
    "        return s.rolling(window=window).std().dropna().mean()\n",
    "\n",
    "    feats['rolling_std_diff'] = rolling_std_mean(s2) - rolling_std_mean(s1)\n",
    "\n",
    "    # --- 3. Theil-Sen 斜率估计\n",
    "    def slope_theil_sen(s):\n",
    "        if len(s) < 2:\n",
    "            return 0.0\n",
    "        try:\n",
    "            slope, intercept, _, _ = scipy.stats.theilslopes(s.values, np.arange(len(s)))\n",
    "            return slope\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    feats['theil_sen_slope_diff'] = slope_theil_sen(s2) - slope_theil_sen(s1)\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 7. 时间序列建模：比较period=0和1下的AR模型残差、AIC差异 ---\n",
    "# @feature\n",
    "def ar_model_residual_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期的AR模型残差、AIC差异。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].reset_index(drop=True)\n",
    "    s2 = u['value'][u['period'] == 1].reset_index(drop=True)\n",
    "    feats = {}\n",
    "\n",
    "    def fit_ar(s, lags=10):\n",
    "        if len(s) <= lags + 1:\n",
    "            return None\n",
    "        try:\n",
    "            return tsa.ar_model.AutoReg(s, lags=lags, old_names=False).fit()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    model1 = fit_ar(s1)\n",
    "    model2 = fit_ar(s2)\n",
    "\n",
    "    # 残差标准差 & AIC 差异\n",
    "    if model1 is not None and model2 is not None:\n",
    "        feats['ar_resid_std_diff'] = model2.resid.std() - model1.resid.std()\n",
    "        feats['ar_aic_diff'] = model2.aic - model1.aic\n",
    "    else:\n",
    "        feats['ar_resid_std_diff'] = 0.0\n",
    "        feats['ar_aic_diff'] = 0.0\n",
    "\n",
    "    # period=0 拟合后预测 period=1 前 len(s2) 步\n",
    "    if model1 is not None and len(s2) > 0:\n",
    "        try:\n",
    "            max_lag = max(model1.model.ar_lags)\n",
    "            # 获取 period=0 的尾部作为预测初值\n",
    "            history = s1.iloc[-max_lag:].tolist()\n",
    "            preds = []\n",
    "\n",
    "            for t in range(len(s2)):\n",
    "                lagged_vals = history[-max_lag:]\n",
    "                pred = model1.params['const'] if 'const' in model1.params else 0.0\n",
    "                for i, lag in enumerate(model1.model.ar_lags):\n",
    "                    pred += model1.params[f'value.L{lag}'] * lagged_vals[-lag]\n",
    "                preds.append(pred)\n",
    "                history.append(s2.iloc[t])  # 模拟滚动更新\n",
    "\n",
    "            preds = np.array(preds)\n",
    "            mse = np.mean((preds - s2.values[:len(preds)]) ** 2)\n",
    "            feats['ar_predict_mse'] = mse\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Prediction error: {e}\")\n",
    "            feats['ar_predict_mse'] = 0.0\n",
    "    else:\n",
    "        feats['ar_predict_mse'] = 0.0\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b36deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. 熵信息 ---\n",
    "# @feature\n",
    "def entropy_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期的熵信息（Shannon、Permutation、Spectral、SVD、Approximate、Sample、Hjorth mobility & complexity）。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    # Shannon entropy\n",
    "    def compute_entropy(x):\n",
    "        hist, _ = np.histogram(x, bins='auto', density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        return scipy.stats.entropy(hist)\n",
    "    feats['shannon_entropy_0'] = compute_entropy(s1)\n",
    "    feats['shannon_entropy_1'] = compute_entropy(s2)\n",
    "    feats['shannon_entropy_diff'] = feats['shannon_entropy_1'] - feats['shannon_entropy_0']\n",
    "\n",
    "    # Permutation entropy\n",
    "    feats['perm_entropy_0'] = antropy.perm_entropy(s1, normalize=True)\n",
    "    feats['perm_entropy_1'] = antropy.perm_entropy(s2, normalize=True)\n",
    "    feats['perm_entropy_diff'] = feats['perm_entropy_1'] - feats['perm_entropy_0']\n",
    "\n",
    "    # Spectral entropy\n",
    "    feats['spectral_entropy_0'] = antropy.spectral_entropy(s1, sf=1.0, normalize=True)\n",
    "    feats['spectral_entropy_1'] = antropy.spectral_entropy(s2, sf=1.0, normalize=True)\n",
    "    feats['spectral_entropy_diff'] = feats['spectral_entropy_1'] - feats['spectral_entropy_0']\n",
    "\n",
    "    # SVD entropy\n",
    "    feats['svd_entropy_0'] = antropy.svd_entropy(s1, normalize=True)\n",
    "    feats['svd_entropy_1'] = antropy.svd_entropy(s2, normalize=True)\n",
    "    feats['svd_entropy_diff'] = feats['svd_entropy_1'] - feats['svd_entropy_0']\n",
    "\n",
    "    # Approximate entropy\n",
    "    feats['approx_entropy_0'] = antropy.app_entropy(s1)\n",
    "    feats['approx_entropy_1'] = antropy.app_entropy(s2)\n",
    "    feats['approx_entropy_diff'] = feats['approx_entropy_1'] - feats['approx_entropy_0']\n",
    "\n",
    "    # Sample entropy\n",
    "    feats['sample_entropy_0'] = antropy.sample_entropy(s1)\n",
    "    feats['sample_entropy_1'] = antropy.sample_entropy(s2)\n",
    "    feats['sample_entropy_diff'] = feats['sample_entropy_1'] - feats['sample_entropy_0']\n",
    "\n",
    "    # Hjorth mobility and complexity\n",
    "    feats['hjorth_mobility_0'], feats['hjorth_complexity_0'] = antropy.hjorth_params(s1)\n",
    "    feats['hjorth_mobility_1'], feats['hjorth_complexity_1'] = antropy.hjorth_params(s2)\n",
    "    feats['hjorth_mobility_diff'] = feats['hjorth_mobility_1'] - feats['hjorth_mobility_0']\n",
    "    feats['hjorth_complexity_diff'] = feats['hjorth_complexity_1'] - feats['hjorth_complexity_0']\n",
    "\n",
    "    # Number of zero-crossings\n",
    "    feats['num_zerocross_0'] = antropy.num_zerocross(s1)\n",
    "    feats['num_zerocross_1'] = antropy.num_zerocross(s2)\n",
    "    feats['num_zerocross_diff'] = feats['num_zerocross_1'] - feats['num_zerocross_0']\n",
    "\n",
    "    # Lempel-Ziv complexity\n",
    "    def series_to_binary_str(x, method='median'):\n",
    "        if method == 'median':\n",
    "            threshold = np.median(x)\n",
    "            return ''.join(['1' if val > threshold else '0' for val in x])\n",
    "        # 可扩展：支持quantile或多符号\n",
    "        return None\n",
    "    bin_str1 = series_to_binary_str(s1)\n",
    "    bin_str2 = series_to_binary_str(s2)\n",
    "    feats['lziv_complexity_0'] = antropy.lziv_complexity(bin_str1, normalize=True)\n",
    "    feats['lziv_complexity_1'] = antropy.lziv_complexity(bin_str2, normalize=True)\n",
    "    feats['lziv_complexity_diff'] = feats['lziv_complexity_1'] - feats['lziv_complexity_0']\n",
    "\n",
    "    def estimate_cond_entropy(x, lag=1):\n",
    "        x = x - np.mean(x)\n",
    "        x_lag = x[:-lag]\n",
    "        x_now = x[lag:]\n",
    "        bins = 10\n",
    "        joint_hist, _, _ = np.histogram2d(x_lag, x_now, bins=bins, density=True)\n",
    "        joint_hist = joint_hist[joint_hist > 0]\n",
    "        H_xy = -np.sum(joint_hist * np.log(joint_hist))\n",
    "        H_x = -np.sum(np.histogram(x_lag, bins=bins, density=True)[0] * \n",
    "                      np.log(np.histogram(x_lag, bins=bins, density=True)[0] + 1e-12))\n",
    "        return H_xy - H_x\n",
    "    feats['cond_entropy_0'] = estimate_cond_entropy(s1)\n",
    "    feats['cond_entropy_1'] = estimate_cond_entropy(s2)\n",
    "    feats['cond_entropy_diff'] = feats['cond_entropy_1'] - feats['cond_entropy_0']\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 9. 分形 ---\n",
    "# @feature\n",
    "def fractal_dimension_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"特征提取：比较两个周期的分形维度（Petrosian、Katz、Higuchi、Detrended Fluctuation Analysis）。\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    # Petrosian fractal dimension\n",
    "    feats['petrosian_fd_diff'] = (antropy.petrosian_fd(s1) - antropy.petrosian_fd(s2)) * 100\n",
    "\n",
    "    # Katz fractal dimension\n",
    "    feats['katz_fd_diff'] = (antropy.katz_fd(s1) - antropy.katz_fd(s2)) * 10\n",
    "\n",
    "    # Higuchi fractal dimension\n",
    "    feats['higuchi_fd_diff'] = (antropy.higuchi_fd(s1) - antropy.higuchi_fd(s2)) * 100\n",
    "\n",
    "    # Detrended fluctuation analysis\n",
    "    feats['detrended_fluctuation_diff'] = (antropy.detrended_fluctuation(s1) - antropy.detrended_fluctuation(s2)) * 10\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d88ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_feature_df = pd.read_parquet('feature_dfs/features-antropy.parquet')\n",
    "print(loaded_feature_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_FUNCTIONS = [\n",
    "    # distributional_stats,\n",
    "    # cumulative_features,\n",
    "    # oscillation_features,\n",
    "    # cyclic_features,\n",
    "    # amplitude_features,\n",
    "    # higher_order_stats_features,\n",
    "    # ar_model_residual_features,\n",
    "    # entropy_features,\n",
    "    # fractal_dimension_features,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_for_id(df_id: pd.DataFrame, id_val: int) -> dict:\n",
    "    \"\"\"为单个ID创建所有已注册的特征-用于推理阶段\"\"\"\n",
    "    all_features = {'id': id_val}\n",
    "    for func in FEATURE_FUNCTIONS:\n",
    "        try:\n",
    "            all_features.update(func(df_id))\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating feature {func.__name__} for id {id_val}: {e}\")\n",
    "    return all_features\n",
    "\n",
    "def apply_feature_func_parallel(func, X_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"对所有id并行地应用某个特征函数-用于训练阶段\"\"\"\n",
    "    all_ids = X_df.index.get_level_values(\"id\").unique()\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(lambda df_id, id_val: {**{'id': id_val}, **func(df_id)})(X_df.loc[id_val], id_val)\n",
    "        for id_val in tqdm(all_ids, desc=f\"Running {func.__name__}\")\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(results).set_index('id')\n",
    "\n",
    "def create_all_features_train(X_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"对所有特征函数依次并行应用-用于训练阶段\"\"\"\n",
    "    print(f\"Start feature generation on {len(X_df.index.get_level_values('id').unique())} ids\")\n",
    "\n",
    "    feature_dfs = []\n",
    "    for func in FEATURE_FUNCTIONS:\n",
    "        try:\n",
    "            feature_df = apply_feature_func_parallel(func, X_df)\n",
    "            feature_dfs.append(feature_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {func.__name__}: {e}\")\n",
    "\n",
    "    final_feature_df = pd.concat(feature_dfs, axis=1)\n",
    "    final_feature_df = final_feature_df.loc[:, ~final_feature_df.columns.duplicated()]\n",
    "    return final_feature_df\n",
    "\n",
    "def check_new_features_corr(feature_df, loaded_feature_df):\n",
    "    \"\"\"检查新特征与已加载特征的相关性\"\"\"\n",
    "    new_features = [col for col in feature_df.columns if col not in loaded_feature_df.columns]\n",
    "    loaded_features = loaded_feature_df.columns\n",
    "    print(f\"\\nNumber of new features: {len(new_features)}\")\n",
    "    print(f\"Number of loaded features: {len(loaded_features)}\")\n",
    "    \n",
    "    # 计算新特征与已加载特征的相关性\n",
    "    corr_matrix = feature_df[new_features + list(loaded_features)].corr()\n",
    "    cross_corr = corr_matrix.loc[new_features, loaded_features]\n",
    "    high_corr_features = cross_corr[(cross_corr.abs() > 0.7).any(axis=1)]\n",
    "    \n",
    "    if not high_corr_features.empty:\n",
    "        print(\"\\nNew features with high correlation (|corr| > 0.7) to loaded features:\")\n",
    "        # 打印每个高相关性新特征及其相关特征\n",
    "        for new_feat in high_corr_features.index:\n",
    "            correlated_with = high_corr_features.columns[high_corr_features.loc[new_feat].abs() > 0.7]\n",
    "            corr_values = high_corr_features.loc[new_feat, high_corr_features.loc[new_feat].abs() > 0.7]\n",
    "            \n",
    "            print(f\"\\n{new_feat} is highly correlated with:\")\n",
    "            for loaded_feat, corr in zip(correlated_with, corr_values):\n",
    "                print(f\"  - {loaded_feat}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNo new features show high correlation (|corr| > 0.7) with loaded features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成特征\n",
    "feature_df = create_all_features_train(X_train)\n",
    "if 'loaded_feature_df' in locals():\n",
    "    feature_df = pd.concat([loaded_feature_df, feature_df], axis=1)\n",
    "    check_new_features_corr(feature_df, loaded_feature_df)\n",
    "\n",
    "print(\"Features created.\")\n",
    "print(feature_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ccbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_parquet('feature_dfs/features-antropy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LGBM CV\n",
    "print(\"\\nStarting 5-fold cross-validation with LightGBM...\")\n",
    "\n",
    "# Basic settings\n",
    "X_train_features = feature_df\n",
    "y_train_target = y_train.loc[X_train_features.index]['structural_breakpoint'].astype(int)\n",
    "\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros(len(X_train_features))\n",
    "models = []\n",
    "feature_importances = pd.DataFrame(index=X_train_features.columns)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_features, y_train_target)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # Split\n",
    "    X_train_fold, y_train_fold = X_train_features.iloc[train_idx], y_train_target.iloc[train_idx]\n",
    "    X_val_fold, y_val_fold = X_train_features.iloc[val_idx], y_train_target.iloc[val_idx]\n",
    "    \n",
    "    # Model\n",
    "    model = lgb.LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric='auc',\n",
    "        random_state=42,\n",
    "        n_estimators=1000, \n",
    "        learning_rate=0.005,\n",
    "        num_leaves=31,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[lgb.early_stopping(100, verbose=True)]\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict_proba(X_val_fold)[:, 1]\n",
    "    oof_preds[val_idx] = preds\n",
    "    models.append(model)\n",
    "    fold_auc = roc_auc_score(y_val_fold, preds)\n",
    "    print(f\"Fold {fold+1} AUC: {fold_auc:.5f}\")\n",
    "\n",
    "    # Feature importance\n",
    "    fold_importance = pd.DataFrame({\n",
    "        'feature': X_train_features.columns,\n",
    "        'importance': model.feature_importances_,\n",
    "        'fold': fold\n",
    "    })\n",
    "    feature_importances = pd.concat([feature_importances, fold_importance], axis=0)\n",
    "\n",
    "# 计算总体验证分数\n",
    "overall_oof_auc = roc_auc_score(y_train_target, oof_preds)\n",
    "print(f\"\\nOverall OOF AUC: {overall_oof_auc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算平均特征重要性\n",
    "mean_feature_importance = feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "# 绘制特征重要性图表\n",
    "plt.figure(figsize=(10, 12))\n",
    "mean_feature_importance.sort_values().plot(kind='barh')\n",
    "plt.title('Average Feature Importance across 5 Folds')\n",
    "plt.xlabel('Mean Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
