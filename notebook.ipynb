{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e8f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['http_proxy'] = 'http://127.0.0.1:11000'\n",
    "# os.environ['https_proxy'] = 'http://127.0.0.1:11000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce981d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\adia\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   # == 2.2.3\n",
    "import numpy as np    # == 2.2.6\n",
    "import torch \n",
    "import scipy.stats\n",
    "import statsmodels.tsa.api as tsa \n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import antropy \n",
    "import sklearn\n",
    "from tsfresh.feature_extraction import feature_calculators as tsfresh_fe\n",
    "import ruptures as rpt\n",
    "\n",
    "import lightgbm as lgb  # == 4.6.0\n",
    "import catboost as cat\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import inspect\n",
    "import typing\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5044c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n",
      "\n",
      "cli version: 6.6.1\n",
      "available ram: 15.73 gb\n",
      "available cpu: 16 core\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "\n",
    "# Load the Crunch Toolings\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ce5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", InterpolationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter('ignore', np.exceptions.RankWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b135c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "def get_logger(name: str, log_dir: Path, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    获取一个配置好的 logger 实例，它会生成带时间戳的详细日志。\n",
    "    \"\"\"\n",
    "    # 确保日志目录存在\n",
    "    log_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # 1. 创建带时间戳的详细日志文件名\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    detail_log_file = log_dir / f'{name.lower()}_{timestamp}.log'\n",
    "\n",
    "    # 2. 为 logger 设置一个唯一的名称（基于时间戳），避免冲突\n",
    "    logger = logging.getLogger(f\"{name}-{timestamp}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # 防止将日志消息传播到根 logger\n",
    "    logger.propagate = False\n",
    "\n",
    "    # 如果已经有处理器，则不重复添加\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # 3. 创建详细日志的文件处理器\n",
    "    detail_handler = logging.FileHandler(detail_log_file, mode='a', encoding='utf-8')\n",
    "    detail_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    detail_handler.setFormatter(detail_formatter)\n",
    "    logger.addHandler(detail_handler)\n",
    "    \n",
    "    # 4. 创建控制台处理器\n",
    "    # 控制台 - INFO级别 (受verbose控制)\n",
    "    if verbose:\n",
    "        info_handler = logging.StreamHandler(sys.stdout)\n",
    "        info_handler.setLevel(logging.INFO)\n",
    "        info_handler.addFilter(lambda record: record.levelno == logging.INFO)\n",
    "        info_formatter = logging.Formatter('%(message)s')\n",
    "        info_handler.setFormatter(info_formatter)\n",
    "        logger.addHandler(info_handler)\n",
    "\n",
    "    # 控制台 - WARNING及以上 (始终输出)\n",
    "    warn_handler = logging.StreamHandler(sys.stdout)\n",
    "    warn_handler.setLevel(logging.WARNING)\n",
    "    warn_formatter = logging.Formatter('%(levelname)s: %(message)s')\n",
    "    warn_handler.setFormatter(warn_formatter)\n",
    "    logger.addHandler(warn_handler)\n",
    "\n",
    "    return logger, detail_log_file # 返回 logger 和日志文件路径 \n",
    "\n",
    "logger = None\n",
    "log_file_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a16c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "class Config:\n",
    "    # --- Feature Engineer ---\n",
    "    N_JOBS = -1\n",
    "    SEED = 42\n",
    "\n",
    "    # --- Data Enhancement ---\n",
    "    # 数据增强配置，指定要加载的增强数据ID列表\n",
    "    # 如果为'0'，则只使用原始数据\n",
    "    ENHANCEMENT_IDS = [\"0\"] \n",
    "\n",
    "    # --- Model ---\n",
    "    TRAIN_STRATEGY = 'cv'   # 'cv' or 'multi'\n",
    "    MODEL = ['LGB', 'CAT', 'XGB']  # ['LGB']\n",
    "    LGBM_PARAMS = {\n",
    "        # --- 基础设定 ---\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 3600, \n",
    "        'learning_rate': 0.005,\n",
    "        'num_leaves': 29,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': N_JOBS,\n",
    "        'verbosity': 0, \n",
    "\n",
    "        # --- 正则化和采样 ---\n",
    "        'reg_alpha': 3,            # L1 正则化\n",
    "        'reg_lambda': 3,           # L2 正则化\n",
    "        # 'min_child_samples': 50,   # 叶子节点样本量\n",
    "        'colsample_bytree': 0.8,   # 构建树时对特征的列采样率\n",
    "        'subsample': 0.8,          # 训练样本的采样率\n",
    "    }\n",
    "    CAT_PARAMS = {\n",
    "        # --- 基础设定 ---\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'grow_policy': 'Lossguide',\n",
    "        # 'task_type': 'GPU',\n",
    "        'iterations': 3600, \n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 29,\n",
    "        'random_seed': 114514,\n",
    "        'thread_count': N_JOBS,\n",
    "        \n",
    "        # --- 正则化和采样 ---\n",
    "        'subsample': 0.8,\n",
    "        # 'rsm': 0.7,\n",
    "        'l2_leaf_reg': 9,\n",
    "    }\n",
    "    XGB_PARAMS = {\n",
    "        # --- 基础设定 ---\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        # 'device': 'cuda', \n",
    "        'n_estimators': 3600,\n",
    "        'learning_rate': 0.0075,\n",
    "        'max_leaves': 29,\n",
    "        'random_state': 2025,\n",
    "        'n_jobs': N_JOBS,\n",
    "        'verbosity': 0, \n",
    "        \n",
    "        # --- 正则化和采样 ---\n",
    "        'reg_alpha': 3,          # L1 正则化\n",
    "        'reg_lambda': 3,         # L2 正则化\n",
    "        'colsample_bytree': 0.8, # 构建树时对特征的列采样率\n",
    "        'subsample': 0.8,        # 训练样本的采样率\n",
    "    }\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    # 设置为 >0 启用早停；设置为 0 禁用早停\n",
    "    EARLY_STOPPING_ROUNDS = 0\n",
    "\n",
    "    # --- CV ---\n",
    "    CV_PARAMS = [{\n",
    "        'n_splits': 5,\n",
    "        'shuffle': True,\n",
    "        'random_state': 42\n",
    "    }, {\n",
    "        'n_splits': 5,\n",
    "        'shuffle': True,\n",
    "        'random_state': 114514\n",
    "    }, {\n",
    "        'n_splits': 5,\n",
    "        'shuffle': True,\n",
    "        'random_state': 2025\n",
    "    } ]\n",
    "\n",
    "    # --- Exclude Features ---\n",
    "    # 在这里定义不希望在 \"一键生成所有特征\" 时运行的函数名称\n",
    "    # 如果要运行这些特征，需要在命令行中通过 --funcs 参数明确指定\n",
    "    # 例如: python -m experiment.main gen-feats --funcs ar_model_features\n",
    "    EXPERIMENTAL_FEATURES = [\n",
    "    ] \n",
    "\n",
    "    # --- Top Features ---\n",
    "    TOP_FEATURES = [\n",
    "    ]\n",
    "\n",
    "    # --- Interaction Operators ---\n",
    "    OPERATOR_FLAGS = [\n",
    "        'mul', 'sqmul', 'sub', 'add', 'div', 'sq'\n",
    "    ]\n",
    "    FEAT_FLAGS = [\n",
    "        '_left', '_right', '_whole', '_diff', '_ratio', '_contribution_left', '_contribution_right', '_ratio_to_whole_left', '_ratio_to_whole_right'\n",
    "    ]\n",
    "\n",
    "    # --- Remain Features ---\n",
    "    REMAIN_FEATURES = [\n",
    "        'sqmul_RAW_1_stats_mean_whole_RAW_3_detrend_volatility_normalized_whole',\n",
    "        'mul_RAW_3_detrend_volatility_normalized_whole_RAW_1_stats_std_whole',\n",
    "        'mul_RAW_1_stats_cv_whole_RAW_1_stats_std_whole',\n",
    "        'sub_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_right_CUMSUM_4_autocorr_lag1_whole',\n",
    "        'div_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left_RAW_8_agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max_ratio_to_whole_right',\n",
    "        'div_RAW_2_ad_stat_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'div_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_diff',\n",
    "        'add_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_8_index_mass_quantile_q_0_1_right',\n",
    "        'mul_CUMSUM_2_ad_pvalue_DIFF_8_change_quantiles_f_agg_var_isabs_True_qh_0_4_ql_0_2_whole',\n",
    "        'div_RAW_8_ratio_value_number_to_time_series_length_whole_CUMSUM_2_ad_pvalue',\n",
    "        'div_RAW_1_stats_median_right_CUMSUM_1_stats_max_diff',\n",
    "        'mul_RAW_2_bartlett_stat_DIFF_2_jb_pvalue_left',\n",
    "        'sqmul_CUMSUM_1_stats_max_ratio_to_whole_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "        'mul_RAW_8_ratio_beyond_r_sigma_3_left_CUMSUM_1_stats_theil_sen_slope_whole',\n",
    "        'mul_RAW_8_ratio_beyond_r_sigma_3_left_CUMSUM_1_stats_max_diff',\n",
    "        'div_RAW_8_agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max_ratio_to_whole_right_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left',\n",
    "        'div_RAW_7_sample_entropy_left_DIFF_2_jb_pvalue_left',\n",
    "        'mul_RAW_8_agg_linear_trend_attr_slope_chunk_len_10_f_agg_mean_contribution_left_CUMSUM_1_stats_max_diff',\n",
    "        'sqmul_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_quantile_0_4_contribution_left',\n",
    "        'sub_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_7_sample_entropy_left',\n",
    "        'add_RAW_7_sample_entropy_left_CUMSUM_8_first_location_of_maximum_whole',\n",
    "        'sub_DIFF_7_hjorth_complexity_contribution_right_RAW_7_higuchi_fd_contribution_right',\n",
    "        'div_RAW_1_stats_median_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_whole',\n",
    "        'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "        'sqmul_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left_RAW_8_index_mass_quantile_q_0_1_right',\n",
    "        'div_RAW_8_benford_correlation_whole_CUMSUM_2_ad_pvalue',\n",
    "        'div_CUMSUM_8_ar_coefficient_coeff_2_k_10_ratio_RAW_1_stats_kurt_left',\n",
    "        'sqmul_RAW_7_sample_entropy_left_RAW_8_count_above_0_whole',\n",
    "        'mul_CUMSUM_2_wilcoxon_stat_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_4_ratio_to_whole_right',\n",
    "        'add_RAW_8_ratio_beyond_r_sigma_3_left_DIFF_7_hjorth_complexity_ratio_to_whole_right',\n",
    "        'sqmul_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left',\n",
    "        'add_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_index_mass_quantile_q_0_8_left',\n",
    "        'sqmul_RAW_8_linear_trend_attr_pvalue_ratio_to_whole_left_CUMSUM_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0_ratio_to_whole_right',\n",
    "        'add_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left',\n",
    "        'div_DIFF_8_change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4_whole_DIFF_8_change_quantiles_f_agg_var_isabs_True_qh_0_4_ql_0_2_whole',\n",
    "        'div_RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2_diff_CUMSUM_9_ar_residuals_s1_pred_mean',\n",
    "        'div_RAW_1_stats_min_whole_CUMSUM_8_friedrich_coefficients_coeff_3_m_3_r_30_ratio_to_whole_left',\n",
    "        'sub_CUMSUM_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0_whole_CUMSUM_2_ks_pvalue',\n",
    "        'add_DIFF_2_bartlett_pvalue_DIFF_8_ar_coefficient_coeff_2_k_10_left',\n",
    "        'sub_RAW_1_stats_std_whole_RAW_8_ratio_value_number_to_time_series_length_whole',\n",
    "        'mul_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left',\n",
    "        'div_RAW_3_detrend_volatility_normalized_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'sqmul_RAW_2_ks_stat_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "        'mul_RAW_8_index_mass_quantile_q_0_1_right_RAW_10_rpt_cost_cosine_whole',\n",
    "        'mul_RAW_10_rpt_cost_cosine_whole_RAW_1_stats_median_ratio',\n",
    "        'div_RAW_2_bartlett_stat_CUMSUM_7_cond_entropy_whole',\n",
    "        'div_RAW_1_stats_mean_whole_RAW_8_ratio_beyond_r_sigma_3_left',\n",
    "        'div_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_2_whole_RAW_1_stats_min_whole',\n",
    "        'CUMSUM_8_benford_correlation_left',\n",
    "        'sqmul_CUMSUM_3_detrend_volatility_normalized_right_RAW_8_quantile_0_4_right',\n",
    "        'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_CUMSUM_5_dominant_freq_ratio_to_whole_right',\n",
    "        'DIFF_7_spectral_entropy_ratio_to_whole_left',\n",
    "        'div_RAW_7_approx_entropy_left_CUMSUM_2_ad_pvalue',\n",
    "        'div_RAW_7_sample_entropy_left_RAW_8_ratio_beyond_r_sigma_1_left',\n",
    "        'div_RAW_7_sample_entropy_whole_RAW_8_ratio_beyond_r_sigma_1_5_whole',\n",
    "        'sqmul_RAW_8_linear_trend_attr_pvalue_ratio_to_whole_left_DIFF_8_change_quantiles_f_agg_var_isabs_False_qh_1_0_ql_0_2_ratio',\n",
    "        'mul_RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4_ratio_to_whole_right_CUMSUM_2_wilcoxon_stat',\n",
    "        'sub_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_8_ratio_beyond_r_sigma_1_left',\n",
    "        'mul_RAW_8_ratio_value_number_to_time_series_length_ratio_RAW_8_ratio_value_number_to_time_series_length_whole',\n",
    "        'div_CUMSUM_1_stats_theil_sen_slope_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'mul_CUMSUM_2_ad_pvalue_RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0_right',\n",
    "        'mul_CUMSUM_3_detrend_volatility_normalized_right_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_2_whole',\n",
    "        'div_RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4_left_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'div_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left_DIFF_8_change_quantiles_f_agg_var_isabs_False_qh_1_0_ql_0_2_ratio',\n",
    "        'div_DIFF_2_levene_pvalue_RAW_1_stats_median_right',\n",
    "        'mul_RAW_8_ratio_value_number_to_time_series_length_diff_CUMSUM_3_trend_normalized_slope_whole',\n",
    "        'sub_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left_RAW_8_quantile_0_1_contribution_right',\n",
    "        'sqmul_RAW_8_agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max_ratio_to_whole_right_RAW_8_ratio_beyond_r_sigma_1_5_whole',\n",
    "        'div_RAW_2_ks_stat_RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0_right',\n",
    "        'add_RAW_7_sample_entropy_left_DIFF_2_bartlett_pvalue',\n",
    "        'sqmul_RAW_8_quantile_0_4_whole_RAW_7_katz_fd_whole',\n",
    "        'mul_RAW_8_agg_linear_trend_attr_slope_chunk_len_10_f_agg_mean_contribution_left_DIFF_2_jb_pvalue_left',\n",
    "        'add_RAW_7_sample_entropy_left_RAW_2_bartlett_pvalue',\n",
    "        'sqmul_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_right_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left',\n",
    "        'sqmul_RAW_2_shapiro_pvalue_whole_RAW_2_bartlett_stat',\n",
    "        'sqmul_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_4_ratio_to_whole_right_CUMSUM_7_hjorth_complexity_whole',\n",
    "        'sub_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_DIFF_7_sample_entropy_right',\n",
    "        'sqmul_CUMSUM_7_cond_entropy_whole_RAW_1_stats_kurt_left',\n",
    "        'add_RAW_8_count_above_0_whole_CUMSUM_8_change_quantiles_f_agg_mean_isabs_True_qh_1_0_ql_0_4_left',\n",
    "        'mul_DIFF_2_levene_pvalue_CUMSUM_3_detrend_volatility_normalized_right',\n",
    "        'div_RAW_1_stats_kurt_left_RAW_1_stats_std_whole',\n",
    "        'add_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_ratio_value_number_to_time_series_length_whole',\n",
    "        'div_RAW_8_quantile_0_4_right_RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_4_whole',\n",
    "        'mul_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left_RAW_8_quantile_0_4_contribution_left',\n",
    "        'DIFF_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2_ratio_to_whole_left',\n",
    "        'sqmul_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left_DIFF_8_benford_correlation_whole',\n",
    "        'div_RAW_8_agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max_ratio_to_whole_right_RAW_8_agg_linear_trend_attr_slope_chunk_len_10_f_agg_mean_contribution_left',\n",
    "        'mul_RAW_8_benford_correlation_whole_RAW_8_count_above_0_whole',\n",
    "        'div_RAW_8_ratio_value_number_to_time_series_length_whole_RAW_7_katz_fd_whole',\n",
    "        'sub_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_ratio_to_whole_left',\n",
    "        'RAW_8_ratio_beyond_r_sigma_0_5_contribution_left',\n",
    "        'sqmul_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_4_contribution_left_CUMSUM_7_katz_fd_whole',\n",
    "        'sqmul_RAW_8_friedrich_coefficients_coeff_3_m_3_r_30_ratio_CUMSUM_8_friedrich_coefficients_coeff_3_m_3_r_30_ratio_to_whole_left',\n",
    "        'sub_RAW_1_stats_min_whole_RAW_7_approx_entropy_whole',\n",
    "        'mul_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left',\n",
    "        'div_RAW_2_bartlett_pvalue_CUMSUM_2_kpss_pvalue_ratio_to_whole_left',\n",
    "        'add_DIFF_2_bartlett_pvalue_RAW_7_sample_entropy_whole',\n",
    "        'sub_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_right_RAW_2_shapiro_pvalue_whole',\n",
    "        'CUMSUM_2_adf_pvalue_ratio_to_whole_right',\n",
    "        'div_DIFF_2_bartlett_pvalue_RAW_8_agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_max_ratio_to_whole_left',\n",
    "        'div_RAW_7_sample_entropy_left_CUMSUM_2_ad_pvalue',\n",
    "        'div_CUMSUM_2_adf_stat_diff_RAW_8_friedrich_coefficients_coeff_3_m_3_r_30_ratio',\n",
    "        'div_CUMSUM_3_detrend_volatility_normalized_right_CUMSUM_1_stats_range_right',\n",
    "        'div_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'add_DIFF_7_hjorth_complexity_contribution_right_RAW_1_stats_mean_right',\n",
    "        'div_RAW_1_stats_median_right_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio',\n",
    "    ]\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c64a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "# --- 时序变换函数注册表 ---\n",
    "TRANSFORM_REGISTRY = {}\n",
    "\n",
    "def register_transform(_func=None, *, output_mode_names=[]):\n",
    "    \"\"\"一个用于注册时序变换函数的装饰器。\"\"\"\n",
    "    def decorator_register(func):\n",
    "        TRANSFORM_REGISTRY[func.__name__] = {\n",
    "            \"func\": func, \n",
    "            \"output_mode_names\": output_mode_names\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    if _func is None:\n",
    "        # Used as @register_transform(output_mode_names=...)\n",
    "        return decorator_register\n",
    "    else:\n",
    "        # Used as @register_transform\n",
    "        return decorator_register(_func)\n",
    "\n",
    "@register_transform(output_mode_names=['RAW'])\n",
    "def no_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    原始时序\n",
    "    \"\"\"\n",
    "    result_dfs = []\n",
    "    result_dfs.append(X_df)\n",
    "\n",
    "    return result_dfs\n",
    "\n",
    "@register_transform(output_mode_names=['CUMSUM'])\n",
    "def cumsum_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    累计和变换\n",
    "    Args:\n",
    "        X_df: 输入数据框，包含MultiIndex (id, time) 和 columns ['value', 'period']\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: 包含一个数据框的列表 [累计和值]\n",
    "    \"\"\"\n",
    "    X_df_sorted = X_df.sort_index()\n",
    "    result_dfs = []\n",
    "\n",
    "    result_df = X_df_sorted.copy()\n",
    "    result_df['value'] = np.nan\n",
    "    \n",
    "    for series_id in X_df_sorted.index.get_level_values('id').unique():\n",
    "        series_data = X_df_sorted.loc[series_id]\n",
    "        series_data = series_data.sort_index()\n",
    "        values = series_data['value'].values\n",
    "        \n",
    "        cumsum_values = np.cumsum(values)\n",
    "        result_df.loc[series_id, 'value'] = cumsum_values\n",
    "    \n",
    "    result_dfs.append(result_df)\n",
    "    return result_dfs\n",
    "\n",
    "@register_transform(output_mode_names=['DIFF'])\n",
    "def diff_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    差分变换\n",
    "    Args:\n",
    "        X_df: 输入数据框，包含MultiIndex (id, time) 和 columns ['value', 'period']\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: 包含一个数据框的列表 [差分值]\n",
    "    \"\"\"\n",
    "    X_df_sorted = X_df.sort_index()\n",
    "    result_dfs = []\n",
    "    \n",
    "    result_df = X_df_sorted.copy()\n",
    "    result_df['value'] = np.nan\n",
    "    \n",
    "    for series_id in X_df_sorted.index.get_level_values('id').unique():\n",
    "        series_data = X_df_sorted.loc[series_id]\n",
    "        series_data = series_data.sort_index()\n",
    "        values = series_data['value'].values\n",
    "        \n",
    "        diff_values = np.diff(values, prepend=0)  # 使用prepend=0使长度保持一致\n",
    "        result_df.loc[series_id, 'value'] = diff_values\n",
    "    \n",
    "    result_dfs.append(result_df)\n",
    "    return result_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e066617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "# --- 特征函数注册表 ---\n",
    "FEATURE_REGISTRY = {}\n",
    "\n",
    "def register_feature(_func=None, *, parallelizable=True, func_id=\"\"):\n",
    "    \"\"\"一个用于注册特征函数的装饰器，可以标记特征是否可并行化。\"\"\"\n",
    "    def decorator_register(func):\n",
    "        FEATURE_REGISTRY[func.__name__] = {\n",
    "            \"func\": func, \n",
    "            \"parallelizable\": parallelizable,\n",
    "            \"func_id\": func_id\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    if _func is None:\n",
    "        # Used as @register_feature(parallelizable=...)\n",
    "        return decorator_register\n",
    "    else:\n",
    "        # Used as @register_feature\n",
    "        return decorator_register(_func)\n",
    "\n",
    "def _add_diff_ratio_feats(feats: dict, name: str, left, right):\n",
    "    \"\"\"\n",
    "    一个辅助函数，用于向特征字典中添加差异和比例特征。\n",
    "\n",
    "    Args:\n",
    "        feats (dict): 要更新的特征字典。\n",
    "        name (str): 特征的基础名称 (例如, 'stats_mean')。\n",
    "        left (float): 左侧分段的特征值。\n",
    "        right (float): 右侧分段的特征值。\n",
    "    \"\"\"\n",
    "    # check nan/None \n",
    "    if np.isnan(left) or np.isnan(right) or left is None or right is None:\n",
    "        feats[f'{name}_diff'] = 0.0\n",
    "        feats[f'{name}_ratio'] = 0.0\n",
    "        return\n",
    "    # 做差\n",
    "    feats[f'{name}_diff'] = right - left\n",
    "    # 做比\n",
    "    feats[f'{name}_ratio'] = right / (left + 1e-6)\n",
    "\n",
    "def _add_contribution_ratio_feats(feats: dict, name: str, left, right, whole):\n",
    "    \"\"\"\n",
    "    一个辅助函数，用于向特征字典中添加贡献度和与整体的比例特征。\n",
    "\n",
    "    Args:\n",
    "        feats (dict): 要更新的特征字典。\n",
    "        name (str): 特征的基础名称 (例如, 'stats_mean')。\n",
    "        left (float): 左侧分段的特征值。\n",
    "        right (float): 右侧分段的特征值。\n",
    "        whole (float): 整个序列的特征值。\n",
    "    \"\"\"\n",
    "    # check nan/None \n",
    "    if np.isnan(left) or np.isnan(right) or np.isnan(whole) or left is None or right is None or whole is None :\n",
    "        feats[f'{name}_contribution_left'] = 0.0\n",
    "        feats[f'{name}_contribution_right'] = 0.0\n",
    "        feats[f'{name}_ratio_to_whole_left'] = 0.0\n",
    "        feats[f'{name}_ratio_to_whole_right'] = 0.0\n",
    "        return\n",
    "    # 特征贡献度\n",
    "    feats[f'{name}_contribution_left'] = left / (left + right + 1e-6)\n",
    "    feats[f'{name}_contribution_right'] = right / (left + right + 1e-6)\n",
    "    # 与整体特征的比例\n",
    "    feats[f'{name}_ratio_to_whole_left'] = left / (whole + 1e-6)\n",
    "    feats[f'{name}_ratio_to_whole_right'] = right / (whole + 1e-6)\n",
    "\n",
    "# --- 1. 分布统计特征 ---\n",
    "def safe_cv(s):\n",
    "    s = pd.Series(s)\n",
    "    m = s.mean()\n",
    "    std = s.std()\n",
    "    return std / m if abs(m) > 1e-6 else 0.0\n",
    "\n",
    "def rolling_std_mean(s, window=50):\n",
    "    s = pd.Series(s)\n",
    "    if len(s) < window:\n",
    "        return 0.0\n",
    "    return s.rolling(window=window).std().dropna().mean()\n",
    "\n",
    "def slope_theil_sen(s):\n",
    "    s = pd.Series(s)\n",
    "    if len(s) < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        slope, intercept, _, _ = scipy.stats.theilslopes(s.values, np.arange(len(s)))\n",
    "        return slope\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "class STATSFeatureExtractor:\n",
    "    def __init__(self, selected_features):\n",
    "        # 所有可用的func类及其名称\n",
    "        self.func_classes = {\n",
    "            'mean': np.mean,\n",
    "            'median': np.median,\n",
    "            'max': np.max,\n",
    "            'min': np.min,\n",
    "            'range': lambda x: np.max(x) - np.min(x),\n",
    "            'std': np.std,\n",
    "            'skew': scipy.stats.skew,\n",
    "            'kurt': scipy.stats.kurtosis,\n",
    "            'cv': safe_cv,\n",
    "            'mean_of_rolling_std': rolling_std_mean,\n",
    "            'theil_sen_slope': slope_theil_sen,\n",
    "        }\n",
    "        self.selected_features = selected_features\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        self.signal = np.asarray(signal)\n",
    "        self.n = len(signal)\n",
    "\n",
    "    def calculate(self, func, start, end):\n",
    "        result = func(self.signal[start:end])\n",
    "        if isinstance(result, float) or isinstance(result, int):\n",
    "            return result\n",
    "        else:\n",
    "            return result.item()\n",
    "\n",
    "    def should_keep(self, name):\n",
    "        if self.selected_features is None:\n",
    "            return True\n",
    "        keep = f'stats_{name}' in self.selected_features\n",
    "        return keep\n",
    "\n",
    "    def extract(self, signal, boundary):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            signal: 1D numpy array，单变量时间序列\n",
    "            boundary: int，分割点\n",
    "        输出：\n",
    "            result: dict，格式为 {func_name: {'left': value, 'right': value}}\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        result = {}\n",
    "        for name, func in self.func_classes.items():\n",
    "            if not self.should_keep(name):\n",
    "                continue\n",
    "            try:\n",
    "                left = self.calculate(func, 0, boundary)\n",
    "                right = self.calculate(func, boundary, n)\n",
    "                whole = self.calculate(func, 0, n)\n",
    "                # diff = right - left\n",
    "                # ratio = right / (left + 1e-6)\n",
    "            except Exception:\n",
    "                left = None\n",
    "                right = None\n",
    "                whole = None\n",
    "                # diff = None\n",
    "                # ratio = None\n",
    "            # Move to _add_diff_ratio_feats, 'diff': diff, 'ratio': ratio\n",
    "            result[name] = {'left': left, 'right': right, 'whole': whole}   \n",
    "        return result\n",
    "\n",
    "@register_feature(func_id=\"1\")\n",
    "def distribution_stats_features(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    \"\"\"统计量的分段值、Diff值、Ratio值\"\"\"\n",
    "    value = u['value'].values.astype(np.float32)\n",
    "    period = u['period'].values.astype(np.float32)\n",
    "    boundary = np.where(np.diff(period) != 0)[0].item()\n",
    "    feats = {}\n",
    "\n",
    "    extractor = STATSFeatureExtractor(selected_features)\n",
    "    extractor.fit(value)\n",
    "    features = extractor.extract(value, boundary)\n",
    "\n",
    "    feats = {}\n",
    "    for k, v in features.items():\n",
    "        for seg, value in v.items():\n",
    "            feats[f'stats_{k}_{seg}'] = value\n",
    "        _add_diff_ratio_feats(feats, f'stats_{k}', v['left'], v['right'])\n",
    "        _add_contribution_ratio_feats(feats, f'stats_{k}', v['left'], v['right'], v['whole'])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "    \n",
    "# --- 2. 假设检验统计量特征 ---\n",
    "@register_feature(func_id=\"2\")\n",
    "def test_stats_features_first(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    def should_keep(candidates):\n",
    "        if selected_features is None:\n",
    "            return True\n",
    "        keep = any(c in selected_features for c in candidates)\n",
    "        return keep\n",
    "\n",
    "    \"\"\"假设检验统计量\"\"\"\n",
    "    # KS检验\n",
    "    if should_keep(['ks_stat', 'ks_pvalue']):\n",
    "        ks_stat, ks_pvalue = scipy.stats.ks_2samp(s1, s2)\n",
    "        feats['ks_stat'] = ks_stat\n",
    "        feats['ks_pvalue'] = -ks_pvalue\n",
    "\n",
    "    # T检验\n",
    "    if should_keep(['ttest_stat', 'ttest_pvalue']):\n",
    "        ttest_stat, ttest_pvalue = scipy.stats.ttest_ind(s1, s2, equal_var=False)\n",
    "        feats['ttest_stat'] = ttest_stat\n",
    "        feats['ttest_pvalue'] = -ttest_pvalue if not np.isnan(ttest_pvalue) else 1\n",
    "\n",
    "    # AD检验\n",
    "    if should_keep(['ad_stat', 'ad_pvalue']):\n",
    "        ad_stat, _, ad_pvalue = scipy.stats.anderson_ksamp([s1.to_numpy(), s2.to_numpy()])\n",
    "        feats['ad_stat'] = ad_stat\n",
    "        feats['ad_pvalue'] = -ad_pvalue\n",
    "\n",
    "    # Mann-Whitney U检验 (非参数，不假设分布)\n",
    "    if should_keep(['mannwhitney_stat', 'mannwhitney_pvalue']):\n",
    "        mw_stat, mw_pvalue = scipy.stats.mannwhitneyu(s1, s2, alternative='two-sided')\n",
    "        feats['mannwhitney_stat'] = mw_stat if not np.isnan(mw_stat) else 0\n",
    "        feats['mannwhitney_pvalue'] = -mw_pvalue if not np.isnan(mw_pvalue) else 1\n",
    "        \n",
    "    # Wilcoxon秩和检验\n",
    "    if should_keep(['wilcoxon_stat', 'wilcoxon_pvalue']):\n",
    "        w_stat, w_pvalue = scipy.stats.ranksums(s1, s2)\n",
    "        feats['wilcoxon_stat'] = w_stat if not np.isnan(w_stat) else 0\n",
    "        feats['wilcoxon_pvalue'] = -w_pvalue if not np.isnan(w_pvalue) else 1\n",
    "\n",
    "    # Levene检验\n",
    "    if should_keep(['levene_stat', 'levene_pvalue']):\n",
    "        levene_stat, levene_pvalue = scipy.stats.levene(s1, s2)\n",
    "        feats['levene_stat'] = levene_stat if not np.isnan(levene_stat) else 0\n",
    "        feats['levene_pvalue'] = -levene_pvalue if not np.isnan(levene_pvalue) else 1\n",
    "    \n",
    "    # Bartlett检验\n",
    "    if should_keep(['bartlett_stat', 'bartlett_pvalue']):\n",
    "        bartlett_stat, bartlett_pvalue = scipy.stats.bartlett(s1, s2)\n",
    "        feats['bartlett_stat'] = bartlett_stat if not np.isnan(bartlett_stat) else 0\n",
    "        feats['bartlett_pvalue'] = -bartlett_pvalue if not np.isnan(bartlett_pvalue) else 1\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 2. 假设检验统计量特征 ---\n",
    "@register_feature(func_id=\"2\")\n",
    "def test_stats_features_second(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    def should_keep(candidates):\n",
    "        if selected_features is None:\n",
    "            return True\n",
    "        keep = any(c in selected_features for c in candidates)\n",
    "        return keep\n",
    "    \n",
    "    # \"\"\"分段假设检验的分段值、Diff值、Ratio值\"\"\"\n",
    "    # Shapiro-Wilk检验\n",
    "    if should_keep(['shapiro_stat', 'shapiro_pvalue']):\n",
    "        sw1_stat, sw1_pvalue, sw2_stat, sw2_pvalue, sw_whole_stat, sw_whole_pvalue = (np.nan,)*6\n",
    "        try:\n",
    "            sw1_stat, sw1_pvalue = scipy.stats.shapiro(s1)\n",
    "            sw2_stat, sw2_pvalue = scipy.stats.shapiro(s2)\n",
    "            sw_whole_stat, sw_whole_pvalue = scipy.stats.shapiro(s_whole)\n",
    "        except Exception as e:\n",
    "            pass \n",
    "        feats['shapiro_pvalue_left'] = sw1_pvalue\n",
    "        feats['shapiro_pvalue_right'] = sw2_pvalue\n",
    "        feats['shapiro_pvalue_whole'] = sw_whole_pvalue\n",
    "        # _add_diff_ratio_feats(feats, 'shapiro_pvalue', sw1_pvalue, sw2_pvalue)\n",
    "        _add_contribution_ratio_feats(feats, 'shapiro_pvalue', sw1_pvalue, sw2_pvalue, sw_whole_pvalue)\n",
    "\n",
    "    # Jarque-Bera检验差异\n",
    "    if should_keep(['jb_stat', 'jb_pvalue']):\n",
    "        jb1_stat, jb1_pvalue, jb2_stat, jb2_pvalue, jb_whole_stat, jb_whole_pvalue = (np.nan,)*6\n",
    "        try:\n",
    "            jb1_stat, jb1_pvalue = scipy.stats.jarque_bera(s1)\n",
    "            jb2_stat, jb2_pvalue = scipy.stats.jarque_bera(s2)\n",
    "            jb_whole_stat, jb_whole_pvalue = scipy.stats.jarque_bera(s_whole)\n",
    "        except Exception as e:\n",
    "            pass \n",
    "        feats['jb_pvalue_left'] = jb1_pvalue\n",
    "        feats['jb_pvalue_right'] = jb2_pvalue\n",
    "        # feats['jb_pvalue_whole'] = jb_whole_pvalue\n",
    "        _add_diff_ratio_feats(feats, 'jb_pvalue', jb1_pvalue, jb2_pvalue)\n",
    "        # _add_contribution_ratio_feats(feats, 'jb_pvalue', jb1_pvalue, jb2_pvalue, jb_whole_pvalue)\n",
    "\n",
    "    # KPSS检验\n",
    "    def extract_kpss_features(s):\n",
    "        if len(s) <= 12:\n",
    "            return {'p': 0.1, 'stat': 0.0, 'lag': 0, 'crit_5pct': 0.0, 'reject_5pct': 0}\n",
    "        kpss = tsa.stattools.kpss(s, regression='c', nlags='auto')\n",
    "        stat, p, lag, crit = kpss\n",
    "        crit_5pct = crit['5%']\n",
    "        return {\n",
    "            'p': p,\n",
    "            'stat': stat,\n",
    "            'lag': lag,\n",
    "            'crit_5pct': crit_5pct,\n",
    "            'reject_5pct': int(stat > crit_5pct)  # KPSS原假设是\"平稳\"，所以 > 临界值 拒绝平稳\n",
    "        }\n",
    "    if should_keep(['kpss_stat', 'kpss_pvalue']):\n",
    "        kpss1_stat, kpss1_pvalue, kpss2_stat, kpss2_pvalue, kpss_whole_stat, kpss_whole_pvalue = (np.nan,)*6\n",
    "        try:\n",
    "            k1 = extract_kpss_features(s1)\n",
    "            k2 = extract_kpss_features(s2)\n",
    "            k_whole = extract_kpss_features(s_whole)\n",
    "\n",
    "            kpss1_stat, kpss1_pvalue = k1['stat'], k1['p']\n",
    "            kpss2_stat, kpss2_pvalue = k2['stat'], k2['p']\n",
    "            kpss_whole_stat, kpss_whole_pvalue = k_whole['stat'], k_whole['p']\n",
    "        except Exception as e:\n",
    "            pass \n",
    "        # feats['kpss_pvalue_left'] = kpss1_pvalue\n",
    "        # feats['kpss_pvalue_right'] = kpss2_pvalue\n",
    "        # feats['kpss_pvalue_whole'] = kpss_whole_pvalue\n",
    "        # _add_diff_ratio_feats(feats, 'kpss_pvalue', kpss1_pvalue, kpss2_pvalue)\n",
    "        _add_contribution_ratio_feats(feats, 'kpss_pvalue', kpss1_pvalue, kpss2_pvalue, kpss_whole_pvalue)\n",
    "        # feats['kpss_stat_left'] = kpss1_stat\n",
    "        # feats['kpss_stat_right'] = kpss2_stat\n",
    "        # feats['kpss_stat_whole'] = kpss_whole_stat\n",
    "        # _add_diff_ratio_feats(feats, 'kpss_stat', kpss1_stat, kpss2_stat)\n",
    "        # _add_contribution_ratio_feats(feats, 'kpss_stat', kpss1_stat, kpss2_stat, kpss_whole_stat)\n",
    "\n",
    "    # 平稳性检验 (ADF)\n",
    "    def extract_adf_features(s):\n",
    "        if len(s) <= 12:\n",
    "            return {'p': np.nan, 'stat': np.nan, 'lag': np.nan, 'ic': np.nan, 'crit_5pct': np.nan, 'reject_5pct': 0}\n",
    "        adf = tsa.stattools.adfuller(s, autolag='AIC')\n",
    "        stat, p, lag, _, crit, ic = adf\n",
    "        crit_5pct = crit['5%']\n",
    "        return {\n",
    "            'p': p,\n",
    "            'stat': stat,\n",
    "            'lag': lag,\n",
    "            'ic': ic,\n",
    "            'crit_5pct': crit_5pct,\n",
    "            'reject_5pct': int(stat < crit_5pct)  # ADF 原假设是“非平稳”，stat < 临界值 ⇒ 拒绝非平稳 ⇒ 平稳\n",
    "        }\n",
    "    if should_keep(['adf_stat', 'adf_pvalue', 'adf_icbest']):\n",
    "        adf1_stat, adf1_pvalue, adf2_stat, adf2_pvalue, adf_whole_stat, adf_whole_pvalue = (np.nan,) * 6\n",
    "        adf1_ic, adf2_ic, adf_whole_ic = (np.nan,) * 3\n",
    "        try:\n",
    "            f1 = extract_adf_features(s1)\n",
    "            f2 = extract_adf_features(s2)\n",
    "            f_whole = extract_adf_features(s_whole)\n",
    "            adf1_stat, adf1_pvalue, adf1_ic = f1['stat'], f1['p'], f1['ic']\n",
    "            adf2_stat, adf2_pvalue, adf2_ic = f2['stat'], f2['p'], f2['ic']\n",
    "            adf_whole_stat, adf_whole_pvalue, adf_whole_ic = f_whole['stat'], f_whole['p'], f_whole['ic']\n",
    "        except Exception as e:\n",
    "            pass \n",
    "        feats['adf_pvalue_left'] = adf1_pvalue\n",
    "        feats['adf_pvalue_right'] = adf2_pvalue\n",
    "        feats['adf_pvalue_whole'] = adf_whole_pvalue\n",
    "        _add_diff_ratio_feats(feats, 'adf_pvalue', adf1_pvalue, adf2_pvalue)\n",
    "        _add_contribution_ratio_feats(feats, 'adf_pvalue', adf1_pvalue, adf2_pvalue, adf_whole_pvalue)\n",
    "        feats['adf_stat_left'] = adf1_stat\n",
    "        feats['adf_stat_right'] = adf2_stat\n",
    "        feats['adf_stat_whole'] = adf_whole_stat\n",
    "        _add_diff_ratio_feats(feats, 'adf_stat', adf1_stat, adf2_stat)\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_stat', adf1_stat, adf2_stat, adf_whole_stat)\n",
    "        # feats['adf_icbest_left'] = adf1_ic\n",
    "        # feats['adf_icbest_right'] = adf2_ic\n",
    "        # feats['adf_icbest_whole'] = adf_whole_ic\n",
    "        # _add_diff_ratio_feats(feats, 'adf_icbest', adf1_ic, adf2_ic)\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_icbest', adf1_ic, adf2_ic, adf_whole_ic)\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 3. 趋势特征 ---\n",
    "@register_feature(func_id=\"3\")\n",
    "def trend_features(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    def analyze_trend(series, seg):\n",
    "        \"\"\"分析时间序列的趋势特征\"\"\"\n",
    "        trend_feats = {}\n",
    "        x = np.arange(len(series))\n",
    "        \n",
    "        try:\n",
    "            # 1. 线性趋势分析 (使用scipy.stats.linregress)\n",
    "            slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, series)\n",
    "            # trend_feats[f'linear_trend_slope_{seg}'] = slope\n",
    "            # trend_feats[f'linear_trend_intercept_{seg}'] = intercept\n",
    "            # trend_feats[f'linear_trend_r_value_{seg}'] = r_value\n",
    "            trend_feats[f'linear_trend_r2_{seg}'] = r_value ** 2\n",
    "            # trend_feats[f'linear_trend_pvalue_{seg}'] = p_value\n",
    "            # trend_feats[f'linear_trend_std_err_{seg}'] = std_err\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in linear trend analysis for {seg}: {e}\")\n",
    "\n",
    "        #     trend_feats[f'linear_trend_slope_{seg}'] = 0\n",
    "        #     trend_feats[f'linear_trend_intercept_{seg}'] = 0\n",
    "        #     trend_feats[f'linear_trend_r_value_{seg}'] = 0\n",
    "            trend_feats[f'linear_trend_r2_{seg}'] = 0\n",
    "        #     trend_feats[f'linear_trend_pvalue_{seg}'] = 1\n",
    "        #     trend_feats[f'linear_trend_std_err_{seg}'] = 0\n",
    "\n",
    "        try:\n",
    "            # 2. 去趋势分析 (detrended features)\n",
    "            linear_trend = slope * x + intercept\n",
    "            detrended = series - linear_trend\n",
    "            # trend_feats[f'detrend_mean_{seg}'] = np.mean(detrended)\n",
    "            # trend_feats[f'detrend_volatility_{seg}'] = np.std(detrended)\n",
    "            trend_feats[f'detrend_volatility_normalized_{seg}'] = np.std(detrended) / (np.abs(np.mean(series)) + 1e-6)\n",
    "            # trend_feats[f'detrend_max_deviation_{seg}'] = np.max(np.abs(detrended))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in detrending analysis for {seg}: {e}\")\n",
    "\n",
    "            # trend_feats[f'detrend_mean_{seg}'] = 0\n",
    "            # trend_feats[f'detrend_volatility_{seg}'] = 0\n",
    "            trend_feats[f'detrend_volatility_normalized_{seg}'] = 0\n",
    "            # trend_feats[f'detrend_max_deviation_{seg}'] = 0\n",
    "\n",
    "        try:\n",
    "        #     # 3. 趋势变化率\n",
    "        #     trend_feats[f'trend_change_rate_{seg}'] = slope / (np.mean(np.abs(series)) + 1e-6)  # 相对变化率\n",
    "            trend_feats[f'trend_normalized_slope_{seg}'] = slope / (np.std(series) + 1e-6)  # 标准化斜率\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in trend change rate analysis for {seg}: {e}\")\n",
    "\n",
    "        #     trend_feats[f'trend_change_rate_{seg}'] = 0\n",
    "            trend_feats[f'trend_normalized_slope_{seg}'] = 0\n",
    "        \n",
    "        return trend_feats\n",
    "    \n",
    "    feats.update(analyze_trend(s1, 'left'))\n",
    "    feats.update(analyze_trend(s2, 'right'))\n",
    "    feats.update(analyze_trend(s_whole, 'whole'))\n",
    "    # _add_diff_ratio_feats(feats, 'linear_trend_slope', feats['linear_trend_slope_left'] if 'linear_trend_slope_left' in feats else 0, feats['linear_trend_slope_right'] if 'linear_trend_slope_right' in feats else 0)\n",
    "    _add_diff_ratio_feats(feats, 'linear_trend_r2', feats['linear_trend_r2_left'] if 'linear_trend_r2_left' in feats else 0, feats['linear_trend_r2_right'] if 'linear_trend_r2_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'linear_trend_pvalue', feats['linear_trend_pvalue_left'] if 'linear_trend_pvalue_left' in feats else 0, feats['linear_trend_pvalue_right'] if 'linear_trend_pvalue_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'detrend_mean', feats['detrend_mean_left'] if 'detrend_mean_left' in feats else 0, feats['detrend_mean_right'] if 'detrend_mean_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'detrend_volatility_normalized', feats['detrend_volatility_normalized_left'] if 'detrend_volatility_normalized_left' in feats else 0, feats['detrend_volatility_normalized_right'] if 'detrend_volatility_normalized_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'detrend_max_deviation', feats['detrend_max_deviation_left'] if 'detrend_max_deviation_left' in feats else 0, feats['detrend_max_deviation_right'] if 'detrend_max_deviation_right' in feats else 0)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 4. 振荡特征 ---\n",
    "@register_feature(func_id=\"4\")\n",
    "def oscillation_features(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0].reset_index(drop=True)\n",
    "    s2 = u['value'][u['period'] == 1].reset_index(drop=True)\n",
    "    s_whole = u['value'].reset_index(drop=True)\n",
    "    feats = {}\n",
    "\n",
    "    # def count_zero_crossings(series: pd.Series):\n",
    "    #     if len(series) < 2: return 0\n",
    "    #     centered_series = series - series.mean()\n",
    "    #     if centered_series.eq(0).all(): return 0\n",
    "    #     return np.sum(np.diff(np.sign(centered_series)) != 0)\n",
    "    # zc1, zc2, zc_whole = count_zero_crossings(s1), count_zero_crossings(s2), count_zero_crossings(s_whole)\n",
    "    # feats['zero_cross_left'] = zc1\n",
    "    # feats['zero_cross_right'] = zc2\n",
    "    # feats['zero_cross_whole'] = zc_whole\n",
    "    # _add_diff_ratio_feats(feats, 'zero_cross', zc1, zc2)\n",
    "    # _add_contribution_ratio_feats(feats, 'zero_cross', zc1, zc2, zc_whole)\n",
    "    \n",
    "    def autocorr_lag1(s):\n",
    "        if len(s) < 2: return 0.0\n",
    "        ac = s.autocorr(lag=1)\n",
    "        return ac if not np.isnan(ac) else 0.0\n",
    "    # ac1 = autocorr_lag1(s1)\n",
    "    # ac2 = autocorr_lag1(s2)\n",
    "    ac_whole = autocorr_lag1(s_whole)\n",
    "    # feats['autocorr_lag1_left'] = ac1\n",
    "    # feats['autocorr_lag1_right'] = ac2\n",
    "    feats['autocorr_lag1_whole'] = ac_whole\n",
    "    # _add_diff_ratio_feats(feats, 'autocorr_lag1', ac1, ac2)\n",
    "    # _add_contribution_ratio_feats(feats, 'autocorr_lag1', ac1, ac2, ac_whole)\n",
    "\n",
    "    # var1, var2, var_whole = s1.diff().var(), s2.diff().var(), s_whole.diff().var()\n",
    "    # feats['diff_var_left'] = var1\n",
    "    # feats['diff_var_right'] = var2\n",
    "    # feats['diff_var_whole'] = var_whole\n",
    "    # # _add_diff_ratio_feats(feats, 'diff_var', var1, var2)\n",
    "    # _add_contribution_ratio_feats(feats, 'diff_var', var1, var2, var_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 5. 频域特征 ---\n",
    "@register_feature(func_id=\"5\")\n",
    "def cyclic_features(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    def get_fft_props(series):\n",
    "        if len(series) < 2: return 0.0, 0.0\n",
    "        \n",
    "        N = len(series)\n",
    "        yf = np.fft.fft(series.values)\n",
    "        power = np.abs(yf[1:N//2])**2\n",
    "        xf = np.fft.fftfreq(N, 1)[1:N//2]\n",
    "        \n",
    "        if len(power) == 0: return 0.0, 0.0\n",
    "            \n",
    "        dominant_freq = xf[np.argmax(power)]\n",
    "        max_power = np.max(power)\n",
    "        return dominant_freq, max_power\n",
    "\n",
    "    freq1, power1 = get_fft_props(s1)\n",
    "    freq2, power2 = get_fft_props(s2)\n",
    "    freq_whole, power_whole = get_fft_props(s_whole)\n",
    "    \n",
    "    feats['dominant_freq_left'] = freq1\n",
    "    feats['dominant_freq_right'] = freq2\n",
    "    feats['dominant_freq_whole'] = freq_whole\n",
    "    # _add_diff_ratio_feats(feats, 'dominant_freq', freq1, freq2)\n",
    "    _add_contribution_ratio_feats(feats, 'dominant_freq', freq1, freq2, freq_whole)\n",
    "\n",
    "    # feats['max_power_left'] = power1\n",
    "    # feats['max_power_right'] = power2\n",
    "    # feats['max_power_whole'] = power_whole\n",
    "    # _add_diff_ratio_feats(feats, 'max_power', power1, power2)\n",
    "    # _add_contribution_ratio_feats(feats, 'max_power', power1, power2, power_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 6. 振幅特征 ---\n",
    "@register_feature(func_id=\"6\")\n",
    "def amplitude_features(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "    \n",
    "    # ptp1, ptp2, ptp_whole = np.ptp(s1), np.ptp(s2), np.ptp(s_whole)\n",
    "    # iqr1, iqr2, iqr_whole = scipy.stats.iqr(s1), scipy.stats.iqr(s2), scipy.stats.iqr(s_whole)\n",
    "\n",
    "    # feats['ptp_left'] = ptp1\n",
    "    # feats['ptp_right'] = ptp2\n",
    "    # feats['ptp_whole'] = ptp_whole\n",
    "    # _add_diff_ratio_feats(feats, 'ptp', ptp1, ptp2)\n",
    "    # _add_contribution_ratio_feats(feats, 'ptp', ptp1, ptp2, ptp_whole)\n",
    "\n",
    "    # feats['iqr_left'] = iqr1\n",
    "    # feats['iqr_right'] = iqr2\n",
    "    # feats['iqr_whole'] = iqr_whole\n",
    "    # _add_diff_ratio_feats(feats, 'iqr', iqr1, iqr2)\n",
    "    # _add_contribution_ratio_feats(feats, 'iqr', iqr1, iqr2, iqr_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 7. 熵信息 ---\n",
    "@register_feature(func_id=\"7\")\n",
    "def entropy_features_first(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    def should_keep(name):\n",
    "        if selected_features is None:\n",
    "            return True\n",
    "        keep = name in selected_features\n",
    "        return keep\n",
    "\n",
    "    def compute_entropy(x):\n",
    "        hist, _ = np.histogram(x, bins='auto', density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        return scipy.stats.entropy(hist)\n",
    "    \n",
    "    entropy_funcs = {\n",
    "        'shannon_entropy': compute_entropy,\n",
    "        'perm_entropy': lambda x: antropy.perm_entropy(x, normalize=True),\n",
    "        'spectral_entropy': lambda x: antropy.spectral_entropy(x, sf=1.0, normalize=True),\n",
    "        'svd_entropy': lambda x: antropy.svd_entropy(x, normalize=True),\n",
    "        'approx_entropy': antropy.app_entropy,\n",
    "        'sample_entropy': antropy.sample_entropy,\n",
    "        'petrosian_fd': antropy.petrosian_fd,\n",
    "        'katz_fd': antropy.katz_fd,\n",
    "        'higuchi_fd': antropy.higuchi_fd,\n",
    "        'detrended_fluctuation': antropy.detrended_fluctuation,\n",
    "    }\n",
    "\n",
    "    for name, func in entropy_funcs.items():\n",
    "        if not should_keep(name):\n",
    "            continue\n",
    "        try:\n",
    "            v1, v2, v_whole = func(s1), func(s2), func(s_whole)\n",
    "            feats[f'{name}_left'] = v1\n",
    "            feats[f'{name}_right'] = v2\n",
    "            feats[f'{name}_whole'] = v_whole\n",
    "            _add_diff_ratio_feats(feats, name, v1, v2)\n",
    "            _add_contribution_ratio_feats(feats, name, v1, v2, v_whole)\n",
    "        except Exception:\n",
    "            feats.update({f'{name}_left': 0, f'{name}_right': 0, f'{name}_whole': 0, f'{name}_diff': 0, f'{name}_ratio': 0})\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 7. 熵信息 ---\n",
    "@register_feature(func_id=\"7\")\n",
    "def entropy_features_second(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    def should_keep(candidates):\n",
    "        if selected_features is None:\n",
    "            return True\n",
    "        keep = any(c in selected_features for c in candidates)\n",
    "        return keep\n",
    "\n",
    "    if should_keep(['hjorth_mobility', 'hjorth_complexity']):\n",
    "        try:\n",
    "            m1, c1 = antropy.hjorth_params(s1)\n",
    "            m2, c2 = antropy.hjorth_params(s2)\n",
    "            m_whole, c_whole = antropy.hjorth_params(s_whole)\n",
    "            feats.update({\n",
    "                # 'hjorth_mobility_left': m1, \n",
    "                # 'hjorth_mobility_right': m2, \n",
    "                # 'hjorth_mobility_whole': m_whole,\n",
    "                'hjorth_complexity_left': c1, \n",
    "                'hjorth_complexity_right': c2, \n",
    "                'hjorth_complexity_whole': c_whole,\n",
    "            })\n",
    "            # _add_diff_ratio_feats(feats, 'hjorth_mobility', m1, m2)\n",
    "            # _add_contribution_ratio_feats(feats, 'hjorth_mobility', m1, m2, m_whole)\n",
    "            _add_diff_ratio_feats(feats, 'hjorth_complexity', c1, c2)\n",
    "            _add_contribution_ratio_feats(feats, 'hjorth_complexity', c1, c2, c_whole)\n",
    "        except Exception:\n",
    "            feats.update({'hjorth_mobility_left':0, 'hjorth_mobility_right':0, 'hjorth_mobility_whole':0, 'hjorth_mobility_diff':0, 'hjorth_mobility_ratio':0,\n",
    "                        'hjorth_complexity_left':0, 'hjorth_complexity_right':0, 'hjorth_complexity_whole':0, 'hjorth_complexity_diff':0, 'hjorth_complexity_ratio':0})\n",
    "\n",
    "    def series_to_binary_str(x, method='median'):\n",
    "        if method == 'median':\n",
    "            threshold = np.median(x)\n",
    "            return ''.join(['1' if val > threshold else '0' for val in x])\n",
    "        return None\n",
    "    if should_keep(['lziv_complexity']):\n",
    "        try:\n",
    "            bin_str1 = series_to_binary_str(s1)\n",
    "            bin_str2 = series_to_binary_str(s2)\n",
    "            bin_str_whole = series_to_binary_str(s_whole)\n",
    "            lz1, lz2, lz_whole = antropy.lziv_complexity(bin_str1, normalize=True), antropy.lziv_complexity(bin_str2, normalize=True), antropy.lziv_complexity(bin_str_whole, normalize=True)\n",
    "            feats.update({\n",
    "                'lziv_complexity_left': lz1, 'lziv_complexity_right': lz2, 'lziv_complexity_whole': lz_whole,\n",
    "            })\n",
    "            _add_diff_ratio_feats(feats, 'lziv_complexity', lz1, lz2)\n",
    "            _add_contribution_ratio_feats(feats, 'lziv_complexity', lz1, lz2, lz_whole)\n",
    "        except Exception:\n",
    "            feats.update({'lziv_complexity_left':0, 'lziv_complexity_right':0, 'lziv_complexity_whole':0, 'lziv_complexity_diff':0, 'lziv_complexity_ratio':0})\n",
    "\n",
    "    def estimate_cond_entropy(x, lag=1):\n",
    "        x = x - np.mean(x)\n",
    "        x_lag = x[:-lag]\n",
    "        x_now = x[lag:]\n",
    "        bins = 10\n",
    "        joint_hist, _, _ = np.histogram2d(x_lag, x_now, bins=bins, density=True)\n",
    "        joint_hist = joint_hist[joint_hist > 0]\n",
    "        H_xy = -np.sum(joint_hist * np.log(joint_hist))\n",
    "        H_x = -np.sum(np.histogram(x_lag, bins=bins, density=True)[0] * \\\n",
    "                      np.log(np.histogram(x_lag, bins=bins, density=True)[0] + 1e-12))\n",
    "        return H_xy - H_x\n",
    "    if should_keep(['cond_entropy']):\n",
    "        try:\n",
    "            # ce1 = estimate_cond_entropy(s1)\n",
    "            # ce2 = estimate_cond_entropy(s2)\n",
    "            ce_whole = estimate_cond_entropy(s_whole)\n",
    "            feats.update({\n",
    "                # 'cond_entropy_left': ce1, \n",
    "                # 'cond_entropy_right': ce2, \n",
    "                'cond_entropy_whole': ce_whole,\n",
    "            })\n",
    "            # _add_diff_ratio_feats(feats, 'cond_entropy', ce1, ce2)\n",
    "            # _add_contribution_ratio_feats(feats, 'cond_entropy', ce1, ce2, ce_whole)\n",
    "        except Exception:\n",
    "            feats.update({'cond_entropy_left':0, 'cond_entropy_right':0, 'cond_entropy_whole':0, 'cond_entropy_diff':0, 'cond_entropy_ratio':0})\n",
    "        \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 8. tsfresh --- \n",
    "@register_feature(func_id=\"8\")\n",
    "def tsfresh_features_first(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    \"\"\"基于tsfresh的特征工程\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    funcs = {\n",
    "        tsfresh_fe.ratio_value_number_to_time_series_length: None,\n",
    "        tsfresh_fe.sum_of_reoccurring_data_points: None,\n",
    "        tsfresh_fe.percentage_of_reoccurring_values_to_all_values: None,\n",
    "        tsfresh_fe.percentage_of_reoccurring_datapoints_to_all_datapoints: None,\n",
    "        tsfresh_fe.last_location_of_maximum: None,\n",
    "        tsfresh_fe.first_location_of_maximum: None,\n",
    "        tsfresh_fe.has_duplicate: None,\n",
    "        tsfresh_fe.benford_correlation: None,\n",
    "        tsfresh_fe.ratio_beyond_r_sigma: [\n",
    "            6, \n",
    "            3, 1.5, 1, \n",
    "            0.5\n",
    "        ],\n",
    "        tsfresh_fe.quantile: [\n",
    "            0.6, \n",
    "            0.4, \n",
    "            0.1\n",
    "        ],\n",
    "        tsfresh_fe.count_above: [0],\n",
    "        tsfresh_fe.number_peaks: [\n",
    "            25, \n",
    "            50\n",
    "        ],\n",
    "        tsfresh_fe.partial_autocorrelation: [\n",
    "            {\"lag\": 2}, \n",
    "            {\"lag\": 4},\n",
    "            {\"lag\": 6}\n",
    "        ],\n",
    "        tsfresh_fe.index_mass_quantile: [\n",
    "            {\"q\": 0.1}, \n",
    "            {\"q\": 0.6}, \n",
    "            {\"q\": 0.8}\n",
    "        ],\n",
    "        tsfresh_fe.ar_coefficient: [\n",
    "            {\"coeff\": 0, \"k\": 10}, \n",
    "            {\"coeff\": 2, \"k\": 10}, \n",
    "            {\"coeff\": 8, \"k\": 10}\n",
    "        ],\n",
    "        tsfresh_fe.linear_trend: [\n",
    "            {\"attr\": \"slope\"}, \n",
    "            {\"attr\": \"rvalue\"}, \n",
    "            {\"attr\": \"pvalue\"}, \n",
    "            {\"attr\": \"intercept\"}\n",
    "        ],\n",
    "        tsfresh_fe.fft_coefficient: [\n",
    "            {\"attr\": \"imag\", \"coeff\": 3}, \n",
    "            {\"attr\": \"imag\", \"coeff\": 2}, \n",
    "            {\"attr\": \"imag\", \"coeff\": 1}\n",
    "        ],\n",
    "        tsfresh_fe.energy_ratio_by_chunks: [\n",
    "            {\"num_segments\": 10, \"segment_focus\": 9},\n",
    "            {\"num_segments\": 20, \"segment_focus\": 16},\n",
    "        ],\n",
    "        tsfresh_fe.friedrich_coefficients: [\n",
    "            {\"coeff\": 2, \"m\": 3, \"r\": 30}, \n",
    "            {\"coeff\": 3, \"m\": 3, \"r\": 30}\n",
    "        ],\n",
    "        tsfresh_fe.change_quantiles: [\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 1.0, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 1.0, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.6},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.6, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.6, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.4, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 1.0, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 1.0, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.0},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.6, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.6, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.4, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"mean\",\"isabs\": True,  \"qh\": 1.0, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"mean\",\"isabs\": True,  \"qh\": 0.6, \"ql\": 0.4},\n",
    "        ],\n",
    "        tsfresh_fe.agg_linear_trend: [\n",
    "            {\"attr\": \"slope\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"slope\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"slope\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 50, \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 5,  \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 10, \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 50, \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 5,  \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 10, \"f_agg\": \"max\"},\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    def param_to_str(param):\n",
    "        if isinstance(param, dict):\n",
    "            return '_'.join([f\"{k}_{v}\" for k, v in param.items()])\n",
    "        else:\n",
    "            return str(param)\n",
    "\n",
    "    def calculate_stats_for_feature(func, param=None):\n",
    "        results = {}\n",
    "        base_name = func.__name__\n",
    "        if param is not None:\n",
    "            base_name += f\"_{param_to_str(param)}\"\n",
    "\n",
    "        try:\n",
    "            # Prepare arguments for each segment\n",
    "            args_s1 = [s1]\n",
    "            args_s2 = [s2]\n",
    "            args_s_whole = [s_whole]\n",
    "            is_combiner = False\n",
    "\n",
    "            if param is None: # Simple function, no params\n",
    "                pass\n",
    "            elif isinstance(param, dict):\n",
    "                # Check if it's a combiner function or a function with kwargs\n",
    "                sig = inspect.signature(func)\n",
    "                if 'param' in sig.parameters: # Combiner function\n",
    "                    is_combiner = True\n",
    "                    args_s1.append([param])\n",
    "                    args_s2.append([param])\n",
    "                    args_s_whole.append([param])\n",
    "                else: # Function with kwargs\n",
    "                    args_s1.append(param)\n",
    "                    args_s2.append(param)\n",
    "                    args_s_whole.append(param)\n",
    "            else: # Simple function with a single parameter\n",
    "                args_s1.append(param)\n",
    "                args_s2.append(param)\n",
    "                args_s_whole.append(param)\n",
    "\n",
    "            # Execute function for each segment\n",
    "            if is_combiner:\n",
    "                v1_dict = {k: v for k, v in func(*args_s1)}\n",
    "                v2_dict = {k: v for k, v in func(*args_s2)}\n",
    "                v_whole_dict = {k: v for k, v in func(*args_s_whole)}\n",
    "                \n",
    "                for key in v1_dict:\n",
    "                    v1, v2, v_whole = v1_dict[key], v2_dict[key], v_whole_dict[key]\n",
    "                    feat_name_base = f\"{func.__name__}_{key}\"\n",
    "                    results[f'{feat_name_base}_left'] = v1\n",
    "                    results[f'{feat_name_base}_right'] = v2\n",
    "                    results[f'{feat_name_base}_whole'] = v_whole\n",
    "                    _add_diff_ratio_feats(feats, feat_name_base, v1, v2)\n",
    "                    _add_contribution_ratio_feats(results, feat_name_base, v1, v2, v_whole)\n",
    "                return results\n",
    "\n",
    "            else:\n",
    "                if isinstance(param, dict) and not is_combiner:\n",
    "                    v1, v2, v_whole = func(args_s1[0], **args_s1[1]), func(args_s2[0], **args_s2[1]), func(args_s_whole[0], **args_s_whole[1])\n",
    "                else:\n",
    "                    v1, v2, v_whole = func(*args_s1), func(*args_s2), func(*args_s_whole)\n",
    "\n",
    "                results[f'{base_name}_left'] = v1\n",
    "                results[f'{base_name}_right'] = v2\n",
    "                results[f'{base_name}_whole'] = v_whole\n",
    "                _add_diff_ratio_feats(feats, base_name, v1, v2)\n",
    "                _add_contribution_ratio_feats(results, base_name, v1, v2, v_whole)\n",
    "        \n",
    "        except Exception:\n",
    "            # For combiner functions, need to know keys to create nulls\n",
    "            if 'param' in locals() and inspect.isfunction(func) and 'param' in inspect.signature(func).parameters:\n",
    "                 # It's a combiner, but we can't get keys without running it. Skip for now on error.\n",
    "                 pass\n",
    "            else:\n",
    "                results[f'{base_name}_left'] = np.nan\n",
    "                results[f'{base_name}_right'] = np.nan\n",
    "                results[f'{base_name}_whole'] = np.nan\n",
    "                results[f'{base_name}_diff'] = np.nan\n",
    "                results[f'{base_name}_ratio'] = np.nan\n",
    "                \n",
    "        return results\n",
    "\n",
    "    def should_keep(func, param):\n",
    "        if selected_features is None:\n",
    "            return True\n",
    "        base_name = func.__name__\n",
    "        if param is not None:\n",
    "            base_name += f\"_{param_to_str(param)}\"\n",
    "        base_name = clean_feature_name(base_name)\n",
    "        candidates = [\n",
    "            f\"{base_name}\"\n",
    "        ]\n",
    "        keep = any(c in selected_features for c in candidates)\n",
    "        return keep\n",
    "\n",
    "    for func, params in funcs.items():\n",
    "        if params is None:\n",
    "            if should_keep(func, None):\n",
    "                feats.update(calculate_stats_for_feature(func))\n",
    "        else:\n",
    "            for param in params:\n",
    "                if should_keep(func, param):\n",
    "                    feats.update(calculate_stats_for_feature(func, param))\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 9. 时间序列建模 ---\n",
    "@register_feature(func_id=\"9\")\n",
    "def ar_model_features(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    \"\"\"\n",
    "    基于AR模型派生特征。\n",
    "    1. 在 period 0 上训练模型，预测 period 1，计算残差统计量。\n",
    "    2. 在 period 1 上训练模型，预测 period 0，计算残差统计量。\n",
    "    3. 分别在 period 0 和 1 上训练模型，比较模型参数、残差和信息准则(AIC/BIC)。\n",
    "    \"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "    lags = 5 # 固定阶数以保证可比性\n",
    "\n",
    "    def should_keep(candidates):\n",
    "        if selected_features is None:\n",
    "            return True\n",
    "        keep = any(c in selected_features for c in candidates)\n",
    "        return keep\n",
    "\n",
    "    # --- 特征组1: 用 s1 训练，预测 s2 ---\n",
    "    if should_keep(['ar_residuals_s2_pred_mean', 'ar_residuals_s2_pred_std', 'ar_residuals_s2_pred_skew', 'ar_residuals_s2_pred_kurt']):\n",
    "        try:\n",
    "            model1_fit = AutoReg(s1, lags=lags).fit()\n",
    "            predictions = model1_fit.predict(start=len(s1), end=len(s1) + len(s2) - 1, dynamic=True)\n",
    "            residuals = s2 - predictions\n",
    "            feats['ar_residuals_s2_pred_mean'] = np.mean(residuals)\n",
    "            feats['ar_residuals_s2_pred_std'] = np.std(residuals)\n",
    "            feats['ar_residuals_s2_pred_skew'] = pd.Series(residuals).skew()\n",
    "            feats['ar_residuals_s2_pred_kurt'] = pd.Series(residuals).kurt()\n",
    "        except Exception:\n",
    "            # 宽泛地捕获异常，防止因数值问题中断\n",
    "            feats.update({'ar_residuals_s2_pred_mean': 0, 'ar_residuals_s2_pred_std': 0, 'ar_residuals_s2_pred_skew': 0, 'ar_residuals_s2_pred_kurt': 0})\n",
    "\n",
    "    # --- 特征组2: 用 s2 训练，预测 s1 ---\n",
    "    if should_keep(['ar_residuals_s1_pred_mean', 'ar_residuals_s1_pred_std', 'ar_residuals_s1_pred_skew', 'ar_residuals_s1_pred_kurt']):\n",
    "        try:\n",
    "            model2_fit = AutoReg(s2, lags=lags).fit()\n",
    "            predictions_on_s1 = model2_fit.predict(start=len(s2), end=len(s2) + len(s1) - 1, dynamic=True)\n",
    "            residuals_s1_pred = s1 - predictions_on_s1\n",
    "            feats['ar_residuals_s1_pred_mean'] = np.mean(residuals_s1_pred)\n",
    "            feats['ar_residuals_s1_pred_std'] = np.std(residuals_s1_pred)\n",
    "            feats['ar_residuals_s1_pred_skew'] = pd.Series(residuals_s1_pred).skew()\n",
    "            feats['ar_residuals_s1_pred_kurt'] = pd.Series(residuals_s1_pred).kurt()\n",
    "        except Exception:\n",
    "            feats.update({'ar_residuals_s1_pred_mean': 0, 'ar_residuals_s1_pred_std': 0, 'ar_residuals_s1_pred_skew': 0, 'ar_residuals_s1_pred_kurt': 0})\n",
    "\n",
    "    # --- 特征组3: 分别建模，比较差异 ---\n",
    "    if should_keep(['ar_param_0', 'ar_param_1', 'ar_param_2', 'ar_param_3', 'ar_param_4', 'ar_param_5']):\n",
    "        s1_resid_std, s1_params = np.nan, np.full(lags + 1, np.nan)\n",
    "        s1_aic, s1_bic = np.nan, np.nan\n",
    "        if len(s1) > lags:\n",
    "            try:\n",
    "                fit1 = AutoReg(s1, lags=lags).fit()\n",
    "                s1_resid_std = np.std(fit1.resid)\n",
    "                s1_params = fit1.params\n",
    "                s1_aic = fit1.aic\n",
    "                s1_bic = fit1.bic\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        s2_resid_std, s2_params = np.nan, np.full(lags + 1, np.nan)\n",
    "        s2_aic, s2_bic = np.nan, np.nan\n",
    "        if len(s2) > lags:\n",
    "            try:\n",
    "                fit2 = AutoReg(s2, lags=lags).fit()\n",
    "                s2_resid_std = np.std(fit2.resid)\n",
    "                s2_params = fit2.params\n",
    "                s2_aic = fit2.aic\n",
    "                s2_bic = fit2.bic\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        swhole_resid_std, swhole_params = np.nan, np.full(lags + 1, np.nan)\n",
    "        swhole_aic, swhole_bic = np.nan, np.nan\n",
    "        if len(s_whole) > lags:\n",
    "            try:\n",
    "                fit_whole = AutoReg(s_whole, lags=lags).fit()\n",
    "                swhole_resid_std = np.std(fit_whole.resid)\n",
    "                swhole_params = fit_whole.params\n",
    "                swhole_aic = fit_whole.aic\n",
    "                swhole_bic = fit_whole.bic\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "        # feats['ar_resid_std_left'] = s1_resid_std\n",
    "        # feats['ar_resid_std_right'] = s2_resid_std\n",
    "        # feats['ar_resid_std_whole'] = swhole_resid_std\n",
    "        # _add_diff_ratio_feats(feats, 'ar_resid_std', s1_resid_std, s2_resid_std)\n",
    "        # _add_contribution_ratio_feats(feats, 'ar_resid_std', s1_resid_std, s2_resid_std, swhole_resid_std)\n",
    "        \n",
    "        # feats['ar_aic_left'] = s1_aic\n",
    "        # feats['ar_aic_right'] = s2_aic\n",
    "        # feats['ar_aic_whole'] = swhole_aic\n",
    "        # _add_diff_ratio_feats(feats, 'ar_aic', s1_aic, s2_aic)\n",
    "        # _add_contribution_ratio_feats(feats, 'ar_aic', s1_aic, s2_aic, swhole_aic)\n",
    "\n",
    "        # feats['ar_bic_left'] = s1_bic\n",
    "        # feats['ar_bic_right'] = s2_bic\n",
    "        # feats['ar_bic_whole'] = swhole_bic\n",
    "        # _add_diff_ratio_feats(feats, 'ar_bic', s1_bic, s2_bic)\n",
    "        # _add_contribution_ratio_feats(feats, 'ar_bic', s1_bic, s2_bic, swhole_bic)\n",
    "        \n",
    "        # 比较模型系数\n",
    "        for i in range(lags + 1):\n",
    "            if should_keep([f'ar_param_{i}']):\n",
    "                feats[f'ar_param_{i}_left'] = s1_params[i]\n",
    "                feats[f'ar_param_{i}_right'] = s2_params[i]\n",
    "                feats[f'ar_param_{i}_whole'] = swhole_params[i]\n",
    "                _add_diff_ratio_feats(feats, f'ar_param_{i}', s1_params[i], s2_params[i])\n",
    "                _add_contribution_ratio_feats(feats, f'ar_param_{i}', s1_params[i], s2_params[i], swhole_params[i])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 10. 分段损失 ---\n",
    "class RPTFeatureExtractor:\n",
    "    def __init__(self, selected_features):\n",
    "        # 所有可用的cost类及其名称\n",
    "        self.cost_classes = {\n",
    "            'l1': rpt.costs.CostL1,               # 中位数\n",
    "            'l2': rpt.costs.CostL2,               # 均值\n",
    "            'clinear': rpt.costs.CostCLinear,     # 线性协方差\n",
    "            'rbf': rpt.costs.CostRbf,             # RBF核\n",
    "            'normal': rpt.costs.CostNormal,       # 协方差\n",
    "            'ar': rpt.costs.CostAR,               # 自回归\n",
    "            'mahalanobis': rpt.costs.CostMl,      # 马氏距离\n",
    "            'rank': rpt.costs.CostRank,           # 排名\n",
    "            'cosine': rpt.costs.CostCosine,       # 余弦距离\n",
    "        }\n",
    "        self.selected_features = selected_features\n",
    "\n",
    "    def calculate(self, cost, start, end):\n",
    "        result = cost.error(start, end)\n",
    "        if isinstance(result, (np.ndarray, list)) and np.array(result).size == 1:\n",
    "            return float(np.array(result).squeeze())\n",
    "        return result\n",
    "\n",
    "    def should_keep(self, name):\n",
    "        if self.selected_features is None:\n",
    "            return True\n",
    "        keep = f'rpt_cost_{name}' in self.selected_features\n",
    "        return keep\n",
    "\n",
    "    def extract(self, signal, boundary):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            signal: 1D numpy array，单变量时间序列\n",
    "            boundary: int，分割点\n",
    "        输出：\n",
    "            result: dict，格式为 {cost_name: {'left': value, 'right': value}}\n",
    "        \"\"\"\n",
    "        signal = np.asarray(signal)\n",
    "        n = len(signal)\n",
    "        result = {}\n",
    "        for name, cls in self.cost_classes.items():\n",
    "            if not self.should_keep(name):\n",
    "                continue\n",
    "            try:\n",
    "                if name == 'ar':\n",
    "                    cost = cls(order=4)\n",
    "                else:\n",
    "                    cost = cls()\n",
    "                cost.fit(signal)\n",
    "                left = self.calculate(cost, 0, boundary)\n",
    "                right = self.calculate(cost, boundary, n)\n",
    "                whole = self.calculate(cost, 0, n)\n",
    "                # diff = right - left if left is not None and right is not None else None\n",
    "                # ratio = right / (left + 1e-6) if left is not None and right is not None else None\n",
    "            except Exception:\n",
    "                left = None\n",
    "                right = None\n",
    "                whole = None\n",
    "                # diff = None\n",
    "                # ratio = None\n",
    "            # Move to _add_diff_ratio_feats, 'diff': diff, 'ratio': ratio\n",
    "            result[name] = {'left': left, 'right': right, 'whole': whole}\n",
    "        return result\n",
    "\n",
    "@register_feature(func_id=\"10\")\n",
    "def rupture_cost_features(u: pd.DataFrame, selected_features: set = None) -> dict:\n",
    "    value = u['value'].values.astype(np.float32)\n",
    "    period = u['period'].values.astype(np.float32)\n",
    "    boundary = np.where(np.diff(period) != 0)[0].item()\n",
    "    feats = {}\n",
    "\n",
    "    extractor = RPTFeatureExtractor(selected_features)\n",
    "    features = extractor.extract(value, boundary)\n",
    "\n",
    "    feats = {}\n",
    "    for k, v in features.items():\n",
    "        for seg, value in v.items():\n",
    "            feats[f'rpt_cost_{k}_{seg}'] = value\n",
    "        _add_diff_ratio_feats(feats, f'rpt_cost_{k}', v['left'], v['right'])\n",
    "        _add_contribution_ratio_feats(feats, f'rpt_cost_{k}', v['left'], v['right'], v['whole'])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3349f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- feature.py 特征生成工具 ---\n",
    "def _apply_feature_func_sequential(\n",
    "        func, \n",
    "        X_df: pd.DataFrame, \n",
    "        use_tqdm: bool = False,\n",
    "        selected_features: set | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"顺序应用单个特征函数\"\"\"\n",
    "\n",
    "    all_ids = X_df.index.get_level_values(\"id\").unique()\n",
    "    iterator = (\n",
    "        tqdm(all_ids, desc=f\"Running {func.__name__} (sequentially)\") \n",
    "        if use_tqdm else all_ids\n",
    "    )\n",
    "\n",
    "    results = [\n",
    "        {**{'id': id_val}, **func(X_df.loc[id_val], selected_features=selected_features)}\n",
    "        for id_val in iterator\n",
    "    ]\n",
    "\n",
    "    return pd.DataFrame(results).set_index('id')\n",
    "\n",
    "def _apply_feature_func_parallel(\n",
    "        func, \n",
    "        X_df: pd.DataFrame, \n",
    "        use_tqdm: bool = False,\n",
    "        selected_features: set | None = None\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"并行应用单个特征函数\"\"\"\n",
    "    all_ids = X_df.index.get_level_values(\"id\").unique()\n",
    "    iterator = (\n",
    "        tqdm(all_ids, desc=f\"Running {func.__name__} (parallel)\")\n",
    "        if use_tqdm else all_ids\n",
    "    )\n",
    "    results = Parallel(n_jobs=config.N_JOBS)(\n",
    "        delayed(lambda df_id, id_val: {**{'id': id_val}, **func(df_id, selected_features=selected_features)})(X_df.loc[id_val], id_val)\n",
    "        for id_val in iterator\n",
    "    )\n",
    "    return pd.DataFrame(results).set_index('id')\n",
    "\n",
    "def _apply_transform_func(func, X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"执行变换函数\"\"\"\n",
    "    return func(X_df)\n",
    "\n",
    "def apply_transformation(X_df: pd.DataFrame, transform_funcs: List[str] = None) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    应用时序变换\n",
    "    \n",
    "    Args:\n",
    "        X_df: 输入数据框\n",
    "        transform_funcs: 要应用的变换函数名称列表，如果为None则应用所有注册的变换函数\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: 键为模态名称，值为对应的数据框\n",
    "    \"\"\"\n",
    "    if transform_funcs is None:\n",
    "        transform_funcs = list(TRANSFORM_REGISTRY.keys())\n",
    "    \n",
    "    # 验证变换函数是否存在\n",
    "    valid_transform_funcs = []\n",
    "    for func_name in transform_funcs:\n",
    "        if func_name not in TRANSFORM_REGISTRY:\n",
    "            pass\n",
    "            # logger.warning(f\"变换函数 {func_name} 未在注册表中找到，已跳过。\")\n",
    "        else:\n",
    "            valid_transform_funcs.append(func_name)\n",
    "    \n",
    "    transform_funcs = valid_transform_funcs\n",
    "    \n",
    "    # 存储所有模态的数据框\n",
    "    transformed_data = {}\n",
    "    \n",
    "    for func_name in transform_funcs:\n",
    "        # logger.info(f\"--- 开始应用变换函数: {func_name} ---\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        transform_info = TRANSFORM_REGISTRY[func_name]\n",
    "        func = transform_info['func']\n",
    "        output_mode_names = transform_info['output_mode_names']\n",
    "        \n",
    "        # 执行变换\n",
    "        transformed_results = _apply_transform_func(func, X_df)\n",
    "        \n",
    "        # 存储结果\n",
    "        for mode_name, mode_df in zip(output_mode_names, transformed_results):\n",
    "            transformed_data[mode_name] = mode_df\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        # logger.info(f\"'{func_name}' 变换完毕，耗时: {duration:.2f} 秒，生成模态: {output_mode_names}\")\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def clean_feature_name(name: str, prefix: str = \"f\") -> str:\n",
    "    \"\"\"清理单个特征名称，确保它是合法的标识符。\"\"\"\n",
    "    # 替换非法字符为 _\n",
    "    cleaned = re.sub(r\"[^\\w]\", \"_\", name)\n",
    "    # 防止开头是数字\n",
    "    if re.match(r\"^\\d\", cleaned):\n",
    "        cleaned = f\"{prefix}_{cleaned}\"\n",
    "    # 多个连续 _ 合并为一个\n",
    "    cleaned = re.sub(r\"__+\", \"_\", cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def clean_feature_names(df: pd.DataFrame, prefix: str = \"f\") -> pd.DataFrame:\n",
    "    \"\"\"清理特征名称，确保它们是合法的列名。\"\"\"\n",
    "    cleaned_columns = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        # 替换非法字符为 _\n",
    "        cleaned = re.sub(r'[^\\w]', '_', col)\n",
    "        # 防止开头是数字（如 \"123_feature\"）非法\n",
    "        if re.match(r'^\\d', cleaned):\n",
    "            cleaned = f\"{prefix}_{cleaned}\"\n",
    "        # 多个连续 _ 合并为一个\n",
    "        cleaned = re.sub(r'__+', '_', cleaned)\n",
    "        cleaned_columns.append(cleaned)\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c449b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- feature.py 特征管理工具 ---\n",
    "def _get_latest_feature_file() -> Path | None:\n",
    "    \"\"\"查找并返回最新的特征文件路径\"\"\"\n",
    "    # 获取特征文件目录下的所有特征文件\n",
    "    feature_files = list(config.FEATURE_DIR.glob('features_*.parquet'))\n",
    "    # 如果没有特征文件，返回None\n",
    "    if not feature_files:\n",
    "        return None\n",
    "    return max(feature_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "def _load_feature_file(file_path: Path):\n",
    "    \"\"\"加载指定的特征文件及其元数据。\"\"\"\n",
    "    if not file_path or not file_path.exists():\n",
    "        return pd.DataFrame(), {}\n",
    "    try:\n",
    "        table = pd.read_parquet(file_path)\n",
    "        metadata_str = table.attrs.get('feature_metadata', '{}')\n",
    "        metadata = json.loads(metadata_str)\n",
    "        return table, metadata\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"无法加载特征文件 {file_path}: {e}。\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "def _load_feature_dict_file(file_path: Path):\n",
    "    \"\"\"加载字典格式的特征文件及其元数据。\"\"\"\n",
    "    if not file_path or not file_path.exists():\n",
    "        return {}, {}\n",
    "    try:\n",
    "        # 加载主文件获取元数据\n",
    "        main_table = pd.read_parquet(file_path)\n",
    "        metadata_str = main_table.attrs.get('feature_metadata', '{}')\n",
    "        metadata = json.loads(metadata_str)\n",
    "        \n",
    "        # 加载字典格式的特征数据\n",
    "        feature_dict = {}\n",
    "        base_name = file_path.stem  # 去掉扩展名\n",
    "        \n",
    "        # 查找所有相关的特征文件\n",
    "        for data_id_file in file_path.parent.glob(f\"{base_name}_id_*.parquet\"):\n",
    "            # 从文件名提取数据ID\n",
    "            data_id = data_id_file.stem.split('_id_')[-1]\n",
    "            feature_dict[data_id] = pd.read_parquet(data_id_file)\n",
    "        \n",
    "        # 如果没有找到分离的文件，尝试从主文件加载（向后兼容）\n",
    "        if not feature_dict and not main_table.empty:\n",
    "            feature_dict[\"0\"] = main_table\n",
    "            \n",
    "        return feature_dict, metadata\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"无法加载字典格式特征文件 {file_path}: {e}。\")\n",
    "        return {}, {}\n",
    "\n",
    "def load_features(feature_file: str = None, data_ids: list = None) -> tuple[pd.DataFrame | None, str | None]:\n",
    "    \"\"\"加载指定的或最新的特征文件，并拼接指定数据ID的特征数据。\n",
    "    \n",
    "    Args:\n",
    "        feature_file (str, optional): 特征文件名。如果未指定，将加载最新版本。\n",
    "        data_ids (list, optional): 要使用的数据ID列表，例如[\"0\", \"1\"]。如果未指定，默认使用[\"0\"]。\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (拼接后的特征数据, 文件名) 或 (None, None)\n",
    "    \"\"\"\n",
    "    # 使用一个临时的logger，避免依赖全局logger\n",
    "    import logging\n",
    "    temp_logger = logging.getLogger('load_features')\n",
    "    if not temp_logger.handlers:\n",
    "        temp_logger.addHandler(logging.StreamHandler())\n",
    "        temp_logger.setLevel(logging.INFO)\n",
    "\n",
    "    if feature_file:\n",
    "        path_to_load = config.FEATURE_DIR / feature_file\n",
    "    else:\n",
    "        temp_logger.info(\"未指定特征文件，将尝试加载最新版本。\")\n",
    "        path_to_load = _get_latest_feature_file()\n",
    "\n",
    "    if not path_to_load or not path_to_load.exists():\n",
    "        temp_logger.error(f\"无法找到要加载的特征文件: {path_to_load}\")\n",
    "        return None, None\n",
    "\n",
    "    temp_logger.info(f\"正在从 {path_to_load.name} 加载特征...\")\n",
    "    \n",
    "    # 如果未指定data_ids，默认使用[\"0\"]\n",
    "    if data_ids is None:\n",
    "        data_ids = [\"0\"]\n",
    "    \n",
    "    # 尝试加载字典格式的特征文件\n",
    "    try:\n",
    "        feature_dict, _ = _load_feature_dict_file(path_to_load)\n",
    "        temp_logger.info(f\"加载字典格式特征文件成功，包含数据ID: {list(feature_dict.keys())}\")\n",
    "        \n",
    "        # 检查请求的数据ID是否存在\n",
    "        available_ids = list(feature_dict.keys())\n",
    "        missing_ids = [id for id in data_ids if id not in available_ids]\n",
    "        if missing_ids:\n",
    "            temp_logger.warning(f\"请求的数据ID {missing_ids} 在特征文件中不存在，可用的ID: {available_ids}\")\n",
    "            # 只使用存在的ID\n",
    "            data_ids = [id for id in data_ids if id in available_ids]\n",
    "            if not data_ids:\n",
    "                temp_logger.error(\"没有可用的数据ID\")\n",
    "                return None, None\n",
    "        \n",
    "        # 拼接指定数据ID的特征数据\n",
    "        feature_dfs = []\n",
    "        for data_id in data_ids:\n",
    "            df = feature_dict[data_id].copy()\n",
    "            feature_dfs.append(df)\n",
    "        \n",
    "        # 按行拼接（concat along axis=0），保持特征列数不变\n",
    "        if len(feature_dfs) == 1:\n",
    "            concatenated_df = feature_dfs[0]\n",
    "        else:\n",
    "            concatenated_df = pd.concat(feature_dfs, axis=0, ignore_index=False)\n",
    "        \n",
    "        total_features = len(concatenated_df.columns)\n",
    "        total_rows = len(concatenated_df)\n",
    "        temp_logger.info(f\"特征拼接成功，使用数据ID: {data_ids}，共 {total_features} 个特征，{total_rows} 行数据。\")\n",
    "        return concatenated_df, path_to_load.name\n",
    "                \n",
    "    except Exception:\n",
    "        # 回退到旧格式\n",
    "        feature_df, _ = _load_feature_file(path_to_load)\n",
    "        \n",
    "        if feature_df.empty:\n",
    "            return None, None\n",
    "        \n",
    "        # 对于旧格式，只能返回单个DataFrame（相当于数据ID \"0\"）\n",
    "        if \"0\" in data_ids:\n",
    "            temp_logger.info(f\"特征加载成功（旧格式），共 {len(feature_df.columns)} 个特征。\")\n",
    "            return feature_df, path_to_load.name\n",
    "        else:\n",
    "            temp_logger.warning(f\"旧格式特征文件只支持数据ID '0'，但请求的是 {data_ids}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a22aa431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [Train&Infer] 特征工具 ---\n",
    "def extract_raw_features(feat):\n",
    "    raw_parts = []\n",
    "    trans_mode_to_run = []\n",
    "    for func_name in TRANSFORM_REGISTRY.keys():\n",
    "        trans_mode_to_run.extend(TRANSFORM_REGISTRY[func_name][\"output_mode_names\"])\n",
    "    \n",
    "    # 找到所有mode_flag的位置\n",
    "    flag_positions = []\n",
    "    for flag in trans_mode_to_run:\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = feat.find(f'_{flag}_', start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "            flag_positions.append((pos + 1, flag))  # +1 to skip the leading underscore\n",
    "            start = pos + 1\n",
    "    \n",
    "    # 按位置排序\n",
    "    flag_positions.sort()\n",
    "    \n",
    "    # 根据位置切分特征\n",
    "    for i, (pos, flag) in enumerate(flag_positions):\n",
    "        # 确定当前特征的结束位置\n",
    "        if i + 1 < len(flag_positions):\n",
    "            end_pos = flag_positions[i + 1][0] - 1  # -1 to exclude the underscore\n",
    "            raw_part = feat[pos:end_pos]\n",
    "        else:\n",
    "            raw_part = feat[pos:]\n",
    "        raw_parts.append(raw_part)\n",
    "    \n",
    "    # print(raw_parts)\n",
    "    return raw_parts\n",
    "\n",
    "def extract_trans_funcs_dict(\n",
    "        trans_mode_to_run: list = None, \n",
    "    ):\n",
    "    if trans_mode_to_run is None:\n",
    "        trans_mode_to_run = []\n",
    "        for func_name in TRANSFORM_REGISTRY.keys():\n",
    "            trans_mode_to_run.extend(TRANSFORM_REGISTRY[func_name][\"output_mode_names\"])\n",
    "        logger.warning(f'变换模式: {trans_mode_to_run}')\n",
    "\n",
    "    # 1. 提取trans-funcs对\n",
    "    trans_funcs_dict = {}\n",
    "    trans_feats_dict = {}\n",
    "    raw_feat_name = []\n",
    "    # 提取原始特征名-去除交互操作\n",
    "    operator_flags = sorted(config.OPERATOR_FLAGS, key=len, reverse=True)\n",
    "    for feat in config.REMAIN_FEATURES:\n",
    "        matched_flag = next((flag for flag in operator_flags if feat.startswith(flag)), None)\n",
    "        if matched_flag is not None:\n",
    "            raw_parts = extract_raw_features(feat)\n",
    "            raw_feat_name.extend(raw_parts)\n",
    "        else:\n",
    "            raw_feat_name.append(feat)\n",
    "\n",
    "    # 提取原始特征名-去除特征计算操作\n",
    "    feats_flags = sorted(config.FEAT_FLAGS, key=len, reverse=True)\n",
    "    sorted_feats_flags = sorted(feats_flags, key=len, reverse=True)\n",
    "    def clean_end(feat: str) -> str:\n",
    "        for flag in sorted_feats_flags:\n",
    "            if feat.endswith(flag):\n",
    "                return feat[: -len(flag)]\n",
    "        return feat  # 如果没有匹配到后缀，原样返回\n",
    "    raw_feat_name = [clean_end(feat) for feat in raw_feat_name]\n",
    "\n",
    "    # 记录\n",
    "    for feat in raw_feat_name:\n",
    "        parts = feat.split('_')\n",
    "        trans_mode, func_mode = parts[0], parts[1]\n",
    "        feat_name = '_'.join(parts[2:])\n",
    "        if trans_mode in trans_mode_to_run:\n",
    "            trans_funcs_dict.setdefault(trans_mode, set()).add(func_mode)\n",
    "            trans_feats_dict.setdefault(trans_mode, set()).add(feat_name)\n",
    "    # trans_funcs_dict = {k: sorted(list(v)) for k, v in trans_funcs_dict.items()}\n",
    "    # trans_feats_dict = {k: sorted(list(v)) for k, v in trans_feats_dict.items()}\n",
    "    logger.warning(f'变换-特征函数匹配: {trans_funcs_dict}')\n",
    "    logger.warning(f'变换-特征名称匹配: {trans_feats_dict}')\n",
    "    return trans_funcs_dict, trans_feats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b905702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [Train] 加载已计算特征 ---\n",
    "def load_precalculated_features(model_directory_path):\n",
    "    \"\"\"Load all LightGBM model files saved with joblib and prepare them for ensemble\"\"\"\n",
    "    # feature\n",
    "    feature_file_name = None\n",
    "    data_ids = None\n",
    "    feature_df, loaded_feature_name = load_features(feature_file_name, data_ids=data_ids)\n",
    "    logger.info(f\"Successfully loaded features from: {loaded_feature_name}\")\n",
    "    if feature_df is not None:\n",
    "        missing_features = [f for f in config.REMAIN_FEATURES if f not in feature_df.columns]\n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing REMAIN_FEATURES in <{data_id}> before filter: {missing_features}\")\n",
    "        feature_df = feature_df[config.REMAIN_FEATURES]\n",
    "    else:\n",
    "        feature_df = pd.DataFrame()\n",
    "\n",
    "    # y_train\n",
    "    try:\n",
    "        y_train_loaded = pd.read_parquet(f\"{model_directory_path}/y_train_head9901.parquet\")\n",
    "    except:\n",
    "        y_train_loaded = pd.Series()\n",
    "\n",
    "    return feature_df, loaded_feature_name, y_train_loaded\n",
    "\n",
    "def filter_unloaded_ids(X_train, y_train, loaded_index):\n",
    "    \"\"\"过滤掉已计算过特征的 id，同时保持 MultiIndex 格式 (X_train) 和对齐的 y_train。\"\"\"\n",
    "    remaining_ids = X_train.index.get_level_values(\"id\").unique().difference(loaded_index)\n",
    "\n",
    "    X_train_filtered = X_train.loc[remaining_ids]\n",
    "    y_train_filtered = y_train.loc[remaining_ids]\n",
    "\n",
    "    return X_train_filtered, y_train_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c20f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [Train] 生成特征 ---\n",
    "def generate_features(\n",
    "        X_data, \n",
    "        funcs_to_run: list = None, \n",
    "        trans_to_run: list = None, \n",
    "        use_tqdm: bool = False,\n",
    "        parallel: bool = False,\n",
    "        trans_funcs_dict: dict = None,\n",
    "        trans_feats_dict: dict = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    生成指定的特征，或者如果未指定，则生成所有已注册的特征。\n",
    "    可以基于一个现有的特征文件进行增量更新。\n",
    "    现在支持字典格式的输入数据和特征存储。\n",
    "\n",
    "    Args:\n",
    "        X_data: 输入数据，可以是:\n",
    "            - pd.DataFrame: 单个数据框（向后兼容）\n",
    "            - dict: 字典格式，键为数据ID（\"0\"表示原始数据，\"1\"、\"2\"等表示增强数据），值为对应的数据框\n",
    "        funcs_to_run (list, optional): 要运行的特征函数名称列表。\n",
    "            如果为 None，则运行所有在 `FEATURE_REGISTRY` 中注册的、且不在 `EXPERIMENTAL_FEATURES` 中的函数。\n",
    "        trans_to_run (list, optional): 要运行的变换函数名称列表。\n",
    "        base_feature_file (str, optional): 基础特征文件名。如果提供，\n",
    "            将加载此文件并在此基础上添加或更新特征。否则，将创建一个新的特征集。\n",
    "    \"\"\"\n",
    "    # utils.ensure_feature_dirs()\n",
    "    \n",
    "    # 处理输入数据格式\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        # 向后兼容：单个数据框转换为字典格式\n",
    "        X_data_dict = {\"0\": X_data}\n",
    "        logger.info(\"输入为单个数据框，已转换为字典格式（数据ID: '0'）\")\n",
    "    elif isinstance(X_data, dict):\n",
    "        X_data_dict = X_data\n",
    "        logger.info(f\"输入为字典格式，包含数据ID: {list(X_data_dict.keys())}\")\n",
    "    else:\n",
    "        raise ValueError(\"X_data必须是pd.DataFrame或dict类型\")\n",
    "    \n",
    "    if funcs_to_run is None:\n",
    "        # 如果未指定函数，则运行所有非实验性特征\n",
    "        funcs_to_run = [\n",
    "            f for f in FEATURE_REGISTRY.keys() \n",
    "            if f not in config.EXPERIMENTAL_FEATURES\n",
    "        ]\n",
    "        logger.info(f\"未指定特征函数，将运行所有 {len(funcs_to_run)} 个非实验性特征。\")\n",
    "    \n",
    "    # 验证请求的函数是否都已注册\n",
    "    valid_funcs_to_run = []\n",
    "    for func_name in funcs_to_run:\n",
    "        if func_name not in FEATURE_REGISTRY:\n",
    "            logger.warning(f\"函数 {func_name} 未在注册表中找到，已跳过。\")\n",
    "        else:\n",
    "            valid_funcs_to_run.append(func_name)\n",
    "    \n",
    "    funcs_to_run = valid_funcs_to_run\n",
    "    feature_dict, metadata = {}, {}\n",
    "    \n",
    "    # 确保每个数据ID都有对应的特征DataFrame\n",
    "    for data_id in X_data_dict.keys():\n",
    "        if data_id not in feature_dict:\n",
    "            # 获取该数据ID的唯一ID列表\n",
    "            unique_ids = X_data_dict[data_id].index.get_level_values('id').unique()\n",
    "            feature_dict[data_id] = pd.DataFrame(index=unique_ids)\n",
    "            logger.info(f\"为数据ID '{data_id}' 创建新的特征DataFrame，包含 {len(unique_ids)} 个样本\")\n",
    "    \n",
    "    logger.info(f\"基础特征字典包含数据ID: {list(feature_dict.keys())}\")\n",
    "    for data_id, df in feature_dict.items():\n",
    "        logger.info(f\"  数据ID '{data_id}': {df.shape}\")\n",
    "\n",
    "    if trans_funcs_dict is None or trans_feats_dict is None:\n",
    "        trans_funcs_dict, trans_feats_dict = extract_trans_funcs_dict()\n",
    "\n",
    "    # 3. 为每个数据ID生成特征\n",
    "    initial_feature_counts = {data_id: len(df.columns) for data_id, df in feature_dict.items()}\n",
    "    \n",
    "    for data_id, X_df in X_data_dict.items():\n",
    "        logger.info(f\"=== 开始为数据ID '{data_id}' 生成特征 ===\")\n",
    "        \n",
    "        # 时序分解\n",
    "        logger.info(f\"--- 开始时序分解（数据ID: {data_id}） ---\")\n",
    "        transformed_data = apply_transformation(X_df, trans_to_run)\n",
    "        logger.info(f\"分解完成，共生成 {len(transformed_data)} 个模态: {list(transformed_data.keys())}\")\n",
    "        \n",
    "        # 获取当前数据ID的特征DataFrame\n",
    "        current_feature_df = feature_dict[data_id]\n",
    "        loaded_features = current_feature_df.columns.tolist()\n",
    "        \n",
    "        # 逐个生成新特征并更新\n",
    "        for mode_name, mode_df in transformed_data.items():\n",
    "            logger.info(f\"=== 开始为数据ID '{data_id}' 的模态 '{mode_name}' 生成特征 ===\")\n",
    "            selected_features = trans_feats_dict.get(mode_name, set())\n",
    "            for func_name in funcs_to_run:\n",
    "                logger.info(f\"--- 开始生成特征: {func_name} ---\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                feature_info = FEATURE_REGISTRY[func_name]\n",
    "                func = feature_info['func']\n",
    "                is_parallelizable = feature_info['parallelizable']\n",
    "                func_id = feature_info['func_id']\n",
    "                if func_id not in trans_funcs_dict[mode_name]:\n",
    "                    logger.info(f\"函数 '{func_name}' 已跳过。\")\n",
    "                    continue\n",
    "                \n",
    "                if is_parallelizable and parallel:\n",
    "                    new_features_df = _apply_feature_func_parallel(func, mode_df, use_tqdm, selected_features=selected_features)\n",
    "                else:\n",
    "                    logger.info(f\"函数 '{func_name}' 不可并行化，将顺序执行。\")\n",
    "                    new_features_df = _apply_feature_func_sequential(func, mode_df, use_tqdm, selected_features=selected_features)\n",
    "                new_features_df.columns = [f\"{mode_name}_{func_id}_{col}\" for col in new_features_df.columns]\n",
    "                new_features_df = clean_feature_names(new_features_df)\n",
    "\n",
    "                # 记录日志\n",
    "                duration = time.time() - start_time\n",
    "                logger.info(f\"'{func_name}' 生成完毕，耗时: {duration:.2f} 秒。\")\n",
    "                logger.info(f\"  新生成特征列名: {new_features_df.columns.tolist()}\")\n",
    "                \n",
    "                for col in new_features_df.columns:\n",
    "                    null_ratio = new_features_df[col].isnull().sum() / len(new_features_df)\n",
    "                    zero_ratio = (new_features_df[col] == 0).sum() / len(new_features_df)\n",
    "                    logger.info(f\"    - '{col}': 空值比例={null_ratio:.2%}, 零值比例={zero_ratio:.2%}\")\n",
    "\n",
    "                # 删除旧版本特征（如果存在），然后合并\n",
    "                current_feature_df = current_feature_df.drop(columns=new_features_df.columns, errors='ignore')\n",
    "                current_feature_df = current_feature_df.merge(new_features_df, left_index=True, right_index=True, how='left')\n",
    "                loaded_features = current_feature_df.columns.tolist()\n",
    "        \n",
    "        # 更新特征字典\n",
    "        feature_dict[data_id] = current_feature_df\n",
    "        logger.info(f\"数据ID '{data_id}' 特征生成完成，最终特征数: {len(current_feature_df.columns)}\")\n",
    "\n",
    "    return feature_dict, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03461576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [Train&Infer] 特征交互核心逻辑 ---\n",
    "def extract_and_generate_interaction_features(\n",
    "        feature_dict: dict, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    根据特征重要性文件生成交互特征。\n",
    "    支持字典格式的特征数据。\n",
    "\n",
    "    Args:\n",
    "        feature_dict (dict): 特征数据框。\n",
    "    \"\"\"\n",
    "    # 1. 提取交互对\n",
    "    raw_feat_name = []\n",
    "    interaction_pairs = {}\n",
    "    operator_flags = sorted(config.OPERATOR_FLAGS, key=len, reverse=True)\n",
    "    for flag in operator_flags:\n",
    "        interaction_pairs[flag] = []\n",
    "    for feat in config.REMAIN_FEATURES:\n",
    "        matched_flag = next((flag for flag in operator_flags if feat.startswith(flag)), None)\n",
    "        if matched_flag is not None:\n",
    "            raw_parts = extract_raw_features(feat)\n",
    "            raw_feat_name.extend(raw_parts)\n",
    "            interaction_pairs[matched_flag].append(tuple(raw_parts))  # 转为元组\n",
    "        else:\n",
    "            raw_feat_name.append(feat)\n",
    "\n",
    "    # 2. 检查是否有特征缺失\n",
    "    for data_id, feature_df in feature_dict.items():\n",
    "        missing_features = [f for f in raw_feat_name if f not in feature_df.columns]\n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing 【RAW FEATURES】 in <{data_id}>: {missing_features}\")\n",
    "\n",
    "    # 3. 为每个数据ID生成交互特征\n",
    "    updated_feature_dict = {}\n",
    "    all_interaction_features = []\n",
    "    epsilon = 1e-6\n",
    "    zscore_cache = {}\n",
    "    asinh_feature_cache = {}\n",
    "    asinh_zscore_cache = {}\n",
    "    def compute_zscore(series: pd.Series) -> pd.Series:\n",
    "        std = series.std(ddof=0)\n",
    "        if pd.isna(std) or std < epsilon:\n",
    "            return pd.Series(0.0, index=series.index)\n",
    "        return (series - series.mean()) / std\n",
    "    def get_zscore(name: str) -> pd.Series:\n",
    "        if name not in zscore_cache:\n",
    "            zscore_cache[name] = compute_zscore(feature_df[name].astype(float))\n",
    "        return zscore_cache[name]\n",
    "    def get_asinh_series(name: str) -> pd.Series:\n",
    "        if name not in asinh_feature_cache:\n",
    "            asinh_feature_cache[name] = np.arcsinh(feature_df[name].astype(float))\n",
    "            asinh_zscore_cache[name] = compute_zscore(asinh_feature_cache[name])\n",
    "        return asinh_feature_cache[name]\n",
    "    def get_asinh_zscore(name: str) -> pd.Series:\n",
    "        if name not in asinh_feature_cache:\n",
    "            get_asinh_series(name)\n",
    "        return asinh_zscore_cache[name]\n",
    "\n",
    "    for data_id, feature_df in feature_dict.items():\n",
    "        logger.info(f\"\\n为数据ID '{data_id}' 生成交互特征...\")\n",
    "        \n",
    "        # 创建交互特征 - 使用字典收集所有特征，避免DataFrame碎片化\n",
    "        interaction_features_dict = {}\n",
    "\n",
    "        # 根据提取的交互对进行高效特征交互\n",
    "        for operator, pairs in interaction_pairs.items():\n",
    "            for f1, f2 in pairs:\n",
    "                if operator == 'mul':\n",
    "                    interaction_features_dict[f'mul_{f1}_{f2}'] = feature_df[f1] * feature_df[f2]\n",
    "                elif operator == 'sqmul':\n",
    "                    interaction_features_dict[f'sqmul_{f1}_{f2}'] = feature_df[f1] * (feature_df[f2] ** 2)\n",
    "                elif operator == 'sub':\n",
    "                    interaction_features_dict[f'sub_{f1}_{f2}'] = feature_df[f1] - feature_df[f2]\n",
    "                elif operator == 'add':\n",
    "                    interaction_features_dict[f'add_{f1}_{f2}'] = feature_df[f1] + feature_df[f2]\n",
    "                elif operator == 'div':\n",
    "                    interaction_features_dict[f'div_{f1}_{f2}'] = feature_df[f1] / (feature_df[f2] + epsilon)\n",
    "                elif operator == 'sq':\n",
    "                    interaction_features_dict[f'sq_{f1}'] = feature_df[f1] ** 2\n",
    "                elif operator == 'cross_mul':\n",
    "                    interaction_features_dict[f'cross_mul_{f1}_{f2}'] = feature_df[f1] * feature_df[f2]\n",
    "                elif operator == 'cross_sqmul':\n",
    "                    interaction_features_dict[f'cross_sqmul_{f1}_{f2}'] = feature_df[f1] * (feature_df[f2] ** 2)\n",
    "                elif operator == 'cross_add':\n",
    "                    interaction_features_dict[f'cross_add_{f1}_{f2}'] = feature_df[f1] + feature_df[f2]\n",
    "                elif operator == 'cross_sub':\n",
    "                    interaction_features_dict[f'cross_sub_{f1}_{f2}'] = feature_df[f1] - feature_df[f2]\n",
    "                elif operator == 'cross_div':\n",
    "                    interaction_features_dict[f'cross_div_{f1}_{f2}'] = feature_df[f1] / (feature_df[f2] + epsilon)\n",
    "                elif operator == 'asinh_add':\n",
    "                    asinh_z_f1 = get_asinh_zscore(f1)\n",
    "                    asinh_z_f2 = get_asinh_zscore(f2)\n",
    "                    interaction_features_dict[f'asinh_add_{f1}_{f2}'] = asinh_z_f1 + asinh_z_f2\n",
    "                elif operator == 'asinh_sub':\n",
    "                    asinh_z_f1 = get_asinh_zscore(f1)\n",
    "                    asinh_z_f2 = get_asinh_zscore(f2)\n",
    "                    interaction_features_dict[f'asinh_sub_{f1}_{f2}'] = asinh_z_f1 - asinh_z_f2\n",
    "                elif operator == 'asinh_add_raw':\n",
    "                    asinh_z_f1 = get_asinh_zscore(f1)\n",
    "                    z_f2 = get_zscore(f2)\n",
    "                    interaction_features_dict[f'asinh_add_raw_{f1}_{f2}'] = asinh_z_f1 + z_f2\n",
    "                elif operator == 'asinh_sub_raw':\n",
    "                    asinh_z_f1 = get_asinh_zscore(f1)\n",
    "                    z_f2 = get_zscore(f2)\n",
    "                    interaction_features_dict[f'asinh_sub_raw_{f1}_{f2}'] = asinh_z_f1 - z_f2\n",
    "                elif operator == 'raw_add_asinh':\n",
    "                    z_f1 = get_zscore(f1)\n",
    "                    asinh_z_f2 = get_asinh_zscore(f2)\n",
    "                    interaction_features_dict[f'raw_add_asinh_{f1}_{f2}'] = z_f1 + asinh_z_f2\n",
    "                elif operator == 'raw_sub_asinh':\n",
    "                    z_f1 = get_zscore(f1)\n",
    "                    asinh_z_f2 = get_asinh_zscore(f2)\n",
    "                    interaction_features_dict[f'raw_sub_asinh_{f1}_{f2}'] = z_f1 - asinh_z_f2\n",
    "                elif operator == 'asinh_mul':\n",
    "                    asinh_f1 = get_asinh_series(f1)\n",
    "                    interaction_features_dict[f'asinh_mul_{f1}_{f2}'] = asinh_f1 * feature_df[f2]\n",
    "                elif operator == 'raw_mul_asinh':\n",
    "                    asinh_f2 = get_asinh_series(f2)\n",
    "                    interaction_features_dict[f'raw_mul_asinh_{f1}_{f2}'] = feature_df[f1] * asinh_f2\n",
    "                elif operator == 'norm_add':\n",
    "                    norm_f1 = get_zscore(f1)\n",
    "                    norm_f2 = get_zscore(f2)\n",
    "                    interaction_features_dict[f'norm_add_{f1}_{f2}'] = norm_f1 + norm_f2\n",
    "                elif operator == 'norm_sub':\n",
    "                    norm_f1 = get_zscore(f1)\n",
    "                    norm_f2 = get_zscore(f2)\n",
    "                    interaction_features_dict[f'norm_sub_{f1}_{f2}'] = norm_f1 - norm_f2\n",
    "\n",
    "        # 一次性创建DataFrame，避免碎片化\n",
    "        if interaction_features_dict:\n",
    "            interaction_features = pd.DataFrame(interaction_features_dict, index=feature_df.index)\n",
    "        else:\n",
    "            interaction_features = pd.DataFrame(index=feature_df.index)\n",
    "        \n",
    "        if interaction_features.empty:\n",
    "            logger.info(f\"数据ID '{data_id}' 没有选择任何交互项类型，跳过。\")\n",
    "            updated_feature_dict[data_id] = feature_df.copy()\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"  数据ID '{data_id}' 成功创建 {len(interaction_features.columns)} 个交互特征\")\n",
    "        \n",
    "        # 合并特征\n",
    "        updated_feature_df = feature_df.drop(columns=interaction_features.columns, errors='ignore')\n",
    "        updated_feature_df = updated_feature_df.merge(interaction_features, left_index=True, right_index=True, how='left')\n",
    "        updated_feature_df = clean_feature_names(updated_feature_df, prefix=\"f_inter\")\n",
    "        \n",
    "        updated_feature_dict[data_id] = updated_feature_df\n",
    "\n",
    "    return updated_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3805e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        model_directory_path: str,\n",
    "    ): \n",
    "    global logger, log_file_path\n",
    "    logger, log_file_path = get_logger('Train', Path(os.path.join(model_directory_path, 'train_logs')), verbose=False)\n",
    "    global config\n",
    "    config.PROJECT_ROOT = Path(model_directory_path)\n",
    "    config.FEATURE_DIR = config.PROJECT_ROOT\n",
    "    run_output_dir = Path(model_directory_path)\n",
    "\n",
    "    try:\n",
    "        # precalculated features\n",
    "        loaded_feature_df, loaded_feature_name, loaded_y_train = load_precalculated_features(model_directory_path)\n",
    "        loaded_id = loaded_feature_df.index\n",
    "        \n",
    "        # data.py\n",
    "        X_data = {}\n",
    "        y_data = {}\n",
    "        X_train, y_train = filter_unloaded_ids(X_train, y_train, loaded_id)\n",
    "        X_data[\"0\"] = X_train\n",
    "        if isinstance(y_train, pd.Series):\n",
    "            y_train = y_train.to_frame('structural_breakpoint')\n",
    "        if isinstance(loaded_y_train, pd.Series):\n",
    "            loaded_y_train = loaded_y_train.to_frame('structural_breakpoint')\n",
    "        y_data[\"0\"] = y_train\n",
    "        logger.warning(f\"训练数据切片. Load -> feature shape: {loaded_feature_df.shape}, y shape: {loaded_y_train.shape}\")\n",
    "        logger.warning(f\"训练数据切片. New -> X shape: {X_train.shape}, y shape: {y_train.shape}\")\n",
    "\n",
    "        # feature.py\n",
    "        if X_train.shape[0] > 0:\n",
    "            feature_dict, metadata = generate_features(X_data, use_tqdm=True, parallel=True)\n",
    "            feature_dict = extract_and_generate_interaction_features(feature_dict)\n",
    "            for data_id, feature_df in feature_dict.items():\n",
    "                missing_features = [f for f in config.REMAIN_FEATURES if f not in feature_df.columns]\n",
    "                if missing_features:\n",
    "                    logger.warning(f\"Missing REMAIN_FEATURES in <{data_id}> before filter: {missing_features}\")\n",
    "\n",
    "            # 拼接特征数据\n",
    "            data_ids = list(feature_dict.keys())\n",
    "            feature_dfs = []\n",
    "            for data_id in data_ids:\n",
    "                df = feature_dict[data_id].copy()\n",
    "                feature_dfs.append(df)\n",
    "            if len(feature_dfs) == 1:\n",
    "                concatenated_df = feature_dfs[0]\n",
    "            else:\n",
    "                concatenated_df = pd.concat(feature_dfs, axis=0, ignore_index=False)\n",
    "            feature_df = concatenated_df[config.REMAIN_FEATURES]\n",
    "        else:\n",
    "            feature_df = pd.DataFrame(index=X_train.index)\n",
    "            data_ids = [\"0\"]\n",
    "\n",
    "        if loaded_feature_df.shape[0] > 0:\n",
    "            logger.warning(f\"加载特征id范围: {loaded_id.min()} ~ {loaded_id.max()}\")\n",
    "            logger.warning(f\"新增特征id范围: {feature_df.index.min()} ~ {feature_df.index.max()}\")\n",
    "            feature_df = pd.concat([loaded_feature_df, feature_df], axis=0, ignore_index=False)\n",
    "        logger.warning(\"--- 完整特征形状 ---\")\n",
    "        logger.warning(feature_df.shape)\n",
    "        logger.info(\"-----------------------------\")\n",
    "        logger.info(f\"生成/更新完成。总特征数: {len(feature_df.columns)}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        logger.info(\"Starting training and evaluation pipeline...\")\n",
    "        logger.info(f\"Model Parameters: {json.dumps(config.LGBM_PARAMS, indent=4)}\")\n",
    "\n",
    "        # train.py\n",
    "        # 1. 加载特征和标签\n",
    "        y_train = pd.concat(list(y_data.values()), axis=0, ignore_index=False)\n",
    "        if loaded_y_train.shape[0] > 0:\n",
    "            logger.warning(f\"加载标签id范围: {loaded_y_train.index.min()} ~ {loaded_y_train.index.max()}\")\n",
    "            logger.warning(f\"新增标签id范围: {y_train.index.min()} ~ {y_train.index.max()}\")\n",
    "            y_train = pd.concat([loaded_y_train, y_train], axis=0, ignore_index=False)\n",
    "        # 确保对齐\n",
    "        common_index = feature_df.index.intersection(y_train.index)\n",
    "        feature_df = feature_df.loc[common_index]\n",
    "        y_train = y_train.loc[common_index]['structural_breakpoint'].astype(int)\n",
    "        logger.warning(f\"训练数据已对齐. X shape: {feature_df.shape}, y shape: {y_train.shape}\")\n",
    "        \n",
    "        # # 2. \n",
    "        # 特征选择\n",
    "        if len(config.REMAIN_FEATURES) > 0:\n",
    "            feature_df = feature_df[config.REMAIN_FEATURES]\n",
    "            for col in feature_df.columns:\n",
    "                null_ratio = feature_df[col].isnull().sum() / len(feature_df)\n",
    "                zero_ratio = (feature_df[col] == 0).sum() / len(feature_df)\n",
    "                if null_ratio > 0.0 or zero_ratio > 0.5:\n",
    "                    logger.warning(f\"    - '{col}': 空值比例={null_ratio:.2%}, 零值比例={zero_ratio:.2%}\")\n",
    "        if feature_df is None:\n",
    "            logger.error(\"特征加载失败，训练中止。\")\n",
    "            return None, None\n",
    "\n",
    "        logger.info(f\"--- 使用的特征列表 (共 {len(feature_df.columns)} 个) ---\")\n",
    "        logger.info(feature_df.columns.tolist())\n",
    "        logger.info(\"-\" * min(50, len(str(feature_df.columns.tolist()))))\n",
    "        \n",
    "        # 3. 模型训练\n",
    "        oof_preds_dict = {}\n",
    "        for n, model_name in enumerate(config.MODEL):\n",
    "            logger.warning(f\"Model: {model_name}\")\n",
    "            # 3. 交叉验证\n",
    "            if config.TRAIN_STRATEGY == 'cv':\n",
    "                logger.info(\"Starting 5-fold cross-validation with enhanced data strategy...\")\n",
    "\n",
    "                oof_preds = np.zeros(len(feature_df))\n",
    "                models = []\n",
    "                feature_importances = pd.DataFrame(index=feature_df.columns)\n",
    "                permutation_results = pd.DataFrame(index=feature_df.columns)\n",
    "                fold_metrics = []\n",
    "                \n",
    "                # 使用增强数据交叉验证策略\n",
    "                cv_params = config.CV_PARAMS[n]\n",
    "                cv_iterator = StratifiedKFold(\n",
    "                    n_splits=cv_params['n_splits'],\n",
    "                    shuffle=cv_params['shuffle'],\n",
    "                    random_state=cv_params['random_state']\n",
    "                ).split(feature_df, y_train)\n",
    "                for fold, (train_idx, val_idx) in enumerate(cv_iterator):\n",
    "                    logger.info(f\"--- Fold {fold+1}/{config.CV_PARAMS[n]['n_splits']} ---\")\n",
    "                    fold_start_time = time.time()\n",
    "\n",
    "                    X_train_fold, y_train_fold = feature_df.iloc[train_idx], y_train.iloc[train_idx]\n",
    "                    X_val_fold, y_val_fold = feature_df.iloc[val_idx], y_train.iloc[val_idx]\n",
    "                    logger.warning(f\"训练数据: {X_train_fold.shape}, 验证数据: {X_val_fold.shape}\")\n",
    "\n",
    "                    # 配置模型\n",
    "                    if model_name == 'LGB':\n",
    "                        model = lgb.LGBMClassifier(**config.LGBM_PARAMS)\n",
    "                        callbacks = []\n",
    "                        if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0:\n",
    "                            callbacks.append(lgb.early_stopping(config.EARLY_STOPPING_ROUNDS, verbose=False))\n",
    "                        model.fit(\n",
    "                            X_train_fold, y_train_fold,\n",
    "                            eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n",
    "                            eval_names=['train', 'valid'],\n",
    "                            eval_metric='auc',\n",
    "                            callbacks=callbacks\n",
    "                        )\n",
    "                        train_auc = model.best_score_['train']['auc']\n",
    "                    elif model_name == 'CAT':\n",
    "                        model = cat.CatBoostClassifier(**config.CAT_PARAMS)\n",
    "                        model.fit(\n",
    "                            X_train_fold, y_train_fold, \n",
    "                            eval_set=[(X_val_fold, y_val_fold)],\n",
    "                            early_stopping_rounds=(config.EARLY_STOPPING_ROUNDS if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0 else None),\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        train_preds = model.predict_proba(X_train_fold)[:, 1]\n",
    "                        # 确保y_train_fold是NumPy格式，兼容cuDF\n",
    "                        y_train_fold_numpy = y_train_fold.to_numpy() if hasattr(y_train_fold, 'to_numpy') else y_train_fold\n",
    "                        train_auc = roc_auc_score(y_train_fold_numpy, train_preds)\n",
    "                    elif model_name == 'XGB':\n",
    "                        if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0:\n",
    "                            config.XGB_PARAMS['early_stopping_rounds'] = config.EARLY_STOPPING_ROUNDS\n",
    "                        model = xgb.XGBClassifier(**config.XGB_PARAMS)\n",
    "                        model.fit(\n",
    "                            X_train_fold, y_train_fold, \n",
    "                            eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        train_preds = model.predict_proba(X_train_fold)[:, 1]\n",
    "                        train_auc = roc_auc_score(y_train_fold, train_preds)\n",
    "                    else:\n",
    "                        raise ValueError(\"Unknown model_name\")\n",
    "\n",
    "                    # 预测验证集\n",
    "                    preds = model.predict_proba(X_val_fold)[:, 1]\n",
    "                    if hasattr(preds, 'get'):\n",
    "                        preds = preds.get()\n",
    "\n",
    "                    oof_preds[val_idx] = preds\n",
    "                    models.append(model)\n",
    "                    feature_importances[f'fold_{fold+1}'] = model.feature_importances_\n",
    "                    \n",
    "                    fold_auc = roc_auc_score(y_val_fold, preds)\n",
    "                    logger.warning(f\"Fold {fold+1} Train AUC: {train_auc:.5f}, Val AUC: {fold_auc:.5f}\")\n",
    "\n",
    "                    # 记录早停的 step（best_iteration）\n",
    "                    best_iteration = None\n",
    "                    if model_name == 'LGB':\n",
    "                        best_iteration = getattr(model, 'best_iteration_', None)\n",
    "                    elif model_name == 'CAT':\n",
    "                        try:\n",
    "                            best_iteration = model.get_best_iteration()\n",
    "                        except Exception:\n",
    "                            best_iteration = getattr(model, 'best_iteration_', None)\n",
    "                    elif model_name == 'XGB':\n",
    "                        best_iteration = getattr(model, 'best_iteration', None)\n",
    "                    logger.warning(f\"Fold {fold+1} Early stopping step (best_iteration): {best_iteration}\")\n",
    "\n",
    "                    # 保存到元数据结构中\n",
    "                    fold_metrics.append({\n",
    "                        'fold': fold + 1,\n",
    "                        'train_auc': float(train_auc),\n",
    "                        'val_auc': float(fold_auc),\n",
    "                        'best_iteration': int(best_iteration) if best_iteration is not None else None,\n",
    "                    })\n",
    "\n",
    "                    fold_duration = time.time() - fold_start_time\n",
    "                    logger.warning(f\"Fold {fold+1} finished in {fold_duration:.2f}s\")\n",
    "\n",
    "                overall_oof_auc = roc_auc_score(y_train, oof_preds)\n",
    "                logger.warning(f\"Overall OOF AUC: {overall_oof_auc:.5f}\")\n",
    "                oof_preds_dict[model_name] = oof_preds\n",
    "\n",
    "                # 6. 保存模型\n",
    "                for i, model in tqdm(enumerate(models), total=len(models), desc=\"Saving models\"):\n",
    "                    joblib.dump(model, run_output_dir / f'online_{model_name}_model_fold_{i+1}.pkl')\n",
    "                logger.info(\"Models saved.\")\n",
    "        \n",
    "            # 3. 多个全量模型\n",
    "            elif config.TRAIN_STRATEGY == 'multi':\n",
    "                logger.info(\"Starting single model training...\")\n",
    "                \n",
    "                # 配置模型\n",
    "                if model_name == 'LGB':\n",
    "                    for i, params in enumerate(config.LGBM_PARAMS):\n",
    "                        model = lgb.LGBMClassifier(**params)\n",
    "                        model.fit(\n",
    "                            feature_df, y_train,\n",
    "                            eval_set=[(feature_df, y_train)],\n",
    "                            eval_names=['train', 'valid'],\n",
    "                            eval_metric='auc',\n",
    "                        )\n",
    "                        train_auc = model.best_score_['train']['auc']\n",
    "                        logger.warning(f\"Train AUC: {train_auc:.5f}\")\n",
    "                        joblib.dump(model, run_output_dir / f'online_{model_name}_model_{i}.pkl')\n",
    "                        logger.info(f\"Model {model_name} {i} saved.\")\n",
    "                elif model_name == 'CAT':\n",
    "                    for i, params in enumerate(config.CAT_PARAMS):\n",
    "                        model = cat.CatBoostClassifier(**params)\n",
    "                        model.fit(\n",
    "                            feature_df, y_train, \n",
    "                            eval_set=[(feature_df, y_train)],\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        train_preds = model.predict_proba(feature_df)[:, 1]\n",
    "                        # 确保y_train_fold是NumPy格式，兼容cuDF\n",
    "                        y_train_numpy = y_train.to_numpy() if hasattr(y_train, 'to_numpy') else y_train\n",
    "                        train_auc = roc_auc_score(y_train_numpy, train_preds)\n",
    "                        logger.warning(f\"Train AUC: {train_auc:.5f}\")\n",
    "                        joblib.dump(model, run_output_dir / f'online_{model_name}_model_{i}.pkl')\n",
    "                        logger.info(f\"Model {model_name} {i} saved.\")\n",
    "                elif model_name == 'XGB':\n",
    "                    for i, params in enumerate(config.XGB_PARAMS):\n",
    "                        model = xgb.XGBClassifier(**params)\n",
    "                        model.fit(\n",
    "                            feature_df, y_train, \n",
    "                            eval_set=[(feature_df, y_train)],\n",
    "                            verbose=False\n",
    "                        )\n",
    "                        train_preds = model.predict_proba(feature_df)[:, 1]\n",
    "                        train_auc = roc_auc_score(y_train, train_preds)\n",
    "                        logger.warning(f\"Train AUC: {train_auc:.5f}\")\n",
    "                        joblib.dump(model, run_output_dir / f'online_{model_name}_model_{i}.pkl')\n",
    "                        logger.info(f\"Model {model_name} {i} saved.\")\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown model_name\")\n",
    "\n",
    "        # ensemble\n",
    "        if config.TRAIN_STRATEGY == 'cv':\n",
    "            oof_preds = np.zeros(len(feature_df))\n",
    "            for model_name in config.MODEL:\n",
    "                oof_preds += oof_preds_dict[model_name] / len(config.MODEL)\n",
    "            overall_oof_auc = roc_auc_score(y_train, oof_preds)\n",
    "            logger.warning(f\"Ensemble: Overall OOF AUC: {overall_oof_auc:.5f}\")\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        logger.warning(f\"训练流程结束，总耗时: {duration:.2f} 秒。\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"训练过程中发生错误: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fbad0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [Infer] 加载模型 ---\n",
    "def load_models(model_directory_path):\n",
    "    \"\"\"Load all LightGBM model files saved with joblib and prepare them for ensemble\"\"\"\n",
    "    local_models = []\n",
    "    online_models = []\n",
    "    dirpath = Path(model_directory_path)\n",
    "    model_files = list(dirpath.glob('*.pkl'))\n",
    "    \n",
    "    if not model_files:\n",
    "        logger.warning(f\"Warning: No model files found under {model_directory_path}!\")\n",
    "        return \n",
    "    logger.warning(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            logger.warning(f\"Loading model: {model_path}\")\n",
    "            model = joblib.load(model_path)\n",
    "            if 'local' in model_path.name:\n",
    "                local_models.append(model)\n",
    "            elif 'online' in model_path.name:\n",
    "                online_models.append(model)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    if len(online_models) > 0:\n",
    "        logger.warning(f\"Loaded {len(online_models)} online models.\")\n",
    "        return online_models\n",
    "    else:\n",
    "        logger.warning(\"Warning: No online models loaded!\")\n",
    "        logger.warning(f\"Loaded {len(local_models)} local models.\")\n",
    "        return local_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd2bf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- [Infer] 生成特征 ---\n",
    "def generate_features_infer_parallel(\n",
    "        X_data,\n",
    "        funcs_to_run: list = None,\n",
    "        trans_to_run: list = None,\n",
    "        use_tqdm: bool = False,\n",
    "        trans_funcs_dict: dict = None,\n",
    "        trans_feats_dict: dict = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    推理阶段：以“特征函数”为并行单元生成特征（一次只处理当前批的数据，通常只有一个样本）。\n",
    "    返回结构与 generate_features 保持一致：{data_id: feature_df}, metadata\n",
    "    \"\"\"\n",
    "    # 输入规范化\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_data_dict = {\"0\": X_data}\n",
    "    elif isinstance(X_data, dict):\n",
    "        X_data_dict = X_data\n",
    "    else:\n",
    "        raise ValueError(\"X_data必须是pd.DataFrame或dict类型\")\n",
    "\n",
    "    # 选择要运行的特征函数（默认：跳过实验性函数）\n",
    "    if funcs_to_run is None:\n",
    "        funcs_to_run = [\n",
    "            f for f in FEATURE_REGISTRY.keys()\n",
    "            if f not in config.EXPERIMENTAL_FEATURES\n",
    "        ]\n",
    "    valid_funcs_to_run = [f for f in funcs_to_run if f in FEATURE_REGISTRY]\n",
    "\n",
    "    # trans-funcs 对齐\n",
    "    if trans_funcs_dict is None or trans_feats_dict is None:\n",
    "        trans_funcs_dict, trans_feats_dict = extract_trans_funcs_dict()\n",
    "\n",
    "    feature_dict = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for data_id, X_df in X_data_dict.items():\n",
    "        unique_ids = X_df.index.get_level_values('id').unique()\n",
    "        current_feature_df = pd.DataFrame(index=unique_ids)\n",
    "\n",
    "        # 先进行时序变换（按变换函数顺序执行，计算量主要在后续特征函数）\n",
    "        transformed_data = apply_transformation(X_df, trans_to_run)\n",
    "\n",
    "        # for each mode 按(func)并行地生成特征\n",
    "        for mode_name, mode_df in transformed_data.items():\n",
    "            # s = time.time()\n",
    "            allowed_func_ids = trans_funcs_dict.get(mode_name, {})\n",
    "            funcs_for_mode = [\n",
    "                fname for fname in valid_funcs_to_run\n",
    "                if FEATURE_REGISTRY[fname]['func_id'] in allowed_func_ids\n",
    "            ]\n",
    "            if not funcs_for_mode:\n",
    "                continue\n",
    "            selected_features = trans_feats_dict.get(mode_name, set())\n",
    "\n",
    "            def run_single_feature(func_name):\n",
    "                feature_info = FEATURE_REGISTRY[func_name]\n",
    "                func = feature_info['func']\n",
    "                func_id = feature_info['func_id']\n",
    "                try:\n",
    "                    # 逐 id 顺序计算，避免在小样本上产生额外进程/序列化开销\n",
    "                    df_res = _apply_feature_func_sequential(func, mode_df, use_tqdm=False, selected_features=selected_features)\n",
    "                    df_res.columns = [f\"{mode_name}_{func_id}_{col}\" for col in df_res.columns]\n",
    "                    df_res = clean_feature_names(df_res)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"特征函数 {func_name} 失败: {e}\")\n",
    "                    df_res = pd.DataFrame(index=mode_df.index.get_level_values('id').unique())\n",
    "                return df_res\n",
    "\n",
    "            new_feature_dfs = Parallel(n_jobs=config.N_JOBS, prefer=\"threads\")(\n",
    "                delayed(run_single_feature)(fname) for fname in funcs_for_mode\n",
    "            )\n",
    "\n",
    "            if len(new_feature_dfs) > 0:\n",
    "                merged_mode_df = pd.concat(new_feature_dfs, axis=1)\n",
    "                current_feature_df = current_feature_df.drop(columns=merged_mode_df.columns, errors='ignore')\n",
    "                current_feature_df = current_feature_df.merge(merged_mode_df, left_index=True, right_index=True, how='left')\n",
    "            # e = time.time()\n",
    "            # print(f\"数据ID '{data_id}' 的模态 '{mode_name}' 特征生成完成，耗时: {e - s:.2f} 秒，共生成 {len(merged_mode_df.columns) if len(new_feature_dfs) > 0 else 0} 个特征\")\n",
    "\n",
    "        feature_dict[data_id] = current_feature_df\n",
    "\n",
    "        # # 按(mode, func)并行地生成特征\n",
    "        # tasks = []\n",
    "        # for mode_name, mode_df in transformed_data.items():\n",
    "        #     allowed_func_ids = set(trans_funcs_dict.get(mode_name, []))\n",
    "        #     funcs_for_mode = [\n",
    "        #         fname for fname in valid_funcs_to_run\n",
    "        #         if FEATURE_REGISTRY[fname]['func_id'] in allowed_func_ids\n",
    "        #     ]\n",
    "        #     for func_name in funcs_for_mode:\n",
    "        #         tasks.append((mode_name, mode_df, func_name))\n",
    "\n",
    "        # def run_mode_func(mode_name, mode_df, func_name):\n",
    "        #     feature_info = FEATURE_REGISTRY[func_name]\n",
    "        #     func = feature_info['func']\n",
    "        #     func_id = feature_info['func_id']\n",
    "        #     try:\n",
    "        #         df_res = _apply_feature_func_sequential(func, mode_df, use_tqdm=False)\n",
    "        #         df_res.columns = [f\"{mode_name}_{func_id}_{col}\" for col in df_res.columns]\n",
    "        #         df_res = clean_feature_names(df_res)\n",
    "        #     except Exception as e:\n",
    "        #         logger.warning(f\"特征函数 {func_name} 在模态 {mode_name} 上失败: {e}\")\n",
    "        #         df_res = pd.DataFrame(index=mode_df.index.get_level_values('id').unique())\n",
    "        #     return df_res\n",
    "\n",
    "        # # 并行执行 (mode, func) 任务\n",
    "        # # s = time.time()\n",
    "        # new_feature_dfs = Parallel(n_jobs=config.N_JOBS, prefer=\"threads\")(\n",
    "        #     delayed(run_mode_func)(mode_name, mode_df, func_name) for mode_name, mode_df, func_name in tasks\n",
    "        # )\n",
    "\n",
    "        # # 合并结果\n",
    "        # if len(new_feature_dfs) > 0:\n",
    "        #     merged_mode_df = pd.concat(new_feature_dfs, axis=1)\n",
    "        #     current_feature_df = current_feature_df.drop(columns=merged_mode_df.columns, errors='ignore')\n",
    "        #     current_feature_df = current_feature_df.merge(\n",
    "        #         merged_mode_df, left_index=True, right_index=True, how='left'\n",
    "        #     )\n",
    "        # # e = time.time()\n",
    "        # # print(f\"数据ID '{data_id}' 全部模态特征生成完成，耗时: {e - s:.2f} 秒，\"\n",
    "        # #       f\"共生成 {len(merged_mode_df.columns) if len(new_feature_dfs) > 0 else 0} 个特征\")\n",
    "        \n",
    "        # feature_dict[data_id] = current_feature_df\n",
    "\n",
    "    return feature_dict, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dde3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "        X_test: typing.Iterable[pd.DataFrame],\n",
    "        model_directory_path: str,\n",
    "    ):\n",
    "    global logger, log_file_path\n",
    "    logger, log_file_path = get_logger('Inference', Path(os.path.join(model_directory_path, 'infer_logs')), verbose=False)\n",
    "    global config\n",
    "    config.PROJECT_ROOT = Path(model_directory_path)\n",
    "    config.FEATURE_DIR = config.PROJECT_ROOT\n",
    "    \n",
    "    # 加载模型\n",
    "    models = load_models(model_directory_path)\n",
    "    # 加载各变换应运行的函数映射\n",
    "    trans_funcs_dict, trans_feats_dict = extract_trans_funcs_dict()\n",
    "\n",
    "    yield  # Ready\n",
    "\n",
    "    # X_test 只能迭代一次；拿到一条就立刻算、立刻推理\n",
    "    for X_df in tqdm(X_test, desc=\"Inference Progress\"):\n",
    "        X_data = {\"0\": X_df}\n",
    "        # st = time.time()\n",
    "        feature_dict, metadata = generate_features_infer_parallel(\n",
    "            X_data, use_tqdm=False, trans_funcs_dict=trans_funcs_dict, trans_feats_dict=trans_feats_dict\n",
    "        )\n",
    "        feature_dict = extract_and_generate_interaction_features(feature_dict)\n",
    "        for data_id, feature_df in feature_dict.items():\n",
    "            missing_features = [f for f in config.REMAIN_FEATURES if f not in feature_df.columns]\n",
    "            if missing_features:\n",
    "                logger.warning(f\"Missing REMAIN_FEATURES in <{data_id}> before filter: {missing_features}\")\n",
    "        # et = time.time()\n",
    "        # print(f'特征用时 {et-st}')\n",
    "\n",
    "        # 拼接特征数据\n",
    "        data_ids = list(feature_dict.keys())\n",
    "        feature_dfs = [feature_dict[data_id].copy() for data_id in data_ids]\n",
    "        concatenated_df = feature_dfs[0] if len(feature_dfs) == 1 else pd.concat(feature_dfs, axis=0, ignore_index=False)\n",
    "        feature_df = concatenated_df[config.REMAIN_FEATURES]\n",
    "        logger.info(feature_df)\n",
    "        logger.info(\"--- 生成后完整特征列表 ---\")\n",
    "        logger.info(f\"{feature_df.columns.tolist()}\")\n",
    "        logger.info(\"-----------------------------\")\n",
    "        logger.info(f\"生成/更新完成。总特征数: {len(feature_df.columns)}\")\n",
    "\n",
    "        def ensemble_predict(models, X):\n",
    "            preds = [model.predict_proba(X)[:, 1] for model in models]\n",
    "            if len(preds) == 0:\n",
    "                logger.warning(\"No predictions generated, returning zeros.\")\n",
    "                return np.zeros(len(X))\n",
    "            return np.mean(preds, axis=0)\n",
    "        # st = time.time()\n",
    "        prediction = ensemble_predict(models, feature_df)\n",
    "        # et = time.time()\n",
    "        # print(f'推理用时 {et-st}')\n",
    "        # prediction = 1 - prediction\n",
    "        yield prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "196f3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m20:13:57\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m20:13:57\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m20:14:02\u001b[0m started\n",
      "\u001b[32m20:14:02\u001b[0m running local test\n",
      "\u001b[32m20:14:02\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m20:14:02\u001b[0m \n",
      "\u001b[32m20:14:05\u001b[0m starting unstructured loop...\n",
      "\u001b[32m20:14:05\u001b[0m executing - command=train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
      "data\\X_train.parquet: already exists, file length match\n",
      "data\\X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
      "data\\X_test.reduced.parquet: already exists, file length match\n",
      "data\\y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
      "data\\y_train.parquet: already exists, file length match\n",
      "data\\y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
      "data\\y_test.reduced.parquet: already exists, file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "未指定特征文件，将尝试加载最新版本。\n",
      "正在从 features_20250926_220622.parquet 加载特征...\n",
      "加载字典格式特征文件成功，包含数据ID: ['0']\n",
      "特征拼接成功，使用数据ID: ['0']，共 107 个特征，9901 行数据。\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 训练数据切片. Load -> feature shape: (9901, 107), y shape: (9901, 1)\n",
      "WARNING: 训练数据切片. New -> X shape: (233394, 2), y shape: (100, 1)\n",
      "WARNING: 变换模式: ['RAW', 'CUMSUM', 'DIFF']\n",
      "WARNING: 变换-特征函数匹配: {'RAW': {'2', '1', '7', '8', '10', '3'}, 'CUMSUM': {'2', '9', '1', '7', '5', '4', '8', '3'}, 'DIFF': {'2', '8', '7'}}\n",
      "WARNING: 变换-特征名称匹配: {'RAW': {'agg_linear_trend_attr_slope_chunk_len_10_f_agg_mean', 'change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4', 'ratio_beyond_r_sigma_0_5', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_4', 'ratio_beyond_r_sigma_1_5', 'ratio_beyond_r_sigma_1', 'agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0', 'quantile_0_1', 'index_mass_quantile_q_0_8', 'percentage_of_reoccurring_values_to_all_values', 'ks_stat', 'approx_entropy', 'agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_max', 'detrend_volatility_normalized', 'linear_trend_attr_pvalue', 'shapiro_pvalue', 'percentage_of_reoccurring_datapoints_to_all_datapoints', 'stats_std', 'ad_stat', 'change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_4', 'stats_min', 'friedrich_coefficients_coeff_3_m_3_r_30', 'stats_median', 'quantile_0_4', 'index_mass_quantile_q_0_1', 'rpt_cost_cosine', 'bartlett_pvalue', 'bartlett_stat', 'ratio_value_number_to_time_series_length', 'energy_ratio_by_chunks_num_segments_10_segment_focus_9', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2', 'sample_entropy', 'stats_kurt', 'stats_cv', 'katz_fd', 'change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_2', 'count_above_0', 'stats_mean', 'higuchi_fd', 'ratio_beyond_r_sigma_3', 'benford_correlation'}, 'CUMSUM': {'ks_pvalue', 'cond_entropy', 'ad_pvalue', 'change_quantiles_f_agg_mean_isabs_True_qh_1_0_ql_0_4', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0', 'ar_coefficient_coeff_2_k_10', 'hjorth_complexity', 'adf_stat', 'detrend_volatility_normalized', 'stats_theil_sen_slope', 'dominant_freq', 'first_location_of_maximum', 'trend_normalized_slope', 'friedrich_coefficients_coeff_3_m_3_r_30', 'wilcoxon_stat', 'stats_max', 'stats_range', 'autocorr_lag1', 'ar_residuals_s1_pred_mean', 'adf_pvalue', 'kpss_pvalue', 'katz_fd', 'benford_correlation'}, 'DIFF': {'change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2', 'change_quantiles_f_agg_var_isabs_True_qh_0_4_ql_0_2', 'sample_entropy', 'spectral_entropy', 'change_quantiles_f_agg_var_isabs_False_qh_1_0_ql_0_2', 'hjorth_complexity', 'levene_pvalue', 'bartlett_pvalue', 'jb_pvalue', 'ar_coefficient_coeff_2_k_10', 'benford_correlation'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running distribution_stats_features (parallel): 100%|██████████| 100/100 [00:04<00:00, 22.00it/s]\n",
      "Running test_stats_features_first (parallel): 100%|██████████| 100/100 [00:00<00:00, 354.51it/s]\n",
      "Running test_stats_features_second (parallel): 100%|██████████| 100/100 [00:00<00:00, 101.67it/s]\n",
      "Running trend_features (parallel): 100%|██████████| 100/100 [00:00<00:00, 255.08it/s]\n",
      "Running entropy_features_first (parallel): 100%|██████████| 100/100 [00:21<00:00,  4.69it/s]\n",
      "Running entropy_features_second (parallel): 100%|██████████| 100/100 [00:00<00:00, 367.37it/s]\n",
      "Running tsfresh_features_first (parallel): 100%|██████████| 100/100 [00:01<00:00, 52.81it/s]\n",
      "Running rupture_cost_features (parallel): 100%|██████████| 100/100 [00:01<00:00, 66.12it/s]\n",
      "Running distribution_stats_features (parallel): 100%|██████████| 100/100 [00:04<00:00, 22.93it/s]\n",
      "Running test_stats_features_first (parallel): 100%|██████████| 100/100 [00:00<00:00, 293.04it/s]\n",
      "Running test_stats_features_second (parallel): 100%|██████████| 100/100 [00:01<00:00, 99.59it/s]\n",
      "Running trend_features (parallel): 100%|██████████| 100/100 [00:00<00:00, 291.23it/s]\n",
      "Running oscillation_features (parallel): 100%|██████████| 100/100 [00:00<00:00, 667.90it/s]\n",
      "Running cyclic_features (parallel): 100%|██████████| 100/100 [00:00<00:00, 416.58it/s]\n",
      "Running entropy_features_first (parallel): 100%|██████████| 100/100 [00:00<00:00, 299.22it/s]\n",
      "Running entropy_features_second (parallel): 100%|██████████| 100/100 [00:00<00:00, 380.57it/s]\n",
      "Running tsfresh_features_first (parallel): 100%|██████████| 100/100 [00:00<00:00, 190.80it/s]\n",
      "Running ar_model_features (parallel): 100%|██████████| 100/100 [00:00<00:00, 201.66it/s]\n",
      "Running test_stats_features_first (parallel): 100%|██████████| 100/100 [00:00<00:00, 351.12it/s]\n",
      "Running test_stats_features_second (parallel): 100%|██████████| 100/100 [00:00<00:00, 256.01it/s]\n",
      "Running entropy_features_first (parallel): 100%|██████████| 100/100 [00:00<00:00, 259.64it/s]\n",
      "Running entropy_features_second (parallel): 100%|██████████| 100/100 [00:00<00:00, 386.63it/s]\n",
      "Running tsfresh_features_first (parallel): 100%|██████████| 100/100 [00:00<00:00, 210.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: 加载特征id范围: 0 ~ 9900\n",
      "WARNING: 新增特征id范围: 9901 ~ 10000\n",
      "WARNING: --- 完整特征形状 ---\n",
      "WARNING: (10001, 107)\n",
      "WARNING: 加载标签id范围: 0 ~ 9900\n",
      "WARNING: 新增标签id范围: 9901 ~ 10000\n",
      "WARNING: 训练数据已对齐. X shape: (10001, 107), y shape: (10001,)\n",
      "WARNING:     - 'div_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_diff': 空值比例=0.00%, 零值比例=74.89%\n",
      "WARNING:     - 'sqmul_CUMSUM_1_stats_max_ratio_to_whole_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left': 空值比例=0.00%, 零值比例=74.98%\n",
      "WARNING:     - 'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_whole': 空值比例=0.00%, 零值比例=74.98%\n",
      "WARNING:     - 'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left': 空值比例=0.00%, 零值比例=74.98%\n",
      "WARNING:     - 'sqmul_RAW_2_ks_stat_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left': 空值比例=0.00%, 零值比例=74.98%\n",
      "WARNING:     - 'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_CUMSUM_5_dominant_freq_ratio_to_whole_right': 空值比例=0.00%, 零值比例=74.98%\n",
      "WARNING:     - 'mul_RAW_8_ratio_value_number_to_time_series_length_diff_CUMSUM_3_trend_normalized_slope_whole': 空值比例=0.00%, 零值比例=74.90%\n",
      "WARNING:     - 'sub_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_ratio_to_whole_left': 空值比例=0.00%, 零值比例=74.98%\n",
      "WARNING:     - 'div_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left': 空值比例=0.00%, 零值比例=74.98%\n",
      "WARNING: Model: LGB\n",
      "WARNING: 训练数据: (8000, 107), 验证数据: (2001, 107)\n",
      "WARNING: Fold 1 Train AUC: 0.99682, Val AUC: 0.90836\n",
      "WARNING: Fold 1 Early stopping step (best_iteration): 0\n",
      "WARNING: Fold 1 finished in 44.18s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 2 Train AUC: 0.99713, Val AUC: 0.89110\n",
      "WARNING: Fold 2 Early stopping step (best_iteration): 0\n",
      "WARNING: Fold 2 finished in 45.53s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 3 Train AUC: 0.99710, Val AUC: 0.90980\n",
      "WARNING: Fold 3 Early stopping step (best_iteration): 0\n",
      "WARNING: Fold 3 finished in 43.90s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 4 Train AUC: 0.99673, Val AUC: 0.90305\n",
      "WARNING: Fold 4 Early stopping step (best_iteration): 0\n",
      "WARNING: Fold 4 finished in 43.39s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 5 Train AUC: 0.99689, Val AUC: 0.90274\n",
      "WARNING: Fold 5 Early stopping step (best_iteration): 0\n",
      "WARNING: Fold 5 finished in 43.68s\n",
      "WARNING: Overall OOF AUC: 0.90289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving models: 100%|██████████| 5/5 [00:02<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Model: XGB\n",
      "WARNING: 训练数据: (8000, 107), 验证数据: (2001, 107)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Fold 1 Train AUC: 0.99810, Val AUC: 0.90834\n",
      "WARNING: Fold 1 Early stopping step (best_iteration): None\n",
      "WARNING: Fold 1 finished in 33.59s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 2 Train AUC: 0.99825, Val AUC: 0.90479\n",
      "WARNING: Fold 2 Early stopping step (best_iteration): None\n",
      "WARNING: Fold 2 finished in 32.28s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 3 Train AUC: 0.99816, Val AUC: 0.89921\n",
      "WARNING: Fold 3 Early stopping step (best_iteration): None\n",
      "WARNING: Fold 3 finished in 33.45s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 4 Train AUC: 0.99865, Val AUC: 0.89143\n",
      "WARNING: Fold 4 Early stopping step (best_iteration): None\n",
      "WARNING: Fold 4 finished in 32.70s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 5 Train AUC: 0.99842, Val AUC: 0.91219\n",
      "WARNING: Fold 5 Early stopping step (best_iteration): None\n",
      "WARNING: Fold 5 finished in 32.48s\n",
      "WARNING: Overall OOF AUC: 0.90300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving models: 100%|██████████| 5/5 [00:00<00:00,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Model: CAT\n",
      "WARNING: 训练数据: (8000, 107), 验证数据: (2001, 107)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Fold 1 Train AUC: 0.99318, Val AUC: 0.90514\n",
      "WARNING: Fold 1 Early stopping step (best_iteration): 1596\n",
      "WARNING: Fold 1 finished in 76.45s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 2 Train AUC: 0.99744, Val AUC: 0.91229\n",
      "WARNING: Fold 2 Early stopping step (best_iteration): 2137\n",
      "WARNING: Fold 2 finished in 78.45s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 3 Train AUC: 0.99954, Val AUC: 0.90132\n",
      "WARNING: Fold 3 Early stopping step (best_iteration): 2937\n",
      "WARNING: Fold 3 finished in 80.06s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 4 Train AUC: 0.99938, Val AUC: 0.90149\n",
      "WARNING: Fold 4 Early stopping step (best_iteration): 2935\n",
      "WARNING: Fold 4 finished in 77.02s\n",
      "WARNING: 训练数据: (8001, 107), 验证数据: (2000, 107)\n",
      "WARNING: Fold 5 Train AUC: 0.99860, Val AUC: 0.90259\n",
      "WARNING: Fold 5 Early stopping step (best_iteration): 2491\n",
      "WARNING: Fold 5 finished in 75.62s\n",
      "WARNING: Overall OOF AUC: 0.90341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving models: 100%|██████████| 5/5 [00:00<00:00, 63.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Ensemble: Overall OOF AUC: 0.90629\n",
      "WARNING: 训练流程结束，总耗时: 775.96 秒。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32m20:27:53\u001b[0m executing - command=infer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Found a total of 30 model files.\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_5.pkl\n",
      "WARNING: Loaded 15 online models.\n",
      "WARNING: 变换模式: ['RAW', 'CUMSUM', 'DIFF']\n",
      "WARNING: 变换-特征函数匹配: {'RAW': {'2', '1', '7', '8', '10', '3'}, 'CUMSUM': {'2', '9', '1', '7', '5', '4', '8', '3'}, 'DIFF': {'2', '8', '7'}}\n",
      "WARNING: 变换-特征名称匹配: {'RAW': {'agg_linear_trend_attr_slope_chunk_len_10_f_agg_mean', 'change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4', 'ratio_beyond_r_sigma_0_5', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_4', 'ratio_beyond_r_sigma_1_5', 'ratio_beyond_r_sigma_1', 'agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0', 'quantile_0_1', 'index_mass_quantile_q_0_8', 'percentage_of_reoccurring_values_to_all_values', 'ks_stat', 'approx_entropy', 'agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_max', 'detrend_volatility_normalized', 'linear_trend_attr_pvalue', 'shapiro_pvalue', 'percentage_of_reoccurring_datapoints_to_all_datapoints', 'stats_std', 'ad_stat', 'change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_4', 'stats_min', 'friedrich_coefficients_coeff_3_m_3_r_30', 'stats_median', 'quantile_0_4', 'index_mass_quantile_q_0_1', 'rpt_cost_cosine', 'bartlett_pvalue', 'bartlett_stat', 'ratio_value_number_to_time_series_length', 'energy_ratio_by_chunks_num_segments_10_segment_focus_9', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2', 'sample_entropy', 'stats_kurt', 'stats_cv', 'katz_fd', 'change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_2', 'count_above_0', 'stats_mean', 'higuchi_fd', 'ratio_beyond_r_sigma_3', 'benford_correlation'}, 'CUMSUM': {'ks_pvalue', 'cond_entropy', 'ad_pvalue', 'change_quantiles_f_agg_mean_isabs_True_qh_1_0_ql_0_4', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0', 'ar_coefficient_coeff_2_k_10', 'hjorth_complexity', 'adf_stat', 'detrend_volatility_normalized', 'stats_theil_sen_slope', 'dominant_freq', 'first_location_of_maximum', 'trend_normalized_slope', 'friedrich_coefficients_coeff_3_m_3_r_30', 'wilcoxon_stat', 'stats_max', 'stats_range', 'autocorr_lag1', 'ar_residuals_s1_pred_mean', 'adf_pvalue', 'kpss_pvalue', 'katz_fd', 'benford_correlation'}, 'DIFF': {'change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2', 'change_quantiles_f_agg_var_isabs_True_qh_0_4_ql_0_2', 'sample_entropy', 'spectral_entropy', 'change_quantiles_f_agg_var_isabs_False_qh_1_0_ql_0_2', 'hjorth_complexity', 'levene_pvalue', 'bartlett_pvalue', 'jb_pvalue', 'ar_coefficient_coeff_2_k_10', 'benford_correlation'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 101it [01:01,  1.65it/s]\n",
      "\u001b[32m20:28:59\u001b[0m checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
      "\u001b[32m20:28:59\u001b[0m executing - command=infer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Found a total of 30 model files.\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_CAT_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_XGB_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\online_CAT_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\online_LGB_model_fold_5.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\online_XGB_model_fold_5.pkl\n",
      "WARNING: Loaded 15 online models.\n",
      "WARNING: 变换模式: ['RAW', 'CUMSUM', 'DIFF']\n",
      "WARNING: 变换-特征函数匹配: {'RAW': {'2', '1', '7', '8', '10', '3'}, 'CUMSUM': {'2', '9', '1', '7', '5', '4', '8', '3'}, 'DIFF': {'2', '8', '7'}}\n",
      "WARNING: 变换-特征名称匹配: {'RAW': {'agg_linear_trend_attr_slope_chunk_len_10_f_agg_mean', 'change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4', 'ratio_beyond_r_sigma_0_5', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_4', 'ratio_beyond_r_sigma_1_5', 'ratio_beyond_r_sigma_1', 'agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0', 'quantile_0_1', 'index_mass_quantile_q_0_8', 'percentage_of_reoccurring_values_to_all_values', 'ks_stat', 'approx_entropy', 'agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_max', 'detrend_volatility_normalized', 'linear_trend_attr_pvalue', 'shapiro_pvalue', 'percentage_of_reoccurring_datapoints_to_all_datapoints', 'stats_std', 'ad_stat', 'change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_4', 'stats_min', 'friedrich_coefficients_coeff_3_m_3_r_30', 'stats_median', 'quantile_0_4', 'index_mass_quantile_q_0_1', 'rpt_cost_cosine', 'bartlett_pvalue', 'bartlett_stat', 'ratio_value_number_to_time_series_length', 'energy_ratio_by_chunks_num_segments_10_segment_focus_9', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2', 'sample_entropy', 'stats_kurt', 'stats_cv', 'katz_fd', 'change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_2', 'count_above_0', 'stats_mean', 'higuchi_fd', 'ratio_beyond_r_sigma_3', 'benford_correlation'}, 'CUMSUM': {'ks_pvalue', 'cond_entropy', 'ad_pvalue', 'change_quantiles_f_agg_mean_isabs_True_qh_1_0_ql_0_4', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0', 'ar_coefficient_coeff_2_k_10', 'hjorth_complexity', 'adf_stat', 'detrend_volatility_normalized', 'stats_theil_sen_slope', 'dominant_freq', 'first_location_of_maximum', 'trend_normalized_slope', 'friedrich_coefficients_coeff_3_m_3_r_30', 'wilcoxon_stat', 'stats_max', 'stats_range', 'autocorr_lag1', 'ar_residuals_s1_pred_mean', 'adf_pvalue', 'kpss_pvalue', 'katz_fd', 'benford_correlation'}, 'DIFF': {'change_quantiles_f_agg_var_isabs_False_qh_0_6_ql_0_4', 'change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_2', 'change_quantiles_f_agg_var_isabs_True_qh_0_4_ql_0_2', 'sample_entropy', 'spectral_entropy', 'change_quantiles_f_agg_var_isabs_False_qh_1_0_ql_0_2', 'hjorth_complexity', 'levene_pvalue', 'bartlett_pvalue', 'jb_pvalue', 'ar_coefficient_coeff_2_k_10', 'benford_correlation'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 30it [00:18,  1.60it/s]\n",
      "\u001b[32m20:29:21\u001b[0m determinism check: passed\n",
      "\u001b[32m20:29:21\u001b[0m \u001b[33msave prediction - path=data\\prediction.parquet\u001b[0m\n",
      "\u001b[32m20:29:21\u001b[0m ended\n",
      "\u001b[32m20:29:21\u001b[0m \u001b[33mduration - time=00:15:19\u001b[0m\n",
      "\u001b[32m20:29:21\u001b[0m \u001b[33mmemory - before=\"474.94 MB\" after=\"512.97 MB\" consumed=\"38.02 MB\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "crunch.test(\n",
    "    # Uncomment to disable the train\n",
    "    force_first_train=True,\n",
    "\n",
    "    # Uncomment to disable the determinism check\n",
    "    # no_determinism_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caf19e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.117964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.036871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.069379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.127808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.425484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>0.165459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>0.017593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>0.212351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>0.017096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>0.055085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction\n",
       "id               \n",
       "10001    0.117964\n",
       "10002    0.036871\n",
       "10003    0.069379\n",
       "10004    0.127808\n",
       "10005    0.425484\n",
       "...           ...\n",
       "10097    0.165459\n",
       "10098    0.017593\n",
       "10099    0.212351\n",
       "10100    0.017096\n",
       "10101    0.055085\n",
       "\n",
       "[101 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0c087a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9305164319248826)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the targets\n",
    "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
    "\n",
    "# Call the scoring function\n",
    "sklearn.metrics.roc_auc_score(\n",
    "    target,\n",
    "    prediction,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
