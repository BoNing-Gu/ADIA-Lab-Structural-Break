{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce981d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\adia\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import antropy\n",
    "import sklearn\n",
    "from tsfresh.feature_extraction import feature_calculators as tsfresh_fe\n",
    "import ruptures as rpt\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import inspect\n",
    "import typing\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5044c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n",
      "\n",
      "cli version: 6.6.1\n",
      "available ram: 15.73 gb\n",
      "available cpu: 16 core\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "\n",
    "# Load the Crunch Toolings\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68ce5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", InterpolationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b135c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "def get_logger(name: str, log_dir: Path, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    获取一个配置好的 logger 实例，它会生成带时间戳的详细日志。\n",
    "    \"\"\"\n",
    "    # 确保日志目录存在\n",
    "    log_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # 1. 创建带时间戳的详细日志文件名\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    detail_log_file = log_dir / f'{name.lower()}_{timestamp}.log'\n",
    "\n",
    "    # 2. 为 logger 设置一个唯一的名称（基于时间戳），避免冲突\n",
    "    logger = logging.getLogger(f\"{name}-{timestamp}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # 防止将日志消息传播到根 logger\n",
    "    logger.propagate = False\n",
    "\n",
    "    # 如果已经有处理器，则不重复添加\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # 3. 创建详细日志的文件处理器\n",
    "    detail_handler = logging.FileHandler(detail_log_file, mode='a', encoding='utf-8')\n",
    "    detail_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    detail_handler.setFormatter(detail_formatter)\n",
    "    logger.addHandler(detail_handler)\n",
    "    \n",
    "    # 4. 创建控制台处理器\n",
    "    # 控制台 - INFO级别 (受verbose控制)\n",
    "    if verbose:\n",
    "        info_handler = logging.StreamHandler(sys.stdout)\n",
    "        info_handler.setLevel(logging.INFO)\n",
    "        info_handler.addFilter(lambda record: record.levelno == logging.INFO)\n",
    "        info_formatter = logging.Formatter('%(message)s')\n",
    "        info_handler.setFormatter(info_formatter)\n",
    "        logger.addHandler(info_handler)\n",
    "\n",
    "    # 控制台 - WARNING及以上 (始终输出)\n",
    "    warn_handler = logging.StreamHandler(sys.stdout)\n",
    "    warn_handler.setLevel(logging.WARNING)\n",
    "    warn_formatter = logging.Formatter('%(levelname)s: %(message)s')\n",
    "    warn_handler.setFormatter(warn_formatter)\n",
    "    logger.addHandler(warn_handler)\n",
    "\n",
    "    return logger, detail_log_file # 返回 logger 和日志文件路径 \n",
    "\n",
    "logger = None\n",
    "log_file_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a16c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "# --- Exclude Features ---\n",
    "EXPERIMENTAL_FEATURES = [\n",
    "] \n",
    "\n",
    "# --- Top Features ---\n",
    "TOP_FEATURES = [\n",
    "    'RAW_1_stats_cv_whole',\n",
    "    'RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left',\n",
    "    'RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "    'RAW_3_cumsum_linear_trend_pvalue_whole',\n",
    "    'RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left',\n",
    "    'RAW_7_sample_entropy_left',\n",
    "    'RAW_1_stats_std_whole',\n",
    "    'RAW_8_quantile_0_4_whole',\n",
    "    'RAW_8_index_mass_quantile_q_0_1_right',\n",
    "    'RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "]\n",
    "\n",
    "# --- Remain Features ---\n",
    "REMAIN_FEATURES = [\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_1_stats_std_whole',\n",
    "    'add_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_8_index_mass_quantile_q_0_1_right',\n",
    "    'sub_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_7_sample_entropy_left',\n",
    "    'sub_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_7_sample_entropy_left',\n",
    "    'div_RAW_7_sample_entropy_left_RAW_3_cumsum_linear_trend_pvalue_whole',\n",
    "    'RAW_2_bartlett_stat',\n",
    "    'RAW_2_bartlett_pvalue',\n",
    "    'div_RAW_1_stats_std_whole_RAW_1_stats_cv_whole',\n",
    "    'sqmul_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left',\n",
    "    'RAW_8_ratio_value_number_to_time_series_length_whole',\n",
    "    'RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0_whole',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_2_ad_stat',\n",
    "    'RAW_2_ks_stat',\n",
    "    'sqmul_RAW_1_stats_cv_whole_RAW_1_stats_std_whole',\n",
    "    'div_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "    'RAW_3_cumsum_max_ratio_to_whole_left',\n",
    "    'RAW_1_stats_median_ratio',\n",
    "    'RAW_2_adf_icbest_left',\n",
    "    'RAW_8_quantile_0_4_ratio_to_whole_left',\n",
    "    'RAW_8_fft_coefficient_attr_imag_coeff_1_ratio_to_whole_left',\n",
    "    'sqmul_RAW_1_stats_std_whole_RAW_1_stats_cv_whole',\n",
    "    'div_RAW_1_stats_cv_whole_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left',\n",
    "    'RAW_4_diff_var_contribution_left',\n",
    "    'RAW_1_stats_kurt_whole',\n",
    "    'div_RAW_8_quantile_0_4_whole_RAW_1_stats_std_whole',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_1_stats_min_whole',\n",
    "    'RAW_8_agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_max_ratio_to_whole_left',\n",
    "    'RAW_2_ad_stat',\n",
    "    'RAW_8_benford_correlation_whole',\n",
    "    'RAW_7_katz_fd_whole',\n",
    "    'RAW_2_ad_pvalue',\n",
    "    'sub_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_7_sample_entropy_left',\n",
    "    'add_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_1_stats_std_whole',\n",
    "    'RAW_7_perm_entropy_left',\n",
    "    'RAW_8_first_location_of_maximum_left',\n",
    "    'div_RAW_1_stats_std_whole_RAW_8_quantile_0_4_whole',\n",
    "    'RAW_2_adf_stat_left',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_8_agg_linear_trend_attr_slope_chunk_len_10_f_agg_mean_contribution_left',\n",
    "    'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "    'RAW_2_ks_pvalue',\n",
    "    'RAW_10_rpt_cost_cosine_whole',\n",
    "    'RAW_8_ratio_beyond_r_sigma_1_left',\n",
    "    'RAW_7_higuchi_fd_ratio_to_whole_right',\n",
    "    'RAW_8_index_mass_quantile_q_0_8_left',\n",
    "    'RAW_8_change_quantiles_f_agg_var_isabs_True_qh_1_0_ql_0_4_contribution_left',\n",
    "    'RAW_8_linear_trend_attr_rvalue_ratio',\n",
    "    'div_RAW_1_stats_cv_whole_RAW_8_quantile_0_4_whole',\n",
    "    'RAW_8_index_mass_quantile_q_0_6_ratio_to_whole_left',\n",
    "    'RAW_8_ratio_beyond_r_sigma_3_diff',\n",
    "    'RAW_5_dominant_freq_right',\n",
    "    'RAW_9_ar_param_5_diff',\n",
    "    'RAW_8_quantile_0_1_ratio_to_whole_right',\n",
    "    'RAW_8_ratio_beyond_r_sigma_0_5_diff',\n",
    "    'RAW_8_quantile_0_6_right',\n",
    "    'RAW_4_diff_var_contribution_right',\n",
    "    'RAW_8_agg_linear_trend_attr_intercept_chunk_len_10_f_agg_max_ratio_to_whole_right',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_8_ar_coefficient_coeff_2_k_10_left',\n",
    "    'RAW_9_ar_param_3_right',\n",
    "    'RAW_8_fft_coefficient_attr_imag_coeff_1_left',\n",
    "    'RAW_8_change_quantiles_f_agg_var_isabs_False_qh_1_0_ql_0_2_ratio_to_whole_right',\n",
    "    'RAW_8_benford_correlation_ratio_to_whole_left',\n",
    "    'RAW_7_svd_entropy_whole',\n",
    "    'RAW_8_ratio_beyond_r_sigma_1_ratio_to_whole_left',\n",
    "    'div_RAW_1_stats_std_whole_RAW_3_cumsum_linear_trend_pvalue_whole',\n",
    "    'RAW_3_cumsum_detrend_volatility_normalized_whole',\n",
    "    'RAW_4_autocorr_lag1_diff',\n",
    "    'div_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "    'RAW_8_ratio_beyond_r_sigma_1_5_diff',\n",
    "    'RAW_8_index_mass_quantile_q_0_6_right',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_8_fft_coefficient_attr_imag_coeff_1_ratio_to_whole_right',\n",
    "    'RAW_1_stats_median_right',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_3_cumsum_max_ratio_to_whole_left',\n",
    "    'RAW_9_ar_param_3_ratio_to_whole_left',\n",
    "    'RAW_8_ratio_beyond_r_sigma_3_left',\n",
    "    'RAW_8_agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_mean_diff',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_2_kpss_stat_whole',\n",
    "    'RAW_8_fft_coefficient_attr_imag_coeff_1_ratio_to_whole_right',\n",
    "    'RAW_2_ttest_pvalue',\n",
    "    'RAW_8_change_quantiles_f_agg_var_isabs_False_qh_0_8_ql_0_0_right',\n",
    "    'RAW_8_friedrich_coefficients_coeff_3_m_3_r_30_ratio',\n",
    "    'RAW_4_autocorr_lag1_ratio',\n",
    "    'RAW_1_stats_median_whole',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_1_stats_min_diff',\n",
    "    'RAW_8_sum_of_reoccurring_data_points_ratio_to_whole_left',\n",
    "    'RAW_1_stats_kurt_left',\n",
    "    'RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left',\n",
    "    'sqmul_RAW_1_stats_std_whole_RAW_7_sample_entropy_left',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_8_fft_coefficient_attr_imag_coeff_3_left',\n",
    "    'RAW_2_kpss_stat_whole',\n",
    "    'mul_RAW_7_sample_entropy_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "    'RAW_3_cumsum_detrend_volatility_normalized_left',\n",
    "    'RAW_8_quantile_0_4_right',\n",
    "    'RAW_7_approx_entropy_left',\n",
    "    'div_RAW_7_sample_entropy_left_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left',\n",
    "    'add_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_3_cumsum_linear_trend_pvalue_whole',\n",
    "    'RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_4_whole',\n",
    "    'div_RAW_8_quantile_0_4_whole_RAW_3_cumsum_linear_trend_pvalue_whole',\n",
    "    'sqmul_RAW_8_quantile_0_4_whole_RAW_7_sample_entropy_left',\n",
    "    'add_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_quantile_0_4_whole',\n",
    "    'RAW_3_cumsum_detrend_volatility_left',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_8_agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_mean_diff',\n",
    "    'RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_whole',\n",
    "    'mul_RAW_1_stats_cv_whole_RAW_8_agg_linear_trend_attr_rvalue_chunk_len_50_f_agg_max_ratio_to_whole_left',\n",
    "    'RAW_7_sample_entropy_ratio_to_whole_right',\n",
    "    'mul_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_8_index_mass_quantile_q_0_1_right',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c64a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "# --- 时序变换函数注册表 ---\n",
    "TRANSFORM_REGISTRY = {}\n",
    "\n",
    "def register_transform(_func=None, *, output_mode_names=[]):\n",
    "    \"\"\"一个用于注册时序变换函数的装饰器。\"\"\"\n",
    "    def decorator_register(func):\n",
    "        TRANSFORM_REGISTRY[func.__name__] = {\n",
    "            \"func\": func, \n",
    "            \"output_mode_names\": output_mode_names\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    if _func is None:\n",
    "        # Used as @register_transform(output_mode_names=...)\n",
    "        return decorator_register\n",
    "    else:\n",
    "        # Used as @register_transform\n",
    "        return decorator_register(_func)\n",
    "\n",
    "@register_transform(output_mode_names=['RAW'])\n",
    "def no_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    原始时序\n",
    "    \"\"\"\n",
    "    result_dfs = []\n",
    "    result_dfs.append(X_df)\n",
    "\n",
    "    return result_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e066617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "# --- 特征函数注册表 ---\n",
    "FEATURE_REGISTRY = {}\n",
    "\n",
    "def register_feature(_func=None, *, parallelizable=True, func_id=\"\"):\n",
    "    \"\"\"一个用于注册特征函数的装饰器，可以标记特征是否可并行化。\"\"\"\n",
    "    def decorator_register(func):\n",
    "        FEATURE_REGISTRY[func.__name__] = {\n",
    "            \"func\": func, \n",
    "            \"parallelizable\": parallelizable,\n",
    "            \"func_id\": func_id\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    if _func is None:\n",
    "        # Used as @register_feature(parallelizable=...)\n",
    "        return decorator_register\n",
    "    else:\n",
    "        # Used as @register_feature\n",
    "        return decorator_register(_func)\n",
    "\n",
    "def _add_diff_ratio_feats(feats: dict, name: str, left, right):\n",
    "    \"\"\"\n",
    "    一个辅助函数，用于向特征字典中添加差异和比例特征。\n",
    "\n",
    "    Args:\n",
    "        feats (dict): 要更新的特征字典。\n",
    "        name (str): 特征的基础名称 (例如, 'stats_mean')。\n",
    "        left (float): 左侧分段的特征值。\n",
    "        right (float): 右侧分段的特征值。\n",
    "    \"\"\"\n",
    "    # check nan/None \n",
    "    if np.isnan(left) or np.isnan(right) or left is None or right is None:\n",
    "        feats[f'{name}_diff'] = 0.0\n",
    "        feats[f'{name}_ratio'] = 0.0\n",
    "        return\n",
    "    # 做差\n",
    "    feats[f'{name}_diff'] = right - left\n",
    "    # 做比\n",
    "    feats[f'{name}_ratio'] = right / (left + 1e-6)\n",
    "\n",
    "\n",
    "def _add_contribution_ratio_feats(feats: dict, name: str, left, right, whole):\n",
    "    \"\"\"\n",
    "    一个辅助函数，用于向特征字典中添加贡献度和与整体的比例特征。\n",
    "\n",
    "    Args:\n",
    "        feats (dict): 要更新的特征字典。\n",
    "        name (str): 特征的基础名称 (例如, 'stats_mean')。\n",
    "        left (float): 左侧分段的特征值。\n",
    "        right (float): 右侧分段的特征值。\n",
    "        whole (float): 整个序列的特征值。\n",
    "    \"\"\"\n",
    "    # check nan/None \n",
    "    if np.isnan(left) or np.isnan(right) or np.isnan(whole) or left is None or right is None or whole is None :\n",
    "        feats[f'{name}_contribution_left'] = 0.0\n",
    "        feats[f'{name}_contribution_right'] = 0.0\n",
    "        feats[f'{name}_ratio_to_whole_left'] = 0.0\n",
    "        feats[f'{name}_ratio_to_whole_right'] = 0.0\n",
    "        return\n",
    "    # 特征贡献度\n",
    "    feats[f'{name}_contribution_left'] = left / (left + right + 1e-6)\n",
    "    feats[f'{name}_contribution_right'] = right / (left + right + 1e-6)\n",
    "    # 与整体特征的比例\n",
    "    feats[f'{name}_ratio_to_whole_left'] = left / (whole + 1e-6)\n",
    "    feats[f'{name}_ratio_to_whole_right'] = right / (whole + 1e-6)\n",
    "\n",
    "# --- 1. 分布统计特征 ---\n",
    "def safe_cv(s):\n",
    "    s = pd.Series(s)\n",
    "    m = s.mean()\n",
    "    std = s.std()\n",
    "    return std / m if abs(m) > 1e-6 else 0.0\n",
    "\n",
    "def rolling_std_mean(s, window=50):\n",
    "    s = pd.Series(s)\n",
    "    if len(s) < window:\n",
    "        return 0.0\n",
    "    return s.rolling(window=window).std().dropna().mean()\n",
    "\n",
    "def slope_theil_sen(s):\n",
    "    s = pd.Series(s)\n",
    "    if len(s) < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        slope, intercept, _, _ = scipy.stats.theilslopes(s.values, np.arange(len(s)))\n",
    "        return slope\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "class STATSFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # 所有可用的func类及其名称\n",
    "        self.func_classes = {\n",
    "            # 'mean': np.mean,\n",
    "            'median': np.median,\n",
    "            # 'max': np.max,\n",
    "            'min': np.min,\n",
    "            # 'range': lambda x: np.max(x) - np.min(x),\n",
    "            'std': np.std,\n",
    "            # 'skew': scipy.stats.skew,\n",
    "            'kurt': scipy.stats.kurtosis,\n",
    "            'cv': safe_cv,\n",
    "            # 'mean_of_rolling_std': rolling_std_mean,\n",
    "            # 'theil_sen_slope': slope_theil_sen\n",
    "        }\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        self.signal = np.asarray(signal)\n",
    "        self.n = len(signal)\n",
    "\n",
    "    def calculate(self, func, start, end):\n",
    "        result = func(self.signal[start:end])\n",
    "        if isinstance(result, float) or isinstance(result, int):\n",
    "            return result\n",
    "        else:\n",
    "            return result.item()\n",
    "\n",
    "    def extract(self, signal, boundary):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            signal: 1D numpy array，单变量时间序列\n",
    "            boundary: int，分割点\n",
    "        输出：\n",
    "            result: dict，格式为 {func_name: {'left': value, 'right': value}}\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        result = {}\n",
    "        for name, func in self.func_classes.items():\n",
    "            try:\n",
    "                left = self.calculate(func, 0, boundary)\n",
    "                right = self.calculate(func, boundary, n)\n",
    "                whole = self.calculate(func, 0, n)\n",
    "                # diff = right - left\n",
    "                # ratio = right / (left + 1e-6)\n",
    "            except Exception:\n",
    "                left = None\n",
    "                right = None\n",
    "                whole = None\n",
    "                # diff = None\n",
    "                # ratio = None\n",
    "            # Move to _add_diff_ratio_feats, 'diff': diff, 'ratio': ratio\n",
    "            result[name] = {'left': left, 'right': right, 'whole': whole}   \n",
    "        return result\n",
    "\n",
    "@register_feature(func_id=\"1\")\n",
    "def distribution_stats_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"统计量的分段值、Diff值、Ratio值\"\"\"\n",
    "    value = u['value'].values.astype(np.float32)\n",
    "    period = u['period'].values.astype(np.float32)\n",
    "    boundary = np.where(np.diff(period) != 0)[0].item()\n",
    "    feats = {}\n",
    "\n",
    "    extractor = STATSFeatureExtractor()\n",
    "    extractor.fit(value)\n",
    "    features = extractor.extract(value, boundary)\n",
    "\n",
    "    feats = {}\n",
    "    for k, v in features.items():\n",
    "        for seg, value in v.items():\n",
    "            feats[f'stats_{k}_{seg}'] = value\n",
    "        _add_diff_ratio_feats(feats, f'stats_{k}', v['left'], v['right'])\n",
    "        # _add_contribution_ratio_feats(feats, f'stats_{k}', v['left'], v['right'], v['whole'])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "    \n",
    "# --- 2. 假设检验统计量特征 ---\n",
    "@register_feature(func_id=\"2\")\n",
    "def test_stats_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    \"\"\"假设检验统计量\"\"\"\n",
    "    # KS检验\n",
    "    ks_stat, ks_pvalue = scipy.stats.ks_2samp(s1, s2)\n",
    "    feats['ks_stat'] = ks_stat\n",
    "    feats['ks_pvalue'] = -ks_pvalue\n",
    "\n",
    "    # T检验\n",
    "    ttest_stat, ttest_pvalue = scipy.stats.ttest_ind(s1, s2, equal_var=False)\n",
    "    feats['ttest_pvalue'] = -ttest_pvalue if not np.isnan(ttest_pvalue) else 1\n",
    "\n",
    "    # AD检验\n",
    "    ad_stat, _, ad_pvalue = scipy.stats.anderson_ksamp([s1.to_numpy(), s2.to_numpy()])\n",
    "    feats['ad_stat'] = ad_stat\n",
    "    feats['ad_pvalue'] = -ad_pvalue\n",
    "\n",
    "    # # Mann-Whitney U检验 (非参数，不假设分布)\n",
    "    # mw_stat, mw_pvalue = scipy.stats.mannwhitneyu(s1, s2, alternative='two-sided')\n",
    "    # feats['mannwhitney_stat'] = mw_stat if not np.isnan(mw_stat) else 0\n",
    "    # feats['mannwhitney_pvalue'] = -mw_pvalue if not np.isnan(mw_pvalue) else 1\n",
    "    \n",
    "    # # Wilcoxon秩和检验\n",
    "    # w_stat, w_pvalue = scipy.stats.ranksums(s1, s2)\n",
    "    # feats['wilcoxon_stat'] = w_stat if not np.isnan(w_stat) else 0\n",
    "    # feats['wilcoxon_pvalue'] = -w_pvalue if not np.isnan(w_pvalue) else 1\n",
    "\n",
    "    # # Levene检验\n",
    "    # levene_stat, levene_pvalue = scipy.stats.levene(s1, s2)\n",
    "    # feats['levene_stat'] = levene_stat if not np.isnan(levene_stat) else 0\n",
    "    # feats['levene_pvalue'] = -levene_pvalue if not np.isnan(levene_pvalue) else 1\n",
    "    \n",
    "    # Bartlett检验\n",
    "    bartlett_stat, bartlett_pvalue = scipy.stats.bartlett(s1, s2)\n",
    "    feats['bartlett_stat'] = bartlett_stat if not np.isnan(bartlett_stat) else 0\n",
    "    feats['bartlett_pvalue'] = -bartlett_pvalue if not np.isnan(bartlett_pvalue) else 1\n",
    "    \n",
    "    # \"\"\"分段假设检验的分段值、Diff值、Ratio值\"\"\"\n",
    "    # # Shapiro-Wilk检验\n",
    "    # sw1_stat, sw1_pvalue, sw2_stat, sw2_pvalue, sw_whole_stat, sw_whole_pvalue = (np.nan,)*6\n",
    "    # try:\n",
    "    #     sw1_stat, sw1_pvalue = scipy.stats.shapiro(s1)\n",
    "    #     sw2_stat, sw2_pvalue = scipy.stats.shapiro(s2)\n",
    "    #     sw_whole_stat, sw_whole_pvalue = scipy.stats.shapiro(s_whole)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # feats['shapiro_pvalue_left'] = sw1_pvalue\n",
    "    # feats['shapiro_pvalue_right'] = sw2_pvalue\n",
    "    # feats['shapiro_pvalue_whole'] = sw_whole_pvalue\n",
    "    # _add_diff_ratio_feats(feats, 'shapiro_pvalue', sw1_pvalue, sw2_pvalue)\n",
    "    # _add_contribution_ratio_feats(feats, 'shapiro_pvalue', sw1_pvalue, sw2_pvalue, sw_whole_pvalue)\n",
    "\n",
    "    # # Jarque-Bera检验差异\n",
    "    # jb1_stat, jb1_pvalue, jb2_stat, jb2_pvalue, jb_whole_stat, jb_whole_pvalue = (np.nan,)*6\n",
    "    # try:\n",
    "    #     jb1_stat, jb1_pvalue = scipy.stats.jarque_bera(s1)\n",
    "    #     jb2_stat, jb2_pvalue = scipy.stats.jarque_bera(s2)\n",
    "    #     jb_whole_stat, jb_whole_pvalue = scipy.stats.jarque_bera(s_whole)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # feats['jb_pvalue_left'] = jb1_pvalue\n",
    "    # feats['jb_pvalue_right'] = jb2_pvalue\n",
    "    # feats['jb_pvalue_whole'] = jb_whole_pvalue\n",
    "    # _add_diff_ratio_feats(feats, 'jb_pvalue', jb1_pvalue, jb2_pvalue)\n",
    "    # _add_contribution_ratio_feats(feats, 'jb_pvalue', jb1_pvalue, jb2_pvalue, jb_whole_pvalue)\n",
    "\n",
    "    # KPSS检验\n",
    "    def extract_kpss_features(s):\n",
    "        if len(s) <= 12:\n",
    "            return {'p': 0.1, 'stat': 0.0, 'lag': 0, 'crit_5pct': 0.0, 'reject_5pct': 0}\n",
    "        kpss = tsa.stattools.kpss(s, regression='c', nlags='auto')\n",
    "        stat, p, lag, crit = kpss\n",
    "        crit_5pct = crit['5%']\n",
    "        return {\n",
    "            'p': p,\n",
    "            'stat': stat,\n",
    "            'lag': lag,\n",
    "            'crit_5pct': crit_5pct,\n",
    "            'reject_5pct': int(stat > crit_5pct)  # KPSS原假设是\"平稳\"，所以 > 临界值 拒绝平稳\n",
    "        }\n",
    "    try:\n",
    "        k1 = extract_kpss_features(s1)\n",
    "        k2 = extract_kpss_features(s2)\n",
    "        k_whole = extract_kpss_features(s_whole)\n",
    "\n",
    "        # feats['kpss_pvalue_left'] = k1['p']\n",
    "        # feats['kpss_pvalue_right'] = k2['p']\n",
    "        # feats['kpss_pvalue_whole'] = k_whole['p']\n",
    "        # _add_diff_ratio_feats(feats, 'kpss_pvalue', k1['p'], k2['p'])\n",
    "        # _add_contribution_ratio_feats(feats, 'kpss_pvalue', k1['p'], k2['p'], k_whole['p'])\n",
    "\n",
    "        feats['kpss_stat_left'] = k1['stat']\n",
    "        feats['kpss_stat_right'] = k2['stat']\n",
    "        feats['kpss_stat_whole'] = k_whole['stat']\n",
    "        # _add_diff_ratio_feats(feats, 'kpss_stat', k1['stat'], k2['stat'])\n",
    "        # _add_contribution_ratio_feats(feats, 'kpss_stat', k1['stat'], k2['stat'], k_whole['stat'])\n",
    "    except:\n",
    "        feats.update({\n",
    "            'kpss_pvalue_left': 1, 'kpss_pvalue_right': 1, 'kpss_pvalue_whole': 1, 'kpss_pvalue_diff': 0, 'kpss_pvalue_ratio': 0,\n",
    "            'kpss_stat_left': 0, 'kpss_stat_right': 0, 'kpss_stat_whole': 0, 'kpss_stat_diff': 0, 'kpss_stat_ratio': 0\n",
    "        })\n",
    "\n",
    "    # 平稳性检验 (ADF)\n",
    "    def extract_adf_features(s):\n",
    "        if len(s) <= 12:\n",
    "            return {'p': 1.0, 'stat': 0.0, 'lag': 0, 'ic': 0.0, 'crit_5pct': 0.0, 'reject_5pct': 0}\n",
    "        adf = tsa.stattools.adfuller(s, autolag='AIC')\n",
    "        stat, p, lag, _, crit, ic = adf\n",
    "        crit_5pct = crit['5%']\n",
    "        return {\n",
    "            'p': p,\n",
    "            'stat': stat,\n",
    "            'lag': lag,\n",
    "            'ic': ic,\n",
    "            'crit_5pct': crit_5pct,\n",
    "            'reject_5pct': int(stat < crit_5pct)\n",
    "        }\n",
    "    try:\n",
    "        f1 = extract_adf_features(s1)\n",
    "        f2 = extract_adf_features(s2)\n",
    "        f_whole = extract_adf_features(s_whole)\n",
    "\n",
    "        # feats['adf_pvalue_left'] = f1['p']\n",
    "        # feats['adf_pvalue_right'] = f2['p']\n",
    "        # feats['adf_pvalue_whole'] = f_whole['p']\n",
    "        # _add_diff_ratio_feats(feats, 'adf_pvalue', f1['p'], f2['p'])\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_pvalue', f1['p'], f2['p'], f_whole['p'])\n",
    "\n",
    "        feats['adf_stat_left'] = f1['stat']\n",
    "        feats['adf_stat_right'] = f2['stat']\n",
    "        feats['adf_stat_whole'] = f_whole['stat']\n",
    "        # _add_diff_ratio_feats(feats, 'adf_stat', f1['stat'], f2['stat'])\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_stat', f1['stat'], f2['stat'], f_whole['stat'])\n",
    "\n",
    "        feats['adf_icbest_left'] = f1['ic']\n",
    "        feats['adf_icbest_right'] = f2['ic']\n",
    "        feats['adf_icbest_whole'] = f_whole['ic']\n",
    "        # _add_diff_ratio_feats(feats, 'adf_icbest', f1['ic'], f2['ic'])\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_icbest', f1['ic'], f2['ic'], f_whole['ic'])\n",
    "    except:\n",
    "        feats.update({\n",
    "            'adf_pvalue_left': 1, 'adf_pvalue_right': 1, 'adf_pvalue_whole': 1, 'adf_pvalue_diff': 0, 'adf_pvalue_ratio': 0,\n",
    "            'adf_stat_left': 0, 'adf_stat_right': 0, 'adf_stat_whole': 0, 'adf_stat_diff': 0, 'adf_stat_ratio': 0,\n",
    "            'adf_icbest_left': 0, 'adf_icbest_right': 0, 'adf_icbest_whole': 0, 'adf_icbest_diff': 0, 'adf_icbest_ratio': 0\n",
    "        })\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 3. 累积和特征 ---\n",
    "@register_feature(func_id=\"3\")\n",
    "def cumulative_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    def analyze_cumsum_curve(series, seg):\n",
    "        \"\"\"分析累积和曲线的各种特征\"\"\"\n",
    "        if len(series) < 3:\n",
    "            return {}\n",
    "        \n",
    "        cumsum_curve = series.cumsum()\n",
    "        curve_feats = {}\n",
    "        \n",
    "        # 线性趋势\n",
    "        x = np.arange(len(cumsum_curve))\n",
    "        slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, cumsum_curve)\n",
    "        # curve_feats[f'cumsum_linear_trend_slope_{seg}'] = slope\n",
    "        # curve_feats[f'cumsum_linear_trend_r2_{seg}'] = r_value ** 2\n",
    "        curve_feats[f'cumsum_linear_trend_pvalue_{seg}'] = p_value\n",
    "\n",
    "        # # 波动率\n",
    "        # curve_feats[f'cumsum_std_{seg}'] = np.std(cumsum_curve)\n",
    "        # curve_feats[f'cumsum_cv_{seg}'] = safe_cv(cumsum_curve)\n",
    "    \n",
    "        # 趋势背离\n",
    "        linear_trend = slope * x + intercept\n",
    "        detrended = cumsum_curve - linear_trend\n",
    "        curve_feats[f'cumsum_detrend_volatility_{seg}'] = np.std(detrended)\n",
    "        curve_feats[f'cumsum_detrend_volatility_normalized_{seg}'] = np.std(detrended) / (np.abs(np.mean(cumsum_curve)) + 1e-6)\n",
    "        curve_feats[f'cumsum_detrend_max_deviation_{seg}'] = np.max(np.abs(detrended))\n",
    "        \n",
    "        # 极值特征\n",
    "        # curve_feats[f'cumsum_min_{seg}'] = np.min(cumsum_curve)\n",
    "        curve_feats[f'cumsum_max_{seg}'] = np.max(cumsum_curve)\n",
    "        \n",
    "        return curve_feats\n",
    "    \n",
    "    feats.update(analyze_cumsum_curve(s1, 'left'))\n",
    "    feats.update(analyze_cumsum_curve(s2, 'right'))\n",
    "    feats.update(analyze_cumsum_curve(s_whole, 'whole'))\n",
    "    \n",
    "    # _add_diff_ratio_feats(feats, 'cumsum_linear_trend_slope', feats.get('cumsum_linear_trend_slope_left', 0), feats.get('cumsum_linear_trend_slope_right', 0))\n",
    "    # _add_diff_ratio_feats(feats, 'cumsum_std', feats.get('cumsum_std_left', 0), feats.get('cumsum_std_right', 0))\n",
    "    # _add_diff_ratio_feats(feats, 'cumsum_cv', feats.get('cumsum_cv_left', 0), feats.get('cumsum_cv_right', 0))\n",
    "    # _add_contribution_ratio_feats(feats, 'cumsum_min', feats.get('cumsum_min_left', 0), feats.get('cumsum_min_right', 0), feats.get('cumsum_min_whole', 0))\n",
    "    _add_contribution_ratio_feats(feats, 'cumsum_max', feats.get('cumsum_max_left', 0), feats.get('cumsum_max_right', 0), feats.get('cumsum_max_whole', 0))\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 4. 振荡特征 ---\n",
    "@register_feature(func_id=\"4\")\n",
    "def oscillation_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0].reset_index(drop=True)\n",
    "    s2 = u['value'][u['period'] == 1].reset_index(drop=True)\n",
    "    s_whole = u['value'].reset_index(drop=True)\n",
    "    feats = {}\n",
    "\n",
    "    def count_zero_crossings(series: pd.Series):\n",
    "        if len(series) < 2: return 0\n",
    "        centered_series = series - series.mean()\n",
    "        if centered_series.eq(0).all(): return 0\n",
    "        return np.sum(np.diff(np.sign(centered_series)) != 0)\n",
    "\n",
    "    # zc1, zc2, zc_whole = count_zero_crossings(s1), count_zero_crossings(s2), count_zero_crossings(s_whole)\n",
    "    # feats['zero_cross_left'] = zc1\n",
    "    # feats['zero_cross_right'] = zc2\n",
    "    # feats['zero_cross_whole'] = zc_whole\n",
    "    # _add_diff_ratio_feats(feats, 'zero_cross', zc1, zc2)\n",
    "    # _add_contribution_ratio_feats(feats, 'zero_cross', zc1, zc2, zc_whole)\n",
    "    \n",
    "    def autocorr_lag1(s):\n",
    "        if len(s) < 2: return 0.0\n",
    "        ac = s.autocorr(lag=1)\n",
    "        return ac if not np.isnan(ac) else 0.0\n",
    "        \n",
    "    ac1, ac2, ac_whole = autocorr_lag1(s1), autocorr_lag1(s2), autocorr_lag1(s_whole)\n",
    "    feats['autocorr_lag1_left'] = ac1\n",
    "    feats['autocorr_lag1_right'] = ac2\n",
    "    feats['autocorr_lag1_whole'] = ac_whole\n",
    "    _add_diff_ratio_feats(feats, 'autocorr_lag1', ac1, ac2)\n",
    "    # _add_contribution_ratio_feats(feats, 'autocorr_lag1', ac1, ac2, ac_whole)\n",
    "\n",
    "    var1, var2, var_whole = s1.diff().var(), s2.diff().var(), s_whole.diff().var()\n",
    "    feats['diff_var_left'] = var1\n",
    "    feats['diff_var_right'] = var2\n",
    "    feats['diff_var_whole'] = var_whole\n",
    "    # _add_diff_ratio_feats(feats, 'diff_var', var1, var2)\n",
    "    _add_contribution_ratio_feats(feats, 'diff_var', var1, var2, var_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 5. 频域特征 ---\n",
    "@register_feature(func_id=\"5\")\n",
    "def cyclic_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    def get_fft_props(series):\n",
    "        if len(series) < 2: return 0.0, 0.0\n",
    "        \n",
    "        N = len(series)\n",
    "        yf = np.fft.fft(series.values)\n",
    "        power = np.abs(yf[1:N//2])**2\n",
    "        xf = np.fft.fftfreq(N, 1)[1:N//2]\n",
    "        \n",
    "        if len(power) == 0: return 0.0, 0.0\n",
    "            \n",
    "        dominant_freq = xf[np.argmax(power)]\n",
    "        max_power = np.max(power)\n",
    "        return dominant_freq, max_power\n",
    "\n",
    "    # freq1, power1 = get_fft_props(s1)\n",
    "    freq2, power2 = get_fft_props(s2)\n",
    "    # freq_whole, power_whole = get_fft_props(s_whole)\n",
    "    \n",
    "    # feats['dominant_freq_left'] = freq1\n",
    "    feats['dominant_freq_right'] = freq2\n",
    "    # feats['dominant_freq_whole'] = freq_whole\n",
    "    # _add_diff_ratio_feats(feats, 'dominant_freq', freq1, freq2)\n",
    "    # _add_contribution_ratio_feats(feats, 'dominant_freq', freq1, freq2, freq_whole)\n",
    "\n",
    "    # feats['max_power_left'] = power1\n",
    "    # feats['max_power_right'] = power2\n",
    "    # feats['max_power_whole'] = power_whole\n",
    "    # _add_diff_ratio_feats(feats, 'max_power', power1, power2)\n",
    "    # _add_contribution_ratio_feats(feats, 'max_power', power1, power2, power_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 6. 振幅特征 ---\n",
    "@register_feature(func_id=\"6\")\n",
    "def amplitude_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "    \n",
    "    # ptp1, ptp2, ptp_whole = np.ptp(s1), np.ptp(s2), np.ptp(s_whole)\n",
    "    # iqr1, iqr2, iqr_whole = scipy.stats.iqr(s1), scipy.stats.iqr(s2), scipy.stats.iqr(s_whole)\n",
    "\n",
    "    # feats['ptp_left'] = ptp1\n",
    "    # feats['ptp_right'] = ptp2\n",
    "    # feats['ptp_whole'] = ptp_whole\n",
    "    # _add_diff_ratio_feats(feats, 'ptp', ptp1, ptp2)\n",
    "    # _add_contribution_ratio_feats(feats, 'ptp', ptp1, ptp2, ptp_whole)\n",
    "\n",
    "    # feats['iqr_left'] = iqr1\n",
    "    # feats['iqr_right'] = iqr2\n",
    "    # feats['iqr_whole'] = iqr_whole\n",
    "    # _add_diff_ratio_feats(feats, 'iqr', iqr1, iqr2)\n",
    "    # _add_contribution_ratio_feats(feats, 'iqr', iqr1, iqr2, iqr_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 7. 熵信息 ---\n",
    "@register_feature(func_id=\"7\")\n",
    "def entropy_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    def compute_entropy(x):\n",
    "        hist, _ = np.histogram(x, bins='auto', density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        return scipy.stats.entropy(hist)\n",
    "    \n",
    "    entropy_funcs = {\n",
    "        # 'shannon_entropy': compute_entropy,\n",
    "        'perm_entropy': lambda x: antropy.perm_entropy(x, normalize=True),\n",
    "        # 'spectral_entropy': lambda x: antropy.spectral_entropy(x, sf=1.0, normalize=True),\n",
    "        'svd_entropy': lambda x: antropy.svd_entropy(x, normalize=True),\n",
    "        'approx_entropy': antropy.app_entropy,\n",
    "        'sample_entropy': antropy.sample_entropy,\n",
    "        # 'petrosian_fd': antropy.petrosian_fd,\n",
    "        'katz_fd': antropy.katz_fd,\n",
    "        'higuchi_fd': antropy.higuchi_fd,\n",
    "        # 'detrended_fluctuation': antropy.detrended_fluctuation,\n",
    "    }\n",
    "\n",
    "    for name, func in entropy_funcs.items():\n",
    "        try:\n",
    "            v1, v2, v_whole = func(s1), func(s2), func(s_whole)\n",
    "            feats[f'{name}_left'] = v1\n",
    "            feats[f'{name}_right'] = v2\n",
    "            feats[f'{name}_whole'] = v_whole\n",
    "            _add_diff_ratio_feats(feats, name, v1, v2)\n",
    "            _add_contribution_ratio_feats(feats, name, v1, v2, v_whole)\n",
    "        except Exception:\n",
    "            feats.update({f'{name}_left': 0, f'{name}_right': 0, f'{name}_whole': 0, f'{name}_diff': 0, f'{name}_ratio': 0})\n",
    "\n",
    "    # try:\n",
    "    #     m1, c1 = antropy.hjorth_params(s1)\n",
    "    #     m2, c2 = antropy.hjorth_params(s2)\n",
    "    #     m_whole, c_whole = antropy.hjorth_params(s_whole)\n",
    "    #     feats.update({\n",
    "    #         'hjorth_mobility_left': m1, 'hjorth_mobility_right': m2, 'hjorth_mobility_whole': m_whole,\n",
    "    #         'hjorth_complexity_left': c1, 'hjorth_complexity_right': c2, 'hjorth_complexity_whole': c_whole,\n",
    "    #     })\n",
    "    #     _add_diff_ratio_feats(feats, 'hjorth_mobility', m1, m2)\n",
    "    #     _add_contribution_ratio_feats(feats, 'hjorth_mobility', m1, m2, m_whole)\n",
    "    #     _add_diff_ratio_feats(feats, 'hjorth_complexity', c1, c2)\n",
    "    #     _add_contribution_ratio_feats(feats, 'hjorth_complexity', c1, c2, c_whole)\n",
    "    # except Exception:\n",
    "    #     feats.update({'hjorth_mobility_left':0, 'hjorth_mobility_right':0, 'hjorth_mobility_whole':0, 'hjorth_mobility_diff':0, 'hjorth_mobility_ratio':0,\n",
    "    #                  'hjorth_complexity_left':0, 'hjorth_complexity_right':0, 'hjorth_complexity_whole':0, 'hjorth_complexity_diff':0, 'hjorth_complexity_ratio':0})\n",
    "\n",
    "\n",
    "    def series_to_binary_str(x, method='median'):\n",
    "        if method == 'median':\n",
    "            threshold = np.median(x)\n",
    "            return ''.join(['1' if val > threshold else '0' for val in x])\n",
    "        return None\n",
    "    \n",
    "    # try:\n",
    "    #     bin_str1 = series_to_binary_str(s1)\n",
    "    #     bin_str2 = series_to_binary_str(s2)\n",
    "    #     bin_str_whole = series_to_binary_str(s_whole)\n",
    "    #     lz1, lz2, lz_whole = antropy.lziv_complexity(bin_str1, normalize=True), antropy.lziv_complexity(bin_str2, normalize=True), antropy.lziv_complexity(bin_str_whole, normalize=True)\n",
    "    #     feats.update({\n",
    "    #         'lziv_complexity_left': lz1, 'lziv_complexity_right': lz2, 'lziv_complexity_whole': lz_whole,\n",
    "    #     })\n",
    "    #     _add_diff_ratio_feats(feats, 'lziv_complexity', lz1, lz2)\n",
    "    #     _add_contribution_ratio_feats(feats, 'lziv_complexity', lz1, lz2, lz_whole)\n",
    "    # except Exception:\n",
    "    #     feats.update({'lziv_complexity_left':0, 'lziv_complexity_right':0, 'lziv_complexity_whole':0, 'lziv_complexity_diff':0, 'lziv_complexity_ratio':0})\n",
    "\n",
    "\n",
    "    def estimate_cond_entropy(x, lag=1):\n",
    "        x = x - np.mean(x)\n",
    "        x_lag = x[:-lag]\n",
    "        x_now = x[lag:]\n",
    "        bins = 10\n",
    "        joint_hist, _, _ = np.histogram2d(x_lag, x_now, bins=bins, density=True)\n",
    "        joint_hist = joint_hist[joint_hist > 0]\n",
    "        H_xy = -np.sum(joint_hist * np.log(joint_hist))\n",
    "        H_x = -np.sum(np.histogram(x_lag, bins=bins, density=True)[0] * \\\n",
    "                      np.log(np.histogram(x_lag, bins=bins, density=True)[0] + 1e-12))\n",
    "        return H_xy - H_x\n",
    "    # try:\n",
    "    #     ce1, ce2, ce_whole = estimate_cond_entropy(s1), estimate_cond_entropy(s2), estimate_cond_entropy(s_whole)\n",
    "    #     feats.update({\n",
    "    #         'cond_entropy_left': ce1, 'cond_entropy_right': ce2, 'cond_entropy_whole': ce_whole,\n",
    "    #     })\n",
    "    #     _add_diff_ratio_feats(feats, 'cond_entropy', ce1, ce2)\n",
    "    #     _add_contribution_ratio_feats(feats, 'cond_entropy', ce1, ce2, ce_whole)\n",
    "    # except Exception:\n",
    "    #     feats.update({'cond_entropy_left':0, 'cond_entropy_right':0, 'cond_entropy_whole':0, 'cond_entropy_diff':0, 'cond_entropy_ratio':0})\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 8. tsfresh --- \n",
    "@register_feature(func_id=\"8\")\n",
    "def tsfresh_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"基于tsfresh的特征工程\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    funcs = {\n",
    "        tsfresh_fe.ratio_value_number_to_time_series_length: None,\n",
    "        tsfresh_fe.sum_of_reoccurring_data_points: None,\n",
    "        tsfresh_fe.percentage_of_reoccurring_values_to_all_values: None,\n",
    "        tsfresh_fe.percentage_of_reoccurring_datapoints_to_all_datapoints: None,\n",
    "        tsfresh_fe.last_location_of_maximum: None,\n",
    "        tsfresh_fe.first_location_of_maximum: None,\n",
    "        tsfresh_fe.has_duplicate: None,\n",
    "        tsfresh_fe.benford_correlation: None,\n",
    "        tsfresh_fe.ratio_beyond_r_sigma: [6, 3, 1.5, 1, 0.5],\n",
    "        tsfresh_fe.quantile: [0.6, 0.4, 0.1],\n",
    "        tsfresh_fe.count_above: [0],\n",
    "        tsfresh_fe.number_peaks: [25, 50],\n",
    "        tsfresh_fe.partial_autocorrelation: [{\"lag\": 2}, {\"lag\": 6}],\n",
    "        tsfresh_fe.index_mass_quantile: [{\"q\": 0.1}, {\"q\": 0.6}, {\"q\": 0.7}, {\"q\": 0.8}],\n",
    "        tsfresh_fe.ar_coefficient: [{\"coeff\": 0, \"k\": 10}, {\"coeff\": 2, \"k\": 10}, {\"coeff\": 8, \"k\": 10}],\n",
    "        tsfresh_fe.linear_trend: [{\"attr\": \"slope\"}, {\"attr\": \"rvalue\"}, {\"attr\": \"pvalue\"}, {\"attr\": \"intercept\"}],\n",
    "        tsfresh_fe.fft_coefficient: [{\"coeff\": 3, \"attr\": \"imag\"}, {\"coeff\": 2, \"attr\": \"imag\"}, {\"coeff\": 1, \"attr\": \"imag\"}],\n",
    "        tsfresh_fe.energy_ratio_by_chunks: [{\"num_segments\": 10, \"segment_focus\": 9}],\n",
    "        tsfresh_fe.friedrich_coefficients: [{\"m\": 3, \"r\": 30, \"coeff\": 2}, {\"m\": 3, \"r\": 30, \"coeff\": 3}],\n",
    "        tsfresh_fe.change_quantiles: [\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 1.0, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 1.0, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.6},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.6, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.6, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.4, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 1.0, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 1.0, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.0},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.6, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.6, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.4, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"mean\",\"isabs\": True,  \"qh\": 1.0, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"mean\",\"isabs\": True,  \"qh\": 0.6, \"ql\": 0.4},\n",
    "        ],\n",
    "        tsfresh_fe.agg_linear_trend: [\n",
    "            {\"attr\": \"slope\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"slope\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"slope\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 50, \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 5,  \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 10, \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 50, \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 5,  \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            {\"attr\": \"intercept\", \"chunk_len\": 10, \"f_agg\": \"max\"},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def param_to_str(param):\n",
    "        if isinstance(param, dict):\n",
    "            return '_'.join([f\"{k}_{v}\" for k, v in param.items()])\n",
    "        else:\n",
    "            return str(param)\n",
    "\n",
    "    def calculate_stats_for_feature(func, param=None):\n",
    "        results = {}\n",
    "        base_name = func.__name__\n",
    "        if param is not None:\n",
    "            base_name += f\"_{param_to_str(param)}\"\n",
    "\n",
    "        try:\n",
    "            # Prepare arguments for each segment\n",
    "            args_s1 = [s1]\n",
    "            args_s2 = [s2]\n",
    "            args_s_whole = [s_whole]\n",
    "            is_combiner = False\n",
    "\n",
    "            if param is None: # Simple function, no params\n",
    "                pass\n",
    "            elif isinstance(param, dict):\n",
    "                # Check if it's a combiner function or a function with kwargs\n",
    "                sig = inspect.signature(func)\n",
    "                if 'param' in sig.parameters: # Combiner function\n",
    "                    is_combiner = True\n",
    "                    args_s1.append([param])\n",
    "                    args_s2.append([param])\n",
    "                    args_s_whole.append([param])\n",
    "                else: # Function with kwargs\n",
    "                    args_s1.append(param)\n",
    "                    args_s2.append(param)\n",
    "                    args_s_whole.append(param)\n",
    "            else: # Simple function with a single parameter\n",
    "                args_s1.append(param)\n",
    "                args_s2.append(param)\n",
    "                args_s_whole.append(param)\n",
    "\n",
    "            # Execute function for each segment\n",
    "            if is_combiner:\n",
    "                v1_dict = {k: v for k, v in func(*args_s1)}\n",
    "                v2_dict = {k: v for k, v in func(*args_s2)}\n",
    "                v_whole_dict = {k: v for k, v in func(*args_s_whole)}\n",
    "                \n",
    "                for key in v1_dict:\n",
    "                    v1, v2, v_whole = v1_dict[key], v2_dict[key], v_whole_dict[key]\n",
    "                    feat_name_base = f\"{func.__name__}_{key}\"\n",
    "                    results[f'{feat_name_base}_left'] = v1\n",
    "                    results[f'{feat_name_base}_right'] = v2\n",
    "                    results[f'{feat_name_base}_whole'] = v_whole\n",
    "                    _add_diff_ratio_feats(feats, feat_name_base, v1, v2)\n",
    "                    _add_contribution_ratio_feats(results, feat_name_base, v1, v2, v_whole)\n",
    "                return results\n",
    "\n",
    "            else:\n",
    "                if isinstance(param, dict) and not is_combiner:\n",
    "                    v1, v2, v_whole = func(args_s1[0], **args_s1[1]), func(args_s2[0], **args_s2[1]), func(args_s_whole[0], **args_s_whole[1])\n",
    "                else:\n",
    "                    v1, v2, v_whole = func(*args_s1), func(*args_s2), func(*args_s_whole)\n",
    "\n",
    "                results[f'{base_name}_left'] = v1\n",
    "                results[f'{base_name}_right'] = v2\n",
    "                results[f'{base_name}_whole'] = v_whole\n",
    "                _add_diff_ratio_feats(feats, base_name, v1, v2)\n",
    "                _add_contribution_ratio_feats(results, base_name, v1, v2, v_whole)\n",
    "        \n",
    "        except Exception:\n",
    "            # For combiner functions, need to know keys to create nulls\n",
    "            if 'param' in locals() and inspect.isfunction(func) and 'param' in inspect.signature(func).parameters:\n",
    "                 # It's a combiner, but we can't get keys without running it. Skip for now on error.\n",
    "                 pass\n",
    "            else:\n",
    "                results[f'{base_name}_left'] = np.nan\n",
    "                results[f'{base_name}_right'] = np.nan\n",
    "                results[f'{base_name}_whole'] = np.nan\n",
    "                results[f'{base_name}_diff'] = np.nan\n",
    "                results[f'{base_name}_ratio'] = np.nan\n",
    "                \n",
    "        return results\n",
    "\n",
    "\n",
    "    for func, params in funcs.items():\n",
    "        if params is None:\n",
    "            feats.update(calculate_stats_for_feature(func))\n",
    "        else:\n",
    "            for param in params:\n",
    "                feats.update(calculate_stats_for_feature(func, param))\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 9. 时间序列建模 ---\n",
    "@register_feature(func_id=\"9\")\n",
    "def ar_model_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    基于AR模型派生特征。\n",
    "    1. 在 period 0 上训练模型，预测 period 1，计算残差统计量。\n",
    "    2. 在 period 1 上训练模型，预测 period 0，计算残差统计量。\n",
    "    3. 分别在 period 0 和 1 上训练模型，比较模型参数、残差和信息准则(AIC/BIC)。\n",
    "    \"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "    lags = 5 # 固定阶数以保证可比性\n",
    "\n",
    "    # # --- 特征组1: 用 s1 训练，预测 s2 ---\n",
    "    # if len(s1) > lags and len(s2) > 0:\n",
    "    #     try:\n",
    "    #         model1_fit = AutoReg(s1, lags=lags).fit()\n",
    "    #         predictions = model1_fit.predict(start=len(s1), end=len(s1) + len(s2) - 1, dynamic=True)\n",
    "    #         residuals = s2 - predictions\n",
    "    #         feats['ar_residuals_s2_pred_mean'] = np.mean(residuals)\n",
    "    #         feats['ar_residuals_s2_pred_std'] = np.std(residuals)\n",
    "    #         feats['ar_residuals_s2_pred_skew'] = pd.Series(residuals).skew()\n",
    "    #         feats['ar_residuals_s2_pred_kurt'] = pd.Series(residuals).kurt()\n",
    "    #     except Exception:\n",
    "    #         # 宽泛地捕获异常，防止因数值问题中断\n",
    "    #         feats.update({'ar_residuals_s2_pred_mean': 0, 'ar_residuals_s2_pred_std': 0, 'ar_residuals_s2_pred_skew': 0, 'ar_residuals_s2_pred_kurt': 0})\n",
    "    # else:\n",
    "    #     feats.update({'ar_residuals_s2_pred_mean': 0, 'ar_residuals_s2_pred_std': 0, 'ar_residuals_s2_pred_skew': 0, 'ar_residuals_s2_pred_kurt': 0})\n",
    "\n",
    "    # # --- 特征组2: 用 s2 训练，预测 s1 ---\n",
    "    # if len(s2) > lags and len(s1) > 0:\n",
    "    #     try:\n",
    "    #         model2_fit = AutoReg(s2, lags=lags).fit()\n",
    "    #         predictions_on_s1 = model2_fit.predict(start=len(s2), end=len(s2) + len(s1) - 1, dynamic=True)\n",
    "    #         residuals_s1_pred = s1 - predictions_on_s1\n",
    "    #         feats['ar_residuals_s1_pred_mean'] = np.mean(residuals_s1_pred)\n",
    "    #         feats['ar_residuals_s1_pred_std'] = np.std(residuals_s1_pred)\n",
    "    #         feats['ar_residuals_s1_pred_skew'] = pd.Series(residuals_s1_pred).skew()\n",
    "    #         feats['ar_residuals_s1_pred_kurt'] = pd.Series(residuals_s1_pred).kurt()\n",
    "    #     except Exception:\n",
    "    #         feats.update({'ar_residuals_s1_pred_mean': 0, 'ar_residuals_s1_pred_std': 0, 'ar_residuals_s1_pred_skew': 0, 'ar_residuals_s1_pred_kurt': 0})\n",
    "    # else:\n",
    "    #     feats.update({'ar_residuals_s1_pred_mean': 0, 'ar_residuals_s1_pred_std': 0, 'ar_residuals_s1_pred_skew': 0, 'ar_residuals_s1_pred_kurt': 0})\n",
    "\n",
    "\n",
    "    # --- 特征组3: 分别建模，比较差异 ---\n",
    "    s1_resid_std, s1_params = np.nan, np.full(lags + 1, np.nan)\n",
    "    s1_aic, s1_bic = np.nan, np.nan\n",
    "    if len(s1) > lags:\n",
    "        try:\n",
    "            fit1 = AutoReg(s1, lags=lags).fit()\n",
    "            s1_resid_std = np.std(fit1.resid)\n",
    "            s1_params = fit1.params\n",
    "            s1_aic = fit1.aic\n",
    "            s1_bic = fit1.bic\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    s2_resid_std, s2_params = np.nan, np.full(lags + 1, np.nan)\n",
    "    s2_aic, s2_bic = np.nan, np.nan\n",
    "    if len(s2) > lags:\n",
    "        try:\n",
    "            fit2 = AutoReg(s2, lags=lags).fit()\n",
    "            s2_resid_std = np.std(fit2.resid)\n",
    "            s2_params = fit2.params\n",
    "            s2_aic = fit2.aic\n",
    "            s2_bic = fit2.bic\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    swhole_resid_std, swhole_params = np.nan, np.full(lags + 1, np.nan)\n",
    "    swhole_aic, swhole_bic = np.nan, np.nan\n",
    "    if len(s_whole) > lags:\n",
    "        try:\n",
    "            fit_whole = AutoReg(s_whole, lags=lags).fit()\n",
    "            swhole_resid_std = np.std(fit_whole.resid)\n",
    "            swhole_params = fit_whole.params\n",
    "            swhole_aic = fit_whole.aic\n",
    "            swhole_bic = fit_whole.bic\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "    # feats['ar_resid_std_left'] = s1_resid_std\n",
    "    # feats['ar_resid_std_right'] = s2_resid_std\n",
    "    # feats['ar_resid_std_whole'] = swhole_resid_std\n",
    "    # _add_diff_ratio_feats(feats, 'ar_resid_std', s1_resid_std, s2_resid_std)\n",
    "    # _add_contribution_ratio_feats(feats, 'ar_resid_std', s1_resid_std, s2_resid_std, swhole_resid_std)\n",
    "    \n",
    "    # feats['ar_aic_left'] = s1_aic\n",
    "    # feats['ar_aic_right'] = s2_aic\n",
    "    # feats['ar_aic_whole'] = swhole_aic\n",
    "    # _add_diff_ratio_feats(feats, 'ar_aic', s1_aic, s2_aic)\n",
    "    # _add_contribution_ratio_feats(feats, 'ar_aic', s1_aic, s2_aic, swhole_aic)\n",
    "\n",
    "    # feats['ar_bic_left'] = s1_bic\n",
    "    # feats['ar_bic_right'] = s2_bic\n",
    "    # feats['ar_bic_whole'] = swhole_bic\n",
    "    # _add_diff_ratio_feats(feats, 'ar_bic', s1_bic, s2_bic)\n",
    "    # _add_contribution_ratio_feats(feats, 'ar_bic', s1_bic, s2_bic, swhole_bic)\n",
    "    \n",
    "    # 比较模型系数\n",
    "    for i in range(len(s1_params)):\n",
    "        feats[f'ar_param_{i}_left'] = s1_params[i]\n",
    "        feats[f'ar_param_{i}_right'] = s2_params[i]\n",
    "        feats[f'ar_param_{i}_whole'] = swhole_params[i]\n",
    "        _add_diff_ratio_feats(feats, f'ar_param_{i}', s1_params[i], s2_params[i])\n",
    "        _add_contribution_ratio_feats(feats, f'ar_param_{i}', s1_params[i], s2_params[i], swhole_params[i])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 10. 分段损失 ---\n",
    "class RPTFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # 所有可用的cost类及其名称\n",
    "        self.cost_classes = {\n",
    "            # 'l1': rpt.costs.CostL1,               # 中位数\n",
    "            # 'l2': rpt.costs.CostL2,               # 均值\n",
    "            # 'clinear': rpt.costs.CostCLinear,     # 线性协方差\n",
    "            # 'rbf': rpt.costs.CostRbf,             # RBF核\n",
    "            # 'normal': rpt.costs.CostNormal,       # 协方差\n",
    "            # 'ar': rpt.costs.CostAR,               # 自回归\n",
    "            # 'mahalanobis': rpt.costs.CostMl,      # 马氏距离\n",
    "            # 'rank': rpt.costs.CostRank,           # 排名\n",
    "            'cosine': rpt.costs.CostCosine,       # 余弦距离\n",
    "        }\n",
    "\n",
    "    def calculate(self, cost, start, end):\n",
    "        result = cost.error(start, end)\n",
    "        if isinstance(result, (np.ndarray, list)) and np.array(result).size == 1:\n",
    "            return float(np.array(result).squeeze())\n",
    "        return result\n",
    "\n",
    "    def extract(self, signal, boundary):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            signal: 1D numpy array，单变量时间序列\n",
    "            boundary: int，分割点\n",
    "        输出：\n",
    "            result: dict，格式为 {cost_name: {'left': value, 'right': value}}\n",
    "        \"\"\"\n",
    "        signal = np.asarray(signal)\n",
    "        n = len(signal)\n",
    "        result = {}\n",
    "        for name, cls in self.cost_classes.items():\n",
    "            try:\n",
    "                if name == 'ar':\n",
    "                    cost = cls(order=4)\n",
    "                else:\n",
    "                    cost = cls()\n",
    "                cost.fit(signal)\n",
    "                left = self.calculate(cost, 0, boundary)\n",
    "                right = self.calculate(cost, boundary, n)\n",
    "                whole = self.calculate(cost, 0, n)\n",
    "                # diff = right - left if left is not None and right is not None else None\n",
    "                # ratio = right / (left + 1e-6) if left is not None and right is not None else None\n",
    "            except Exception:\n",
    "                left = None\n",
    "                right = None\n",
    "                whole = None\n",
    "                # diff = None\n",
    "                # ratio = None\n",
    "            # Move to _add_diff_ratio_feats, 'diff': diff, 'ratio': ratio\n",
    "            result[name] = {'left': left, 'right': right, 'whole': whole}\n",
    "        return result\n",
    "\n",
    "@register_feature(func_id=\"10\")\n",
    "def rupture_cost_features(u: pd.DataFrame) -> dict:\n",
    "    value = u['value'].values.astype(np.float32)\n",
    "    period = u['period'].values.astype(np.float32)\n",
    "    boundary = np.where(np.diff(period) != 0)[0].item()\n",
    "    feats = {}\n",
    "\n",
    "    extractor = RPTFeatureExtractor()\n",
    "    features = extractor.extract(value, boundary)\n",
    "\n",
    "    feats = {}\n",
    "    for k, v in features.items():\n",
    "        for seg, value in v.items():\n",
    "            feats[f'rpt_cost_{k}_{seg}'] = value\n",
    "        # _add_diff_ratio_feats(feats, f'rpt_cost_{k}', v['left'], v['right'])\n",
    "        # _add_contribution_ratio_feats(feats, f'rpt_cost_{k}', v['left'], v['right'], v['whole'])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3349f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_feature_func_sequential(func, X_df: pd.DataFrame, use_tqdm: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"顺序应用单个特征函数\"\"\"\n",
    "    all_ids = X_df.index.get_level_values(\"id\").unique()\n",
    "    iterator = (\n",
    "        tqdm(all_ids, desc=f\"Running {func.__name__} (sequentially)\")\n",
    "        if use_tqdm else all_ids\n",
    "    )\n",
    "    results = [\n",
    "        {**{'id': id_val}, **func(X_df.loc[id_val])}\n",
    "        for id_val in iterator\n",
    "    ]\n",
    "    return pd.DataFrame(results).set_index('id')\n",
    "\n",
    "def _apply_feature_func_parallel(func, X_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"并行应用单个特征函数\"\"\"\n",
    "    all_ids = X_df.index.get_level_values(\"id\").unique()\n",
    "    results = Parallel(n_jobs=config.N_JOBS)(\n",
    "        delayed(lambda df_id, id_val: {**{'id': id_val}, **func(df_id)})(X_df.loc[id_val], id_val)\n",
    "        for id_val in tqdm(all_ids, desc=f\"Running {func.__name__}\")\n",
    "    )\n",
    "    return pd.DataFrame(results).set_index('id')\n",
    "\n",
    "def _apply_transform_func(func, X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"执行变换函数\"\"\"\n",
    "    return func(X_df)\n",
    "\n",
    "def apply_transformation(X_df: pd.DataFrame, transform_funcs: List[str] = None) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    应用时序变换\n",
    "    \n",
    "    Args:\n",
    "        X_df: 输入数据框\n",
    "        transform_funcs: 要应用的变换函数名称列表，如果为None则应用所有注册的变换函数\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: 键为模态名称，值为对应的数据框\n",
    "    \"\"\"\n",
    "    if transform_funcs is None:\n",
    "        transform_funcs = list(TRANSFORM_REGISTRY.keys())\n",
    "    \n",
    "    # 验证变换函数是否存在\n",
    "    valid_transform_funcs = []\n",
    "    for func_name in transform_funcs:\n",
    "        if func_name not in TRANSFORM_REGISTRY:\n",
    "            pass\n",
    "            # logger.warning(f\"变换函数 {func_name} 未在注册表中找到，已跳过。\")\n",
    "        else:\n",
    "            valid_transform_funcs.append(func_name)\n",
    "    \n",
    "    transform_funcs = valid_transform_funcs\n",
    "    \n",
    "    # 存储所有模态的数据框\n",
    "    transformed_data = {}\n",
    "    \n",
    "    for func_name in transform_funcs:\n",
    "        # logger.info(f\"--- 开始应用变换函数: {func_name} ---\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        transform_info = TRANSFORM_REGISTRY[func_name]\n",
    "        func = transform_info['func']\n",
    "        output_mode_names = transform_info['output_mode_names']\n",
    "        \n",
    "        # 执行变换\n",
    "        transformed_results = _apply_transform_func(func, X_df)\n",
    "        \n",
    "        # 存储结果\n",
    "        for mode_name, mode_df in zip(output_mode_names, transformed_results):\n",
    "            transformed_data[mode_name] = mode_df\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        # logger.info(f\"'{func_name}' 变换完毕，耗时: {duration:.2f} 秒，生成模态: {output_mode_names}\")\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def clean_feature_names(df: pd.DataFrame, prefix: str = \"f\") -> pd.DataFrame:\n",
    "    \"\"\"清理特征名称，确保它们是合法的列名。\"\"\"\n",
    "    cleaned_columns = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        # 替换非法字符为 _\n",
    "        cleaned = re.sub(r'[^\\w]', '_', col)\n",
    "        # 防止开头是数字（如 \"123_feature\"）非法\n",
    "        if re.match(r'^\\d', cleaned):\n",
    "            cleaned = f\"{prefix}_{cleaned}\"\n",
    "        # 多个连续 _ 合并为一个\n",
    "        cleaned = re.sub(r'__+', '_', cleaned)\n",
    "        cleaned_columns.append(cleaned)\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea90bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_pairs(\n",
    "    operator_flag='mul', \n",
    "    mode_flag='RAW',\n",
    "    target_feature=['RAW_1_stats_cv_whole']\n",
    "):\n",
    "    result = {tf: [] for tf in target_feature}\n",
    "\n",
    "    for feat in REMAIN_FEATURES:\n",
    "        # 1. 只处理以 operator_flag 中任意前缀开头的字符串\n",
    "        if not feat.startswith(operator_flag + '_'):\n",
    "            continue\n",
    "\n",
    "        # 2. 只处理包含任意 target_feature 的字符串\n",
    "        if not any(tf in feat for tf in target_feature):\n",
    "            continue\n",
    "\n",
    "        # 3. 解析格式\n",
    "        def extract_raw_features(feat, mode_flag):\n",
    "            if mode_flag == 'RAW':\n",
    "                split = feat.split('_RAW')\n",
    "                raw_parts = []\n",
    "                for part in split[1:]:\n",
    "                    tokens = part.split('_')\n",
    "                    raw = 'RAW' + '_'.join(tokens)\n",
    "                    raw_parts.append(raw)\n",
    "                return raw_parts\n",
    "            return []\n",
    "\n",
    "        parts = extract_raw_features(feat, mode_flag)\n",
    "        # print(parts)\n",
    "        if len(parts) != 2:\n",
    "            continue  # 结构不合法\n",
    "        raw1, raw2 = parts\n",
    "\n",
    "        # 4. 如果两个都在 top_features，丢弃\n",
    "        if raw1 in TOP_FEATURES and raw2 in TOP_FEATURES:\n",
    "            continue\n",
    "\n",
    "        # 5. 找出包含的 target_feature 是哪个\n",
    "        for tf in target_feature:\n",
    "            if tf == raw1 and raw2 not in result[tf]:\n",
    "                result[tf].append(raw2)\n",
    "            elif tf == raw2 and raw1 not in result[tf]:\n",
    "                result[tf].append(raw1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03461576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interaction_features(\n",
    "    feature_df: pd.DataFrame, \n",
    "    create_mul: bool = True,\n",
    "    create_sqmul: bool = False,\n",
    "    create_add: bool = False,\n",
    "    create_sub: bool = False,\n",
    "    create_div: bool = False,\n",
    "    create_sq: bool = False,\n",
    "    create_onemulall: bool = False,\n",
    "    target_feature: str = ['RAW_1_stats_cv_whole']\n",
    "):\n",
    "    \"\"\"\n",
    "    根据特征重要性文件生成交互特征。\n",
    "    支持字典格式的特征数据。\n",
    "\n",
    "    Args:\n",
    "        feature_df (pd.DataFrame): 特征数据框。\n",
    "        top_features (list): 要创建交互项的特征列名。\n",
    "        create_mul (bool): 是否创建乘法交互项。默认为 True。\n",
    "        create_sqmul (bool): 是否创建乘法平方交互项。默认为 False。\n",
    "        create_add (bool): 是否创建加法交互项。默认为 False。\n",
    "        create_sub (bool): 是否创建减法交互项。默认为 False。\n",
    "        create_div (bool): 是否创建除法交互项。默认为 False。\n",
    "        create_sq (bool): 是否创建平方交互项。默认为 False。\n",
    "    \"\"\"\n",
    "    # 1. 加载重要性文件并获取 Top N 特征\n",
    "    logger.info(f\"选择 Top {len(TOP_FEATURES)} 特征进行交互项生成: {TOP_FEATURES}\")\n",
    "\n",
    "    # 2. 加载基础特征文件\n",
    "    initial_feature_count = len(feature_df.columns)\n",
    "\n",
    "    missing_features = [f for f in TOP_FEATURES if f not in feature_df.columns]\n",
    "    if missing_features:\n",
    "        logger.error(f\"以下 Top 特征在基础特征文件中缺失，无法创建交互项: {missing_features}\")\n",
    "        return\n",
    "\n",
    "    # 3. 创建交互特征\n",
    "    epsilon = 1e-6\n",
    "    interaction_features_dict = {}\n",
    "    \n",
    "    for f1, f2 in combinations(TOP_FEATURES, 2):\n",
    "        if create_mul:\n",
    "            interaction_features_dict[f'mul_{f1}_{f2}'] = feature_df[f1] * feature_df[f2]\n",
    "        if create_sqmul:\n",
    "            interaction_features_dict[f'sqmul_{f1}_{f2}'] = feature_df[f1] * (feature_df[f2] ** 2)\n",
    "            interaction_features_dict[f'sqmul_{f2}_{f1}'] = feature_df[f2] * (feature_df[f1] ** 2)\n",
    "        if create_sub:\n",
    "            interaction_features_dict[f'sub_{f1}_{f2}'] = feature_df[f1] - feature_df[f2]\n",
    "        if create_add:\n",
    "            interaction_features_dict[f'add_{f1}_{f2}'] = feature_df[f1] + feature_df[f2]\n",
    "        if create_div:\n",
    "            interaction_features_dict[f'div_{f1}_{f2}'] = feature_df[f1] / (feature_df[f2] + epsilon)\n",
    "            interaction_features_dict[f'div_{f2}_{f1}'] = feature_df[f2] / (feature_df[f1] + epsilon)\n",
    "\n",
    "    for f in TOP_FEATURES:\n",
    "        if create_sq:\n",
    "            interaction_features_dict[f'sq_{f}'] = feature_df[f] ** 2\n",
    "\n",
    "    onemulall_dict = extract_feature_pairs(operator_flag='mul', mode_flag='RAW', target_feature=target_feature)\n",
    "    for k, v in onemulall_dict.items():\n",
    "        logger.info(f\"target {k} onemulall 交互 {len(v)} 个特征。\")\n",
    "        for f in v:\n",
    "            if create_onemulall:\n",
    "                interaction_features_dict[f'mul_{k}_{f}'] = feature_df[k] * feature_df[f]\n",
    "    \n",
    "    # 一次性创建DataFrame，避免碎片化\n",
    "    if interaction_features_dict:\n",
    "        interaction_features = pd.DataFrame(interaction_features_dict, index=feature_df.index)\n",
    "    else:\n",
    "        interaction_features = pd.DataFrame(index=feature_df.index)\n",
    "    \n",
    "    if interaction_features.empty:\n",
    "        logger.info(\"没有选择任何交互项类型，操作中止。\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"成功创建 {len(interaction_features.columns)} 个交互特征。\")\n",
    "    logger.info(\"--- 新生成的交互特征列表 ---\")\n",
    "    logger.info(interaction_features.columns.tolist())\n",
    "    logger.info(\"------------------------------\")\n",
    "    \n",
    "    # 4. 合并并保存\n",
    "    feature_df = feature_df.drop(columns=interaction_features.columns, errors='ignore')\n",
    "    feature_df = feature_df.merge(interaction_features, left_index=True, right_index=True, how='left')\n",
    "    feature_df = clean_feature_names(feature_df)\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3805e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    model_directory_path: str,\n",
    "):\n",
    "    # For our baseline t-test approach, we don't need to train a model\n",
    "    # This is essentially an unsupervised approach calculated at inference time\n",
    "    model = None\n",
    "\n",
    "    # You could enhance this by training an actual model, for example:\n",
    "    # 1. Extract features from before/after segments of each time series\n",
    "    # 2. Train a classifier using these features and y_train labels\n",
    "    # 3. Save the trained model\n",
    "\n",
    "    joblib.dump(model, os.path.join(model_directory_path, 'none.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fbad0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_directory_path):\n",
    "    \"\"\"Load all LightGBM model files saved with joblib and prepare them for ensemble\"\"\"\n",
    "    models = []\n",
    "    dirpath = Path(model_directory_path)\n",
    "    model_files = list(dirpath.glob('*.pkl'))\n",
    "    \n",
    "    if not model_files:\n",
    "        logger.warning(f\"Warning: No model files found under {model_directory_path}!\")\n",
    "        return models\n",
    "    logger.info(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            logger.info(f\"Loading model: {model_path}\")\n",
    "            model = joblib.load(model_path)\n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dde3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "    X_test: typing.Iterable[pd.DataFrame],\n",
    "    model_directory_path: str,\n",
    "):\n",
    "    global logger, log_file_path\n",
    "    if logger is None:  # 防止重复初始化\n",
    "        logger, log_file_path = get_logger('Inference', Path(os.path.join(model_directory_path, 'infer_logs')), verbose=False)\n",
    "\n",
    "    # Load models\n",
    "    models = load_models(model_directory_path)\n",
    "\n",
    "    # Funcs to run\n",
    "    funcs_to_run = [\n",
    "        f for f in FEATURE_REGISTRY.keys() \n",
    "        if f not in EXPERIMENTAL_FEATURES\n",
    "    ]\n",
    "    trans_to_run = None\n",
    "    logger.info(f\"未指定特征函数，将运行所有 {len(funcs_to_run)} 个非实验性特征。\")\n",
    "\n",
    "    yield  # Mark as ready\n",
    "\n",
    "    # X_test can only be iterated once.\n",
    "    # Before getting the next dataset, you must predict the current one.\n",
    "    for X_df in tqdm(X_test, desc=\"Inference Progress\"):\n",
    "        logger.info(X_df)\n",
    "        logger.info(\"未找到基础特征文件，将创建全新的特征集。\")\n",
    "        feature_df, metadata = pd.DataFrame(index=X_df.index.get_level_values('id').unique()), {}\n",
    "        logger.info(feature_df)\n",
    "        logger.info(\"=== 开始时序分解 ===\")\n",
    "        transformed_data = apply_transformation(X_df, trans_to_run)\n",
    "        logger.info(f\"分解完成，共生成 {len(transformed_data)} 个模态: {list(transformed_data.keys())}\")\n",
    "\n",
    "        loaded_features = feature_df.columns.tolist()\n",
    "        initial_feature_count = len(feature_df.columns)\n",
    "\n",
    "        for mode_name, mode_df in transformed_data.items():\n",
    "            logger.info(f\"=== 开始为模态 '{mode_name}' 生成特征 ===\")\n",
    "            for func_name in funcs_to_run:\n",
    "                logger.info(f\"--- 开始生成特征: {func_name} ---\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                feature_info = FEATURE_REGISTRY[func_name]\n",
    "                func = feature_info['func']\n",
    "                is_parallelizable = feature_info['parallelizable']\n",
    "                is_parallelizable = None  # 强制禁用并行化\n",
    "                func_id = feature_info['func_id']\n",
    "                \n",
    "                if is_parallelizable:\n",
    "                    new_features_df = _apply_feature_func_parallel(func, mode_df)\n",
    "                else:\n",
    "                    logger.info(f\"函数 '{func_name}' 不可并行化，将顺序执行。\")\n",
    "                    new_features_df = _apply_feature_func_sequential(func, mode_df)\n",
    "                new_features_df.columns = [f\"{mode_name}_{func_id}_{col}\" for col in new_features_df.columns]\n",
    "\n",
    "                # 记录日志\n",
    "                duration = time.time() - start_time\n",
    "                logger.info(f\"'{func_name}' 生成完毕，耗时: {duration:.2f} 秒。\")\n",
    "                logger.info(f\"  新生成特征列名: {new_features_df.columns.tolist()}\")\n",
    "                \n",
    "                for col in new_features_df.columns:\n",
    "                    null_ratio = new_features_df[col].isnull().sum() / len(new_features_df)\n",
    "                    zero_ratio = (new_features_df[col] == 0).sum() / len(new_features_df)\n",
    "                    if null_ratio > 0.1:\n",
    "                        logger.warning(f\"    - '{col}': 空值比例={null_ratio:.2%}, 零值比例={zero_ratio:.2%}\")\n",
    "\n",
    "                # 删除旧版本特征（如果存在），然后合并\n",
    "                feature_df = feature_df.drop(columns=new_features_df.columns, errors='ignore')\n",
    "                feature_df = feature_df.merge(new_features_df, left_index=True, right_index=True, how='left')\n",
    "                # feature_df, removed_features = check_new_features_corr(feature_df, loaded_features, drop_flag=True, threshold=0.95)\n",
    "                feature_df = clean_feature_names(feature_df)\n",
    "                loaded_features = feature_df.columns.tolist()\n",
    "\n",
    "        missing_features = [f for f in TOP_FEATURES if f not in feature_df.columns]\n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing TOP_FEATURES before interaction: {missing_features}\")\n",
    "        feature_df = generate_interaction_features(\n",
    "            feature_df, \n",
    "            create_mul=True, create_sqmul=True, create_add=True, create_sub=True, create_div=True, create_sq=True, create_onemulall=True,\n",
    "            target_feature = ['RAW_1_stats_cv_whole']\n",
    "        )\n",
    "        missing_features = [f for f in REMAIN_FEATURES if f not in feature_df.columns]\n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing REMAIN_FEATURES before filter: {missing_features}\")\n",
    "        feature_df = feature_df[REMAIN_FEATURES]\n",
    "        \n",
    "        logger.info(feature_df)\n",
    "        logger.info(\"--- 生成后完整特征列表 ---\")\n",
    "        logger.info(f\"{feature_df.columns.tolist()}\")\n",
    "        logger.info(\"-----------------------------\")\n",
    "        logger.info(f\"生成/更新完成。总特征数: {len(feature_df.columns)}\")\n",
    "\n",
    "        def ensemble_predict(models, X):\n",
    "            preds = [model.predict_proba(X)[:, 1] for model in models]\n",
    "            if len(preds) == 0:\n",
    "                logger.warning(\"No predictions generated, returning zeros.\")\n",
    "                return np.zeros(len(X))\n",
    "            return np.mean(preds, axis=0)\n",
    "        prediction = ensemble_predict(models, feature_df)\n",
    "        prediction = 1 - prediction\n",
    "\n",
    "        yield prediction  # Send the prediction for the current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196f3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:02:07\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m10:02:07\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m10:02:11\u001b[0m started\n",
      "\u001b[32m10:02:11\u001b[0m running local test\n",
      "\u001b[32m10:02:11\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m10:02:11\u001b[0m \n",
      "\u001b[32m10:02:15\u001b[0m starting unstructured loop...\n",
      "\u001b[32m10:02:15\u001b[0m executing - command=train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
      "data\\X_train.parquet: already exists, file length match\n",
      "data\\X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
      "data\\X_test.reduced.parquet: already exists, file length match\n",
      "data\\y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
      "data\\y_train.parquet: already exists, file length match\n",
      "data\\y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
      "data\\y_test.reduced.parquet: already exists, file length match\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m10:02:16\u001b[0m executing - command=infer\n",
      "Inference Progress: 101it [00:55,  1.81it/s]\n",
      "\u001b[32m10:03:13\u001b[0m checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
      "\u001b[32m10:03:13\u001b[0m executing - command=infer\n",
      "Inference Progress: 30it [00:16,  1.78it/s]\n",
      "\u001b[32m10:03:30\u001b[0m determinism check: passed\n",
      "\u001b[32m10:03:30\u001b[0m \u001b[33msave prediction - path=data\\prediction.parquet\u001b[0m\n",
      "\u001b[32m10:03:30\u001b[0m ended\n",
      "\u001b[32m10:03:30\u001b[0m \u001b[33mduration - time=00:01:19\u001b[0m\n",
      "\u001b[32m10:03:30\u001b[0m \u001b[33mmemory - before=\"321.32 MB\" after=\"405.96 MB\" consumed=\"84.64 MB\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "crunch.test(\n",
    "    # Uncomment to disable the train\n",
    "    # force_first_train=False,\n",
    "\n",
    "    # Uncomment to disable the determinism check\n",
    "    # no_determinism_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf19e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.928542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.914761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.817947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.890018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.696638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>0.931696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>0.958928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>0.822730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>0.969648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>0.947873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction\n",
       "id               \n",
       "10001    0.928542\n",
       "10002    0.914761\n",
       "10003    0.817947\n",
       "10004    0.890018\n",
       "10005    0.696638\n",
       "...           ...\n",
       "10097    0.931696\n",
       "10098    0.958928\n",
       "10099    0.822730\n",
       "10100    0.969648\n",
       "10101    0.947873\n",
       "\n",
       "[101 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0c087a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0812206572769953)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the targets\n",
    "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
    "\n",
    "# Call the scoring function\n",
    "sklearn.metrics.roc_auc_score(\n",
    "    target,\n",
    "    prediction,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
