{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cce981d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   # == 2.2.3\n",
    "import numpy as np    # == 2.2.6\n",
    "import scipy.stats\n",
    "import statsmodels.tsa.api as tsa \n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import antropy \n",
    "import sklearn\n",
    "from tsfresh.feature_extraction import feature_calculators as tsfresh_fe\n",
    "import ruptures as rpt\n",
    "\n",
    "import lightgbm as lgb  # == 4.6.0\n",
    "import catboost as cat\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import inspect\n",
    "import typing\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import InterpolationWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca5044c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded inline runner with module: <module '__main__'>\n",
      "\n",
      "cli version: 6.6.1\n",
      "available ram: 15.73 gb\n",
      "available cpu: 16 core\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "import crunch\n",
    "\n",
    "# Load the Crunch Toolings\n",
    "crunch = crunch.load_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68ce5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\", InterpolationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter('ignore', np.exceptions.RankWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b135c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "def get_logger(name: str, log_dir: Path, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    获取一个配置好的 logger 实例，它会生成带时间戳的详细日志。\n",
    "    \"\"\"\n",
    "    # 确保日志目录存在\n",
    "    log_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # 1. 创建带时间戳的详细日志文件名\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    detail_log_file = log_dir / f'{name.lower()}_{timestamp}.log'\n",
    "\n",
    "    # 2. 为 logger 设置一个唯一的名称（基于时间戳），避免冲突\n",
    "    logger = logging.getLogger(f\"{name}-{timestamp}\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # 防止将日志消息传播到根 logger\n",
    "    logger.propagate = False\n",
    "\n",
    "    # 如果已经有处理器，则不重复添加\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # 3. 创建详细日志的文件处理器\n",
    "    detail_handler = logging.FileHandler(detail_log_file, mode='a', encoding='utf-8')\n",
    "    detail_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    detail_handler.setFormatter(detail_formatter)\n",
    "    logger.addHandler(detail_handler)\n",
    "    \n",
    "    # 4. 创建控制台处理器\n",
    "    # 控制台 - INFO级别 (受verbose控制)\n",
    "    if verbose:\n",
    "        info_handler = logging.StreamHandler(sys.stdout)\n",
    "        info_handler.setLevel(logging.INFO)\n",
    "        info_handler.addFilter(lambda record: record.levelno == logging.INFO)\n",
    "        info_formatter = logging.Formatter('%(message)s')\n",
    "        info_handler.setFormatter(info_formatter)\n",
    "        logger.addHandler(info_handler)\n",
    "\n",
    "    # 控制台 - WARNING及以上 (始终输出)\n",
    "    warn_handler = logging.StreamHandler(sys.stdout)\n",
    "    warn_handler.setLevel(logging.WARNING)\n",
    "    warn_formatter = logging.Formatter('%(levelname)s: %(message)s')\n",
    "    warn_handler.setFormatter(warn_formatter)\n",
    "    logger.addHandler(warn_handler)\n",
    "\n",
    "    return logger, detail_log_file # 返回 logger 和日志文件路径 \n",
    "\n",
    "logger = None\n",
    "log_file_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68a16c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "class Config:\n",
    "    # --- Feature Engineer ---\n",
    "    N_JOBS = -1\n",
    "    SEED = 42\n",
    "\n",
    "    # --- Data Enhancement ---\n",
    "    # 数据增强配置，指定要加载的增强数据ID列表\n",
    "    # 如果为'0'，则只使用原始数据\n",
    "    ENHANCEMENT_IDS = [\"0\"] \n",
    "\n",
    "    # --- Model ---\n",
    "    TRAIN_STRATEGY = 'cv'   # 'cv' or 'one'\n",
    "    MODEL = 'LGB'  # 'LGB' or 'CAT\n",
    "    LGBM_PARAMS = {\n",
    "        # --- 基础设定 ---\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': 3000, \n",
    "        'learning_rate': 0.005,\n",
    "        'num_leaves': 36,\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': N_JOBS,\n",
    "        'is_unbalance': True,\n",
    "\n",
    "        # --- 正则化和采样 ---\n",
    "        'reg_alpha': 3,          # L1 正则化\n",
    "        'reg_lambda': 3,         # L2 正则化\n",
    "        'colsample_bytree': 0.8,   # 构建树时对特征的列采样率\n",
    "        'subsample': 0.8,          # 训练样本的采样率\n",
    "    }\n",
    "    CAT_PARAMS = {\n",
    "        # --- 基础设定 ---\n",
    "        'bootstrap_type': 'Bernoulli',\n",
    "        'loss_function': 'Logloss',\n",
    "        'eval_metric': 'AUC',\n",
    "        'task_type': 'GPU',\n",
    "        'iterations': 4000, \n",
    "        'learning_rate': 0.005,\n",
    "        'depth': 7,\n",
    "        'random_seed': SEED,\n",
    "        'thread_count': N_JOBS,\n",
    "        \n",
    "        # --- 正则化和采样 ---\n",
    "        'subsample': 0.8,\n",
    "        # 'rsm': 0.7,\n",
    "        'l2_leaf_reg': 3,\n",
    "    }\n",
    "    XGB_PARAMS = {\n",
    "        # --- 基础设定 ---\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'device': 'cuda', \n",
    "        'n_estimators': 3000,\n",
    "        'learning_rate': 0.005,\n",
    "        'max_leaves': 29,\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': N_JOBS,\n",
    "        'verbosity': 0, \n",
    "        \n",
    "        # --- 正则化和采样 ---\n",
    "        'reg_alpha': 3,          # L1 正则化\n",
    "        'reg_lambda': 3,         # L2 正则化\n",
    "        'colsample_bytree': 0.8, # 构建树时对特征的列采样率\n",
    "        'subsample': 0.8,        # 训练样本的采样率\n",
    "    }\n",
    "\n",
    "    # --- Early Stopping ---\n",
    "    # 设置为 >0 启用早停；设置为 0 禁用早停\n",
    "    EARLY_STOPPING_ROUNDS = 0\n",
    "\n",
    "    # --- CV ---\n",
    "    CV_PARAMS = {\n",
    "        'n_splits': 5,\n",
    "        'shuffle': True,\n",
    "        'random_state': SEED\n",
    "    } \n",
    "\n",
    "    # --- Exclude Features ---\n",
    "    # 在这里定义不希望在 \"一键生成所有特征\" 时运行的函数名称\n",
    "    # 如果要运行这些特征，需要在命令行中通过 --funcs 参数明确指定\n",
    "    # 例如: python -m experiment.main gen-feats --funcs ar_model_features\n",
    "    EXPERIMENTAL_FEATURES = [\n",
    "        \n",
    "    ] \n",
    "\n",
    "    # --- Top Features ---\n",
    "    TOP_FEATURES = [\n",
    "        'RAW_1_stats_cv_mul_std_whole',\n",
    "        'ASINH_1_stats_cv_mul_std_whole',\n",
    "        'RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left',\n",
    "        'RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'RAW_7_sample_entropy_left',\n",
    "        'RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left',\n",
    "        'RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "        'RAW_8_index_mass_quantile_q_0_1_right',\n",
    "        'DIFF_2_levene_pvalue',\n",
    "        'CUMSUM_2_ad_pvalue',\n",
    "        'RAW_1_stats_cv_whole',\n",
    "        'RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left',\n",
    "        'ASINH_1_stats_cv_whole',\n",
    "        'CUMSUM_1_stats_theil_sen_slope_whole',\n",
    "        'RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_8_ql_0_2_ratio_to_whole_left',\n",
    "        'RAW_8_ratio_beyond_r_sigma_1_left',\n",
    "        'DIFF_2_bartlett_pvalue',\n",
    "        'RAW_8_benford_correlation_whole',\n",
    "        'RAW_2_bartlett_stat',\n",
    "        'RAW_7_sample_entropy_whole',\n",
    "    ]\n",
    "\n",
    "    # --- Remain Features ---\n",
    "    REMAIN_FEATURES = [\n",
    "        'div_RAW_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left',\n",
    "        'div_ASINH_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left',\n",
    "        'add_RAW_1_stats_cv_mul_std_whole_CUMSUM_1_stats_theil_sen_slope_whole',\n",
    "        'div_RAW_1_stats_cv_whole_RAW_7_sample_entropy_whole',\n",
    "        'div_RAW_7_sample_entropy_left_RAW_1_stats_cv_whole',\n",
    "        'add_ASINH_1_stats_cv_mul_std_whole_CUMSUM_1_stats_theil_sen_slope_whole',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left',\n",
    "        'add_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_8_index_mass_quantile_q_0_1_right',\n",
    "        'cross_mul_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left_CUMSUM_1_stats_range_ratio_to_whole_left',\n",
    "        'cross_mul_CUMSUM_2_ad_pvalue_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_6_ql_0_2_whole',\n",
    "        'cross_mul_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left_RAW_8_quantile_0_6_ratio_to_whole_left',\n",
    "        'sub_ASINH_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_1_right_RAW_10_rpt_cost_cosine_whole',\n",
    "        'sub_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_8_ql_0_2_ratio_to_whole_left',\n",
    "        'sub_RAW_7_sample_entropy_left_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left',\n",
    "        'sqmul_CUMSUM_1_stats_theil_sen_slope_whole_RAW_7_sample_entropy_left',\n",
    "        'div_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "        'div_RAW_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'sqmul_CUMSUM_1_stats_theil_sen_slope_whole_RAW_7_sample_entropy_whole',\n",
    "        'div_CUMSUM_2_ad_pvalue_RAW_1_stats_cv_mul_std_whole',\n",
    "        'sub_RAW_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_7_sample_entropy_whole_CUMSUM_2_kpss_pvalue_ratio_to_whole_left',\n",
    "        'cross_mul_CUMSUM_1_stats_theil_sen_slope_whole_RAW_7_sample_entropy_diff',\n",
    "        'add_RAW_8_ratio_beyond_r_sigma_1_left_DIFF_2_bartlett_pvalue',\n",
    "        'cross_mul_RAW_8_benford_correlation_whole_RAW_8_count_above_0_whole',\n",
    "        'sub_DIFF_2_levene_pvalue_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_8_ql_0_2_ratio_to_whole_left',\n",
    "        'cross_mul_CUMSUM_1_stats_theil_sen_slope_whole_RAW_8_ratio_value_number_to_time_series_length_diff',\n",
    "        'sqmul_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_8_benford_correlation_whole_CUMSUM_8_change_quantiles_f_agg_var_isabs_False_qh_0_4_ql_0_2_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_2_bartlett_stat_RAW_1_stats_median_ratio',\n",
    "        'cross_mul_CUMSUM_1_stats_theil_sen_slope_whole_CUMSUM_8_change_quantiles_f_agg_var_isabs_False_qh_1_0_ql_0_4_contribution_right',\n",
    "        'div_RAW_2_bartlett_stat_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left',\n",
    "        'sqmul_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_8_ql_0_2_ratio_to_whole_left_RAW_7_sample_entropy_whole',\n",
    "        'sub_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_CUMSUM_1_stats_theil_sen_slope_whole',\n",
    "        'cross_mul_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left_DIFF_8_ratio_value_number_to_time_series_length_ratio_to_whole_right',\n",
    "        'cross_mul_CUMSUM_1_stats_theil_sen_slope_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_1_right_ASINH_8_ar_coefficient_coeff_8_k_10_ratio_to_whole_right',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_quantile_0_4_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_7_sample_entropy_left_ASINH_8_ratio_beyond_r_sigma_1_5_ratio_to_whole_right',\n",
    "        'cross_mul_RAW_8_ratio_beyond_r_sigma_1_left_CUMSUM_2_kpss_pvalue_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_2_bartlett_stat_RAW_2_jb_pvalue_ratio_to_whole_right',\n",
    "        'cross_mul_CUMSUM_2_ad_pvalue_RAW_2_levene_pvalue',\n",
    "        'sub_RAW_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_DIFF_8_ratio_value_number_to_time_series_length_ratio_to_whole_right',\n",
    "        'div_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'cross_mul_RAW_7_sample_entropy_left_CUMSUM_8_last_location_of_maximum_whole',\n",
    "        'DIFF_7_hjorth_complexity_ratio_to_whole_right',\n",
    "        'cross_mul_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_4_autocorr_lag1_ratio',\n",
    "        'cross_mul_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_8_ql_0_2_ratio_to_whole_left_CUMSUM_3_linear_trend_std_err_left',\n",
    "        'div_RAW_7_sample_entropy_left_RAW_8_ratio_beyond_r_sigma_1_left',\n",
    "        'cross_mul_CUMSUM_2_ad_pvalue_RAW_8_quantile_0_6_whole',\n",
    "        'cross_mul_CUMSUM_2_ad_pvalue_DIFF_8_linear_trend_attr_intercept_contribution_right',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_1_right_DIFF_8_agg_linear_trend_attr_rvalue_chunk_len_10_f_agg_mean_whole',\n",
    "        'cross_mul_RAW_8_benford_correlation_whole_RAW_8_number_peaks_50_diff',\n",
    "        'cross_mul_DIFF_2_levene_pvalue_RAW_2_levene_stat',\n",
    "        'div_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left',\n",
    "        'DIFF_7_spectral_entropy_ratio_to_whole_left',\n",
    "        'sub_RAW_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'mul_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_8_benford_correlation_whole_RAW_8_quantile_0_6_ratio_to_whole_left',\n",
    "        'add_RAW_8_percentage_of_reoccurring_datapoints_to_all_datapoints_contribution_left_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left_DIFF_7_detrended_fluctuation_whole',\n",
    "        'cross_mul_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_8_ql_0_2_ratio_to_whole_left_RAW_2_jb_pvalue_ratio_to_whole_right',\n",
    "        'cross_mul_RAW_8_percentage_of_reoccurring_values_to_all_values_ratio_to_whole_left_CUMSUM_1_stats_max_ratio_to_whole_left',\n",
    "        'sqmul_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_index_mass_quantile_q_0_8_ratio_to_whole_left',\n",
    "        'cross_mul_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left_DIFF_8_benford_correlation_whole',\n",
    "        'cross_mul_RAW_8_change_quantiles_f_agg_var_isabs_True_qh_0_8_ql_0_2_ratio_to_whole_left_RAW_8_energy_ratio_by_chunks_num_segments_10_segment_focus_9_left',\n",
    "        'cross_mul_RAW_8_index_mass_quantile_q_0_1_right_CUMSUM_1_stats_max_ratio_to_whole_left',\n",
    "        'cross_mul_CUMSUM_2_ad_pvalue_DIFF_8_fft_coefficient_attr_imag_coeff_2_left',\n",
    "        'sub_RAW_8_index_mass_quantile_q_0_1_right_RAW_8_benford_correlation_whole',\n",
    "        'cross_mul_CUMSUM_2_ad_pvalue_CUMSUM_3_linear_trend_std_err_left',\n",
    "        'div_ASINH_1_stats_cv_mul_std_whole_RAW_8_percentage_of_reoccurring_values_to_all_values_contribution_left',\n",
    "        'cross_mul_RAW_8_ratio_value_number_to_time_series_length_ratio_to_whole_left_RAW_7_perm_entropy_left',\n",
    "        'cross_mul_RAW_8_benford_correlation_whole_RAW_8_ratio_value_number_to_time_series_length_whole',\n",
    "    ]\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3c64a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "# --- 时序变换函数注册表 ---\n",
    "TRANSFORM_REGISTRY = {}\n",
    "\n",
    "def register_transform(_func=None, *, output_mode_names=[]):\n",
    "    \"\"\"一个用于注册时序变换函数的装饰器。\"\"\"\n",
    "    def decorator_register(func):\n",
    "        TRANSFORM_REGISTRY[func.__name__] = {\n",
    "            \"func\": func, \n",
    "            \"output_mode_names\": output_mode_names\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    if _func is None:\n",
    "        # Used as @register_transform(output_mode_names=...)\n",
    "        return decorator_register\n",
    "    else:\n",
    "        # Used as @register_transform\n",
    "        return decorator_register(_func)\n",
    "\n",
    "@register_transform(output_mode_names=['RAW'])\n",
    "def no_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    原始时序\n",
    "    \"\"\"\n",
    "    result_dfs = []\n",
    "    result_dfs.append(X_df)\n",
    "\n",
    "    return result_dfs\n",
    "\n",
    "# @register_transform(output_mode_names=['MAtrend', 'MAresid'])\n",
    "# def moving_average_decomposition(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "#     \"\"\"\n",
    "#     滑动平均分解\n",
    "#     Args:\n",
    "#         X_df: 输入数据框，包含MultiIndex (id, time) 和 columns ['value', 'period']\n",
    "#     Returns:\n",
    "#         List[pd.DataFrame]: 包含两个数据框的列表 [趋势值, 残差值]\n",
    "#     \"\"\"\n",
    "#     X_df_sorted = X_df.sort_index()\n",
    "#     result_dfs = []\n",
    "    \n",
    "#     # 为每个模态创建一个空的数据框\n",
    "#     for mode_name in ['trend', 'resid']:\n",
    "#         mode_df = X_df_sorted.copy()\n",
    "#         mode_df['value'] = np.nan\n",
    "#         result_dfs.append(mode_df)\n",
    "    \n",
    "#     # 对每个id进行分解\n",
    "#     for series_id in X_df_sorted.index.get_level_values('id').unique():\n",
    "#         series_data = X_df_sorted.loc[series_id]\n",
    "#         series_data = series_data.sort_index()\n",
    "#         values = series_data['value'].values\n",
    "        \n",
    "#         # 滑动平均分解\n",
    "#         window_size = 200\n",
    "#         trend = pd.Series(values).rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "#         trend.iloc[:window_size//2] = trend.iloc[window_size//2]\n",
    "#         trend.iloc[-(window_size//2):] = trend.iloc[-(window_size//2)]\n",
    "        \n",
    "#         residual = values - trend.values\n",
    "        \n",
    "#         result_dfs[0].loc[series_id, 'value'] = trend.values  # 趋势值\n",
    "#         result_dfs[1].loc[series_id, 'value'] = residual  # 残差值\n",
    "    \n",
    "#     return result_dfs\n",
    "\n",
    "@register_transform(output_mode_names=['CUMSUM'])\n",
    "def cumsum_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    累计和变换\n",
    "    Args:\n",
    "        X_df: 输入数据框，包含MultiIndex (id, time) 和 columns ['value', 'period']\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: 包含一个数据框的列表 [累计和值]\n",
    "    \"\"\"\n",
    "    X_df_sorted = X_df.sort_index()\n",
    "    result_dfs = []\n",
    "\n",
    "    result_df = X_df_sorted.copy()\n",
    "    result_df['value'] = np.nan\n",
    "    \n",
    "    for series_id in X_df_sorted.index.get_level_values('id').unique():\n",
    "        series_data = X_df_sorted.loc[series_id]\n",
    "        series_data = series_data.sort_index()\n",
    "        values = series_data['value'].values\n",
    "        \n",
    "        cumsum_values = np.cumsum(values)\n",
    "        result_df.loc[series_id, 'value'] = cumsum_values\n",
    "    \n",
    "    result_dfs.append(result_df)\n",
    "    return result_dfs\n",
    "\n",
    "@register_transform(output_mode_names=['DIFF'])\n",
    "def diff_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    差分变换\n",
    "    Args:\n",
    "        X_df: 输入数据框，包含MultiIndex (id, time) 和 columns ['value', 'period']\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: 包含一个数据框的列表 [差分值]\n",
    "    \"\"\"\n",
    "    X_df_sorted = X_df.sort_index()\n",
    "    result_dfs = []\n",
    "    \n",
    "    result_df = X_df_sorted.copy()\n",
    "    result_df['value'] = np.nan\n",
    "    \n",
    "    for series_id in X_df_sorted.index.get_level_values('id').unique():\n",
    "        series_data = X_df_sorted.loc[series_id]\n",
    "        series_data = series_data.sort_index()\n",
    "        values = series_data['value'].values\n",
    "        \n",
    "        diff_values = np.diff(values, prepend=0)  # 使用prepend=0使长度保持一致\n",
    "        result_df.loc[series_id, 'value'] = diff_values\n",
    "    \n",
    "    result_dfs.append(result_df)\n",
    "    return result_dfs\n",
    "\n",
    "@register_transform(output_mode_names=['ASINH'])\n",
    "def asinh_transformation(X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    反双曲正弦变换\n",
    "    Args:\n",
    "        X_df: 输入数据框，包含MultiIndex (id, time) 和 columns ['value', 'period']\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: 包含一个数据框的列表 [反双曲正弦值]\n",
    "    \"\"\"\n",
    "    X_df_sorted = X_df.sort_index()\n",
    "    result_dfs = []\n",
    "    \n",
    "    result_df = X_df_sorted.copy()\n",
    "    result_df['value'] = np.nan\n",
    "    \n",
    "    for series_id in X_df_sorted.index.get_level_values('id').unique():\n",
    "        series_data = X_df_sorted.loc[series_id]\n",
    "        series_data = series_data.sort_index()\n",
    "        values = series_data['value'].values\n",
    "        \n",
    "        asinh_values = np.arcsinh(values)\n",
    "        result_df.loc[series_id, 'value'] = asinh_values\n",
    "    \n",
    "    result_dfs.append(result_df)\n",
    "    return result_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e066617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @crunch/keep:on\n",
    "# --- 特征函数注册表 ---\n",
    "FEATURE_REGISTRY = {}\n",
    "\n",
    "def register_feature(_func=None, *, parallelizable=True, func_id=\"\"):\n",
    "    \"\"\"一个用于注册特征函数的装饰器，可以标记特征是否可并行化。\"\"\"\n",
    "    def decorator_register(func):\n",
    "        FEATURE_REGISTRY[func.__name__] = {\n",
    "            \"func\": func, \n",
    "            \"parallelizable\": parallelizable,\n",
    "            \"func_id\": func_id\n",
    "        }\n",
    "        return func\n",
    "\n",
    "    if _func is None:\n",
    "        # Used as @register_feature(parallelizable=...)\n",
    "        return decorator_register\n",
    "    else:\n",
    "        # Used as @register_feature\n",
    "        return decorator_register(_func)\n",
    "\n",
    "def _add_diff_ratio_feats(feats: dict, name: str, left, right):\n",
    "    \"\"\"\n",
    "    一个辅助函数，用于向特征字典中添加差异和比例特征。\n",
    "\n",
    "    Args:\n",
    "        feats (dict): 要更新的特征字典。\n",
    "        name (str): 特征的基础名称 (例如, 'stats_mean')。\n",
    "        left (float): 左侧分段的特征值。\n",
    "        right (float): 右侧分段的特征值。\n",
    "    \"\"\"\n",
    "    # check nan/None \n",
    "    if np.isnan(left) or np.isnan(right) or left is None or right is None:\n",
    "        feats[f'{name}_diff'] = 0.0\n",
    "        feats[f'{name}_ratio'] = 0.0\n",
    "        return\n",
    "    # 做差\n",
    "    feats[f'{name}_diff'] = right - left\n",
    "    # 做比\n",
    "    feats[f'{name}_ratio'] = right / (left + 1e-6)\n",
    "\n",
    "\n",
    "def _add_contribution_ratio_feats(feats: dict, name: str, left, right, whole):\n",
    "    \"\"\"\n",
    "    一个辅助函数，用于向特征字典中添加贡献度和与整体的比例特征。\n",
    "\n",
    "    Args:\n",
    "        feats (dict): 要更新的特征字典。\n",
    "        name (str): 特征的基础名称 (例如, 'stats_mean')。\n",
    "        left (float): 左侧分段的特征值。\n",
    "        right (float): 右侧分段的特征值。\n",
    "        whole (float): 整个序列的特征值。\n",
    "    \"\"\"\n",
    "    # check nan/None \n",
    "    if np.isnan(left) or np.isnan(right) or np.isnan(whole) or left is None or right is None or whole is None :\n",
    "        feats[f'{name}_contribution_left'] = 0.0\n",
    "        feats[f'{name}_contribution_right'] = 0.0\n",
    "        feats[f'{name}_ratio_to_whole_left'] = 0.0\n",
    "        feats[f'{name}_ratio_to_whole_right'] = 0.0\n",
    "        return\n",
    "    # 特征贡献度\n",
    "    feats[f'{name}_contribution_left'] = left / (left + right + 1e-6)\n",
    "    feats[f'{name}_contribution_right'] = right / (left + right + 1e-6)\n",
    "    # 与整体特征的比例\n",
    "    feats[f'{name}_ratio_to_whole_left'] = left / (whole + 1e-6)\n",
    "    feats[f'{name}_ratio_to_whole_right'] = right / (whole + 1e-6)\n",
    "\n",
    "# --- 1. 分布统计特征 ---\n",
    "def safe_cv(s):\n",
    "    s = pd.Series(s)\n",
    "    m = s.mean()\n",
    "    std = s.std()\n",
    "    return std / m if abs(m) > 1e-6 else 0.0\n",
    "\n",
    "def safe_cv_mul_std(s):\n",
    "    s = pd.Series(s)\n",
    "    m = s.mean()\n",
    "    std = s.std()\n",
    "    return std**2 / m if abs(m) > 1e-6 else 0.0\n",
    "\n",
    "def rolling_std_mean(s, window=50):\n",
    "    s = pd.Series(s)\n",
    "    if len(s) < window:\n",
    "        return 0.0\n",
    "    return s.rolling(window=window).std().dropna().mean()\n",
    "\n",
    "def slope_theil_sen(s):\n",
    "    s = pd.Series(s)\n",
    "    if len(s) < 2:\n",
    "        return 0.0\n",
    "    try:\n",
    "        slope, intercept, _, _ = scipy.stats.theilslopes(s.values, np.arange(len(s)))\n",
    "        return slope\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "class STATSFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # 所有可用的func类及其名称\n",
    "        self.func_classes = {\n",
    "            # 'mean': np.mean,\n",
    "            'median': np.median,\n",
    "            'max': np.max,\n",
    "            # 'min': np.min,\n",
    "            'range': lambda x: np.max(x) - np.min(x),\n",
    "            # 'std': np.std,\n",
    "            # 'skew': scipy.stats.skew,\n",
    "            # 'kurt': scipy.stats.kurtosis,\n",
    "            'cv': safe_cv,\n",
    "            # 'mean_of_rolling_std': rolling_std_mean,\n",
    "            'theil_sen_slope': slope_theil_sen,\n",
    "            'cv_mul_std': safe_cv_mul_std,\n",
    "        }\n",
    "    \n",
    "    def fit(self, signal):\n",
    "        self.signal = np.asarray(signal)\n",
    "        self.n = len(signal)\n",
    "\n",
    "    def calculate(self, func, start, end):\n",
    "        result = func(self.signal[start:end])\n",
    "        if isinstance(result, float) or isinstance(result, int):\n",
    "            return result\n",
    "        else:\n",
    "            return result.item()\n",
    "\n",
    "    def extract(self, signal, boundary):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            signal: 1D numpy array，单变量时间序列\n",
    "            boundary: int，分割点\n",
    "        输出：\n",
    "            result: dict，格式为 {func_name: {'left': value, 'right': value}}\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        result = {}\n",
    "        for name, func in self.func_classes.items():\n",
    "            try:\n",
    "                left = self.calculate(func, 0, boundary)\n",
    "                right = self.calculate(func, boundary, n)\n",
    "                whole = self.calculate(func, 0, n)\n",
    "                # diff = right - left\n",
    "                # ratio = right / (left + 1e-6)\n",
    "            except Exception:\n",
    "                left = None\n",
    "                right = None\n",
    "                whole = None\n",
    "                # diff = None\n",
    "                # ratio = None\n",
    "            # Move to _add_diff_ratio_feats, 'diff': diff, 'ratio': ratio\n",
    "            result[name] = {'left': left, 'right': right, 'whole': whole}   \n",
    "        return result\n",
    "\n",
    "@register_feature(func_id=\"1\")\n",
    "def distribution_stats_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"统计量的分段值、Diff值、Ratio值\"\"\"\n",
    "    value = u['value'].values.astype(np.float32)\n",
    "    period = u['period'].values.astype(np.float32)\n",
    "    boundary = np.where(np.diff(period) != 0)[0].item()\n",
    "    feats = {}\n",
    "\n",
    "    extractor = STATSFeatureExtractor()\n",
    "    extractor.fit(value)\n",
    "    features = extractor.extract(value, boundary)\n",
    "\n",
    "    feats = {}\n",
    "    for k, v in features.items():\n",
    "        for seg, value in v.items():\n",
    "            feats[f'stats_{k}_{seg}'] = value\n",
    "        _add_diff_ratio_feats(feats, f'stats_{k}', v['left'], v['right'])\n",
    "        _add_contribution_ratio_feats(feats, f'stats_{k}', v['left'], v['right'], v['whole'])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "    \n",
    "# --- 2. 假设检验统计量特征 ---\n",
    "@register_feature(func_id=\"2\")\n",
    "def test_stats_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    \"\"\"假设检验统计量\"\"\"\n",
    "    # # KS检验\n",
    "    # ks_stat, ks_pvalue = scipy.stats.ks_2samp(s1, s2)\n",
    "    # feats['ks_stat'] = ks_stat\n",
    "    # feats['ks_pvalue'] = -ks_pvalue\n",
    "\n",
    "    # T检验\n",
    "    # ttest_stat, ttest_pvalue = scipy.stats.ttest_ind(s1, s2, equal_var=False)\n",
    "    # feats['ttest_pvalue'] = -ttest_pvalue if not np.isnan(ttest_pvalue) else 1\n",
    "\n",
    "    # AD检验\n",
    "    ad_stat, _, ad_pvalue = scipy.stats.anderson_ksamp([s1.to_numpy(), s2.to_numpy()])\n",
    "    feats['ad_stat'] = ad_stat\n",
    "    feats['ad_pvalue'] = -ad_pvalue\n",
    "\n",
    "    # # Mann-Whitney U检验 (非参数，不假设分布)\n",
    "    # mw_stat, mw_pvalue = scipy.stats.mannwhitneyu(s1, s2, alternative='two-sided')\n",
    "    # feats['mannwhitney_stat'] = mw_stat if not np.isnan(mw_stat) else 0\n",
    "    # feats['mannwhitney_pvalue'] = -mw_pvalue if not np.isnan(mw_pvalue) else 1\n",
    "    \n",
    "    # Wilcoxon秩和检验\n",
    "    # w_stat, w_pvalue = scipy.stats.ranksums(s1, s2)\n",
    "    # feats['wilcoxon_stat'] = w_stat if not np.isnan(w_stat) else 0\n",
    "    # feats['wilcoxon_pvalue'] = -w_pvalue if not np.isnan(w_pvalue) else 1\n",
    "\n",
    "    # Levene检验\n",
    "    levene_stat, levene_pvalue = scipy.stats.levene(s1, s2)\n",
    "    feats['levene_stat'] = levene_stat if not np.isnan(levene_stat) else 0\n",
    "    feats['levene_pvalue'] = -levene_pvalue if not np.isnan(levene_pvalue) else 1\n",
    "    \n",
    "    # Bartlett检验\n",
    "    bartlett_stat, bartlett_pvalue = scipy.stats.bartlett(s1, s2)\n",
    "    feats['bartlett_stat'] = bartlett_stat if not np.isnan(bartlett_stat) else 0\n",
    "    feats['bartlett_pvalue'] = -bartlett_pvalue if not np.isnan(bartlett_pvalue) else 1\n",
    "    \n",
    "    # \"\"\"分段假设检验的分段值、Diff值、Ratio值\"\"\"\n",
    "    # # Shapiro-Wilk检验\n",
    "    # sw1_stat, sw1_pvalue, sw2_stat, sw2_pvalue, sw_whole_stat, sw_whole_pvalue = (np.nan,)*6\n",
    "    # try:\n",
    "    #     sw1_stat, sw1_pvalue = scipy.stats.shapiro(s1)\n",
    "    #     sw2_stat, sw2_pvalue = scipy.stats.shapiro(s2)\n",
    "    #     sw_whole_stat, sw_whole_pvalue = scipy.stats.shapiro(s_whole)\n",
    "    # except:\n",
    "    #     pass\n",
    "    # feats['shapiro_pvalue_left'] = sw1_pvalue\n",
    "    # feats['shapiro_pvalue_right'] = sw2_pvalue\n",
    "    # feats['shapiro_pvalue_whole'] = sw_whole_pvalue\n",
    "    # _add_diff_ratio_feats(feats, 'shapiro_pvalue', sw1_pvalue, sw2_pvalue)\n",
    "    # _add_contribution_ratio_feats(feats, 'shapiro_pvalue', sw1_pvalue, sw2_pvalue, sw_whole_pvalue)\n",
    "\n",
    "    # Jarque-Bera检验差异\n",
    "    jb1_stat, jb1_pvalue, jb2_stat, jb2_pvalue, jb_whole_stat, jb_whole_pvalue = (np.nan,)*6\n",
    "    try:\n",
    "        jb1_stat, jb1_pvalue = scipy.stats.jarque_bera(s1)\n",
    "        jb2_stat, jb2_pvalue = scipy.stats.jarque_bera(s2)\n",
    "        jb_whole_stat, jb_whole_pvalue = scipy.stats.jarque_bera(s_whole)\n",
    "    except:\n",
    "        pass\n",
    "    feats['jb_pvalue_left'] = jb1_pvalue\n",
    "    feats['jb_pvalue_right'] = jb2_pvalue\n",
    "    feats['jb_pvalue_whole'] = jb_whole_pvalue\n",
    "    # _add_diff_ratio_feats(feats, 'jb_pvalue', jb1_pvalue, jb2_pvalue)\n",
    "    _add_contribution_ratio_feats(feats, 'jb_pvalue', jb1_pvalue, jb2_pvalue, jb_whole_pvalue)\n",
    "\n",
    "    # KPSS检验\n",
    "    def extract_kpss_features(s):\n",
    "        if len(s) <= 12:\n",
    "            return {'p': 0.1, 'stat': 0.0, 'lag': 0, 'crit_5pct': 0.0, 'reject_5pct': 0}\n",
    "        kpss = tsa.stattools.kpss(s, regression='c', nlags='auto')\n",
    "        stat, p, lag, crit = kpss\n",
    "        crit_5pct = crit['5%']\n",
    "        return {\n",
    "            'p': p,\n",
    "            'stat': stat,\n",
    "            'lag': lag,\n",
    "            'crit_5pct': crit_5pct,\n",
    "            'reject_5pct': int(stat > crit_5pct)  # KPSS原假设是\"平稳\"，所以 > 临界值 拒绝平稳\n",
    "        }\n",
    "    try:\n",
    "        k1 = extract_kpss_features(s1)\n",
    "        k2 = extract_kpss_features(s2)\n",
    "        k_whole = extract_kpss_features(s_whole)\n",
    "\n",
    "        feats['kpss_pvalue_left'] = k1['p']\n",
    "        feats['kpss_pvalue_right'] = k2['p']\n",
    "        feats['kpss_pvalue_whole'] = k_whole['p']\n",
    "        # _add_diff_ratio_feats(feats, 'kpss_pvalue', k1['p'], k2['p'])\n",
    "        _add_contribution_ratio_feats(feats, 'kpss_pvalue', k1['p'], k2['p'], k_whole['p'])\n",
    "\n",
    "        # feats['kpss_stat_left'] = k1['stat']\n",
    "        # feats['kpss_stat_right'] = k2['stat']\n",
    "        # feats['kpss_stat_whole'] = k_whole['stat']\n",
    "        # _add_diff_ratio_feats(feats, 'kpss_stat', k1['stat'], k2['stat'])\n",
    "        # _add_contribution_ratio_feats(feats, 'kpss_stat', k1['stat'], k2['stat'], k_whole['stat'])\n",
    "    except:\n",
    "        feats.update({\n",
    "            'kpss_pvalue_left': 1, 'kpss_pvalue_right': 1, 'kpss_pvalue_whole': 1, 'kpss_pvalue_diff': 0, 'kpss_pvalue_ratio': 0,\n",
    "            'kpss_stat_left': 0, 'kpss_stat_right': 0, 'kpss_stat_whole': 0, 'kpss_stat_diff': 0, 'kpss_stat_ratio': 0\n",
    "        })\n",
    "\n",
    "    # 平稳性检验 (ADF)\n",
    "    # def extract_adf_features(s):\n",
    "    #     if len(s) <= 12:\n",
    "    #         return {'p': 1.0, 'stat': 0.0, 'lag': 0, 'ic': 0.0, 'crit_5pct': 0.0, 'reject_5pct': 0}\n",
    "    #     adf = tsa.stattools.adfuller(s, autolag='AIC')\n",
    "    #     stat, p, lag, _, crit, ic = adf\n",
    "    #     crit_5pct = crit['5%']\n",
    "    #     return {\n",
    "    #         'p': p,\n",
    "    #         'stat': stat,\n",
    "    #         'lag': lag,\n",
    "    #         'ic': ic,\n",
    "    #         'crit_5pct': crit_5pct,\n",
    "    #         'reject_5pct': int(stat < crit_5pct)\n",
    "    #     }\n",
    "    # try:\n",
    "        # f1 = extract_adf_features(s1)\n",
    "        # f2 = extract_adf_features(s2)\n",
    "        # f_whole = extract_adf_features(s_whole)\n",
    "\n",
    "        # feats['adf_pvalue_left'] = f1['p']\n",
    "        # feats['adf_pvalue_right'] = f2['p']\n",
    "        # feats['adf_pvalue_whole'] = f_whole['p']\n",
    "        # _add_diff_ratio_feats(feats, 'adf_pvalue', f1['p'], f2['p'])\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_pvalue', f1['p'], f2['p'], f_whole['p'])\n",
    "\n",
    "        # feats['adf_stat_left'] = f1['stat']\n",
    "        # feats['adf_stat_right'] = f2['stat']\n",
    "        # feats['adf_stat_whole'] = f_whole['stat']\n",
    "        # _add_diff_ratio_feats(feats, 'adf_stat', f1['stat'], f2['stat'])\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_stat', f1['stat'], f2['stat'], f_whole['stat'])\n",
    "\n",
    "        # feats['adf_icbest_left'] = f1['ic']\n",
    "        # feats['adf_icbest_right'] = f2['ic']\n",
    "        # feats['adf_icbest_whole'] = f_whole['ic']\n",
    "        # _add_diff_ratio_feats(feats, 'adf_icbest', f1['ic'], f2['ic'])\n",
    "        # _add_contribution_ratio_feats(feats, 'adf_icbest', f1['ic'], f2['ic'], f_whole['ic'])\n",
    "    # except:\n",
    "    #     feats.update({\n",
    "    #         'adf_pvalue_left': 1, 'adf_pvalue_right': 1, 'adf_pvalue_whole': 1, 'adf_pvalue_diff': 0, 'adf_pvalue_ratio': 0,\n",
    "    #         'adf_stat_left': 0, 'adf_stat_right': 0, 'adf_stat_whole': 0, 'adf_stat_diff': 0, 'adf_stat_ratio': 0,\n",
    "    #         'adf_icbest_left': 0, 'adf_icbest_right': 0, 'adf_icbest_whole': 0, 'adf_icbest_diff': 0, 'adf_icbest_ratio': 0\n",
    "    #     })\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 3. 趋势特征 ---\n",
    "@register_feature(func_id=\"3\")\n",
    "def trend_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    def analyze_trend(series, seg):\n",
    "        \"\"\"分析时间序列的趋势特征\"\"\"\n",
    "        trend_feats = {}\n",
    "        x = np.arange(len(series))\n",
    "        \n",
    "        try:\n",
    "            # 1. 线性趋势分析 (使用scipy.stats.linregress)\n",
    "            slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, series)\n",
    "            # trend_feats[f'linear_trend_slope_{seg}'] = slope\n",
    "            # trend_feats[f'linear_trend_intercept_{seg}'] = intercept\n",
    "            # trend_feats[f'linear_trend_r_value_{seg}'] = r_value\n",
    "            # trend_feats[f'linear_trend_r2_{seg}'] = r_value ** 2\n",
    "            # trend_feats[f'linear_trend_pvalue_{seg}'] = p_value\n",
    "            trend_feats[f'linear_trend_std_err_{seg}'] = std_err\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in linear trend analysis for {seg}: {e}\")\n",
    "\n",
    "            trend_feats[f'linear_trend_slope_{seg}'] = 0\n",
    "            trend_feats[f'linear_trend_intercept_{seg}'] = 0\n",
    "            trend_feats[f'linear_trend_r_value_{seg}'] = 0\n",
    "            trend_feats[f'linear_trend_r2_{seg}'] = 0\n",
    "            trend_feats[f'linear_trend_pvalue_{seg}'] = 1\n",
    "            trend_feats[f'linear_trend_std_err_{seg}'] = 0\n",
    "\n",
    "        # try:\n",
    "        #     # 2. 去趋势分析 (detrended features)\n",
    "        #     linear_trend = slope * x + intercept\n",
    "        #     detrended = series - linear_trend\n",
    "        #     trend_feats[f'detrend_mean_{seg}'] = np.mean(detrended)\n",
    "        #     trend_feats[f'detrend_volatility_{seg}'] = np.std(detrended)\n",
    "        #     trend_feats[f'detrend_volatility_normalized_{seg}'] = np.std(detrended) / (np.abs(np.mean(series)) + 1e-6)\n",
    "        #     trend_feats[f'detrend_max_deviation_{seg}'] = np.max(np.abs(detrended))\n",
    "        # except Exception as e:\n",
    "        #     logger.error(f\"Error in detrending analysis for {seg}: {e}\")\n",
    "\n",
    "        #     trend_feats[f'detrend_mean_{seg}'] = 0\n",
    "        #     trend_feats[f'detrend_volatility_{seg}'] = 0\n",
    "        #     trend_feats[f'detrend_volatility_normalized_{seg}'] = 0\n",
    "        #     trend_feats[f'detrend_max_deviation_{seg}'] = 0\n",
    "\n",
    "        # try:\n",
    "        #     # 3. 趋势变化率\n",
    "        #     trend_feats[f'trend_change_rate_{seg}'] = slope / (np.mean(np.abs(series)) + 1e-6)  # 相对变化率\n",
    "        #     trend_feats[f'trend_normalized_slope_{seg}'] = slope / (np.std(series) + 1e-6)  # 标准化斜率\n",
    "        # except Exception as e:\n",
    "        #     logger.error(f\"Error in trend change rate analysis for {seg}: {e}\")\n",
    "\n",
    "        #     trend_feats[f'trend_change_rate_{seg}'] = 0\n",
    "        #     trend_feats[f'trend_normalized_slope_{seg}'] = 0\n",
    "        \n",
    "        return trend_feats\n",
    "    \n",
    "    feats.update(analyze_trend(s1, 'left'))\n",
    "    # feats.update(analyze_trend(s2, 'right'))\n",
    "    # feats.update(analyze_trend(s_whole, 'whole'))\n",
    "    # _add_diff_ratio_feats(feats, 'linear_trend_slope', feats['linear_trend_slope_left'] if 'linear_trend_slope_left' in feats else 0, feats['linear_trend_slope_right'] if 'linear_trend_slope_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'linear_trend_r2', feats['linear_trend_r2_left'] if 'linear_trend_r2_left' in feats else 0, feats['linear_trend_r2_right'] if 'linear_trend_r2_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'linear_trend_pvalue', feats['linear_trend_pvalue_left'] if 'linear_trend_pvalue_left' in feats else 0, feats['linear_trend_pvalue_right'] if 'linear_trend_pvalue_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'detrend_mean', feats['detrend_mean_left'] if 'detrend_mean_left' in feats else 0, feats['detrend_mean_right'] if 'detrend_mean_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'detrend_volatility_normalized', feats['detrend_volatility_normalized_left'] if 'detrend_volatility_normalized_left' in feats else 0, feats['detrend_volatility_normalized_right'] if 'detrend_volatility_normalized_right' in feats else 0)\n",
    "    # _add_diff_ratio_feats(feats, 'detrend_max_deviation', feats['detrend_max_deviation_left'] if 'detrend_max_deviation_left' in feats else 0, feats['detrend_max_deviation_right'] if 'detrend_max_deviation_right' in feats else 0)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 4. 振荡特征 ---\n",
    "@register_feature(func_id=\"4\")\n",
    "def oscillation_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0].reset_index(drop=True)\n",
    "    s2 = u['value'][u['period'] == 1].reset_index(drop=True)\n",
    "    s_whole = u['value'].reset_index(drop=True)\n",
    "    feats = {}\n",
    "\n",
    "    # def count_zero_crossings(series: pd.Series):\n",
    "    #     if len(series) < 2: return 0\n",
    "    #     centered_series = series - series.mean()\n",
    "    #     if centered_series.eq(0).all(): return 0\n",
    "    #     return np.sum(np.diff(np.sign(centered_series)) != 0)\n",
    "    # zc1, zc2, zc_whole = count_zero_crossings(s1), count_zero_crossings(s2), count_zero_crossings(s_whole)\n",
    "    # feats['zero_cross_left'] = zc1\n",
    "    # feats['zero_cross_right'] = zc2\n",
    "    # feats['zero_cross_whole'] = zc_whole\n",
    "    # _add_diff_ratio_feats(feats, 'zero_cross', zc1, zc2)\n",
    "    # _add_contribution_ratio_feats(feats, 'zero_cross', zc1, zc2, zc_whole)\n",
    "    \n",
    "    def autocorr_lag1(s):\n",
    "        if len(s) < 2: return 0.0\n",
    "        ac = s.autocorr(lag=1)\n",
    "        return ac if not np.isnan(ac) else 0.0\n",
    "    ac1, ac2, ac_whole = autocorr_lag1(s1), autocorr_lag1(s2), autocorr_lag1(s_whole)\n",
    "    feats['autocorr_lag1_left'] = ac1\n",
    "    feats['autocorr_lag1_right'] = ac2\n",
    "    # feats['autocorr_lag1_whole'] = ac_whole\n",
    "    _add_diff_ratio_feats(feats, 'autocorr_lag1', ac1, ac2)\n",
    "    # _add_contribution_ratio_feats(feats, 'autocorr_lag1', ac1, ac2, ac_whole)\n",
    "\n",
    "    # var1, var2, var_whole = s1.diff().var(), s2.diff().var(), s_whole.diff().var()\n",
    "    # feats['diff_var_left'] = var1\n",
    "    # feats['diff_var_right'] = var2\n",
    "    # feats['diff_var_whole'] = var_whole\n",
    "    # # _add_diff_ratio_feats(feats, 'diff_var', var1, var2)\n",
    "    # _add_contribution_ratio_feats(feats, 'diff_var', var1, var2, var_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 5. 频域特征 ---\n",
    "@register_feature(func_id=\"5\")\n",
    "def cyclic_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "\n",
    "    # def get_fft_props(series):\n",
    "    #     if len(series) < 2: return 0.0, 0.0\n",
    "        \n",
    "    #     N = len(series)\n",
    "    #     yf = np.fft.fft(series.values)\n",
    "    #     power = np.abs(yf[1:N//2])**2\n",
    "    #     xf = np.fft.fftfreq(N, 1)[1:N//2]\n",
    "        \n",
    "    #     if len(power) == 0: return 0.0, 0.0\n",
    "            \n",
    "    #     dominant_freq = xf[np.argmax(power)]\n",
    "    #     max_power = np.max(power)\n",
    "    #     return dominant_freq, max_power\n",
    "\n",
    "    # freq1, power1 = get_fft_props(s1)\n",
    "    # freq2, power2 = get_fft_props(s2)\n",
    "    # freq_whole, power_whole = get_fft_props(s_whole)\n",
    "    \n",
    "    # feats['dominant_freq_left'] = freq1\n",
    "    # feats['dominant_freq_right'] = freq2\n",
    "    # feats['dominant_freq_whole'] = freq_whole\n",
    "    # _add_diff_ratio_feats(feats, 'dominant_freq', freq1, freq2)\n",
    "    # _add_contribution_ratio_feats(feats, 'dominant_freq', freq1, freq2, freq_whole)\n",
    "\n",
    "    # feats['max_power_left'] = power1\n",
    "    # feats['max_power_right'] = power2\n",
    "    # feats['max_power_whole'] = power_whole\n",
    "    # _add_diff_ratio_feats(feats, 'max_power', power1, power2)\n",
    "    # _add_contribution_ratio_feats(feats, 'max_power', power1, power2, power_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 6. 振幅特征 ---\n",
    "@register_feature(func_id=\"6\")\n",
    "def amplitude_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0]\n",
    "    s2 = u['value'][u['period'] == 1]\n",
    "    s_whole = u['value']\n",
    "    feats = {}\n",
    "    \n",
    "    # ptp1, ptp2, ptp_whole = np.ptp(s1), np.ptp(s2), np.ptp(s_whole)\n",
    "    # iqr1, iqr2, iqr_whole = scipy.stats.iqr(s1), scipy.stats.iqr(s2), scipy.stats.iqr(s_whole)\n",
    "\n",
    "    # feats['ptp_left'] = ptp1\n",
    "    # feats['ptp_right'] = ptp2\n",
    "    # feats['ptp_whole'] = ptp_whole\n",
    "    # _add_diff_ratio_feats(feats, 'ptp', ptp1, ptp2)\n",
    "    # _add_contribution_ratio_feats(feats, 'ptp', ptp1, ptp2, ptp_whole)\n",
    "\n",
    "    # feats['iqr_left'] = iqr1\n",
    "    # feats['iqr_right'] = iqr2\n",
    "    # feats['iqr_whole'] = iqr_whole\n",
    "    # _add_diff_ratio_feats(feats, 'iqr', iqr1, iqr2)\n",
    "    # _add_contribution_ratio_feats(feats, 'iqr', iqr1, iqr2, iqr_whole)\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 7. 熵信息 ---\n",
    "@register_feature(func_id=\"7\")\n",
    "def entropy_features(u: pd.DataFrame) -> dict:\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    def compute_entropy(x):\n",
    "        hist, _ = np.histogram(x, bins='auto', density=True)\n",
    "        hist = hist[hist > 0]\n",
    "        return scipy.stats.entropy(hist)\n",
    "    \n",
    "    entropy_funcs = {\n",
    "        # 'shannon_entropy': compute_entropy,\n",
    "        'perm_entropy': lambda x: antropy.perm_entropy(x, normalize=True),\n",
    "        'spectral_entropy': lambda x: antropy.spectral_entropy(x, sf=1.0, normalize=True),\n",
    "        # 'svd_entropy': lambda x: antropy.svd_entropy(x, normalize=True),\n",
    "        # 'approx_entropy': antropy.app_entropy,\n",
    "        'sample_entropy': antropy.sample_entropy,\n",
    "        # 'petrosian_fd': antropy.petrosian_fd,\n",
    "        # 'katz_fd': antropy.katz_fd,\n",
    "        # 'higuchi_fd': antropy.higuchi_fd,\n",
    "        'detrended_fluctuation': antropy.detrended_fluctuation,\n",
    "    }\n",
    "\n",
    "    for name, func in entropy_funcs.items():\n",
    "        try:\n",
    "            v1, v2, v_whole = func(s1), func(s2), func(s_whole)\n",
    "            feats[f'{name}_left'] = v1\n",
    "            feats[f'{name}_right'] = v2\n",
    "            feats[f'{name}_whole'] = v_whole\n",
    "            _add_diff_ratio_feats(feats, name, v1, v2)\n",
    "            _add_contribution_ratio_feats(feats, name, v1, v2, v_whole)\n",
    "        except Exception:\n",
    "            feats.update({f'{name}_left': 0, f'{name}_right': 0, f'{name}_whole': 0, f'{name}_diff': 0, f'{name}_ratio': 0})\n",
    "\n",
    "    try:\n",
    "        m1, c1 = antropy.hjorth_params(s1)\n",
    "        m2, c2 = antropy.hjorth_params(s2)\n",
    "        m_whole, c_whole = antropy.hjorth_params(s_whole)\n",
    "        feats.update({\n",
    "            # 'hjorth_mobility_left': m1, 'hjorth_mobility_right': m2, 'hjorth_mobility_whole': m_whole,\n",
    "            'hjorth_complexity_left': c1, 'hjorth_complexity_right': c2, 'hjorth_complexity_whole': c_whole,\n",
    "        })\n",
    "        # _add_diff_ratio_feats(feats, 'hjorth_mobility', m1, m2)\n",
    "        # _add_contribution_ratio_feats(feats, 'hjorth_mobility', m1, m2, m_whole)\n",
    "        # _add_diff_ratio_feats(feats, 'hjorth_complexity', c1, c2)\n",
    "        _add_contribution_ratio_feats(feats, 'hjorth_complexity', c1, c2, c_whole)\n",
    "    except Exception:\n",
    "        feats.update({'hjorth_mobility_left':0, 'hjorth_mobility_right':0, 'hjorth_mobility_whole':0, 'hjorth_mobility_diff':0, 'hjorth_mobility_ratio':0,\n",
    "                     'hjorth_complexity_left':0, 'hjorth_complexity_right':0, 'hjorth_complexity_whole':0, 'hjorth_complexity_diff':0, 'hjorth_complexity_ratio':0})\n",
    "\n",
    "\n",
    "    # def series_to_binary_str(x, method='median'):\n",
    "    #     if method == 'median':\n",
    "    #         threshold = np.median(x)\n",
    "    #         return ''.join(['1' if val > threshold else '0' for val in x])\n",
    "    #     return None\n",
    "    # try:\n",
    "    #     bin_str1 = series_to_binary_str(s1)\n",
    "    #     bin_str2 = series_to_binary_str(s2)\n",
    "    #     bin_str_whole = series_to_binary_str(s_whole)\n",
    "    #     lz1, lz2, lz_whole = antropy.lziv_complexity(bin_str1, normalize=True), antropy.lziv_complexity(bin_str2, normalize=True), antropy.lziv_complexity(bin_str_whole, normalize=True)\n",
    "    #     feats.update({\n",
    "    #         'lziv_complexity_left': lz1, 'lziv_complexity_right': lz2, 'lziv_complexity_whole': lz_whole,\n",
    "    #     })\n",
    "    #     _add_diff_ratio_feats(feats, 'lziv_complexity', lz1, lz2)\n",
    "        # _add_contribution_ratio_feats(feats, 'lziv_complexity', lz1, lz2, lz_whole)\n",
    "    # except Exception:\n",
    "    #     feats.update({'lziv_complexity_left':0, 'lziv_complexity_right':0, 'lziv_complexity_whole':0, 'lziv_complexity_diff':0, 'lziv_complexity_ratio':0})\n",
    "\n",
    "\n",
    "    # def estimate_cond_entropy(x, lag=1):\n",
    "    #     x = x - np.mean(x)\n",
    "    #     x_lag = x[:-lag]\n",
    "    #     x_now = x[lag:]\n",
    "    #     bins = 10\n",
    "    #     joint_hist, _, _ = np.histogram2d(x_lag, x_now, bins=bins, density=True)\n",
    "    #     joint_hist = joint_hist[joint_hist > 0]\n",
    "    #     H_xy = -np.sum(joint_hist * np.log(joint_hist))\n",
    "    #     H_x = -np.sum(np.histogram(x_lag, bins=bins, density=True)[0] * \\\n",
    "    #                   np.log(np.histogram(x_lag, bins=bins, density=True)[0] + 1e-12))\n",
    "    #     return H_xy - H_x\n",
    "    # try:\n",
    "    #     ce1, ce2, ce_whole = estimate_cond_entropy(s1), estimate_cond_entropy(s2), estimate_cond_entropy(s_whole)\n",
    "    #     feats.update({\n",
    "    #         'cond_entropy_left': ce1, 'cond_entropy_right': ce2, 'cond_entropy_whole': ce_whole,\n",
    "    #     })\n",
    "    #     _add_diff_ratio_feats(feats, 'cond_entropy', ce1, ce2)\n",
    "    #     _add_contribution_ratio_feats(feats, 'cond_entropy', ce1, ce2, ce_whole)\n",
    "    # except Exception:\n",
    "    #     feats.update({'cond_entropy_left':0, 'cond_entropy_right':0, 'cond_entropy_whole':0, 'cond_entropy_diff':0, 'cond_entropy_ratio':0})\n",
    "    \n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 8. tsfresh --- \n",
    "@register_feature(func_id=\"8\")\n",
    "def tsfresh_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"基于tsfresh的特征工程\"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "\n",
    "    funcs = {\n",
    "        tsfresh_fe.ratio_value_number_to_time_series_length: None,\n",
    "        # tsfresh_fe.sum_of_reoccurring_data_points: None,\n",
    "        tsfresh_fe.percentage_of_reoccurring_values_to_all_values: None,\n",
    "        tsfresh_fe.percentage_of_reoccurring_datapoints_to_all_datapoints: None,\n",
    "        tsfresh_fe.last_location_of_maximum: None,\n",
    "        # tsfresh_fe.first_location_of_maximum: None,\n",
    "        # tsfresh_fe.has_duplicate: None,\n",
    "        tsfresh_fe.benford_correlation: None,\n",
    "        tsfresh_fe.ratio_beyond_r_sigma: [\n",
    "            # 6, 3, \n",
    "            1.5, 1, \n",
    "            # 0.5\n",
    "        ],\n",
    "        tsfresh_fe.quantile: [\n",
    "            0.6, 0.4, \n",
    "            # 0.1\n",
    "        ],\n",
    "        tsfresh_fe.count_above: [0],\n",
    "        tsfresh_fe.number_peaks: [\n",
    "            # 25, \n",
    "            50\n",
    "        ],\n",
    "        # tsfresh_fe.partial_autocorrelation: [\n",
    "        #     {\"lag\": 2}, \n",
    "        #     {\"lag\": 4},\n",
    "            # {\"lag\": 6}\n",
    "        # ],\n",
    "        tsfresh_fe.index_mass_quantile: [\n",
    "            {\"q\": 0.1}, \n",
    "            # {\"q\": 0.6}, \n",
    "            {\"q\": 0.8}\n",
    "        ],\n",
    "        tsfresh_fe.ar_coefficient: [\n",
    "            # {\"coeff\": 0, \"k\": 10}, \n",
    "            # {\"coeff\": 2, \"k\": 10}, \n",
    "            {\"coeff\": 8, \"k\": 10}\n",
    "        ],\n",
    "        tsfresh_fe.linear_trend: [\n",
    "        #     {\"attr\": \"slope\"}, \n",
    "        #     {\"attr\": \"rvalue\"}, \n",
    "        #     {\"attr\": \"pvalue\"}, \n",
    "            {\"attr\": \"intercept\"}\n",
    "        ],\n",
    "        tsfresh_fe.fft_coefficient: [\n",
    "            # {\"coeff\": 3, \"attr\": \"imag\"}, \n",
    "            {\"coeff\": 2, \"attr\": \"imag\"}, \n",
    "            # {\"coeff\": 1, \"attr\": \"imag\"}\n",
    "        ],\n",
    "        tsfresh_fe.energy_ratio_by_chunks: [\n",
    "            {\"num_segments\": 10, \"segment_focus\": 9},\n",
    "            # {\"num_segments\": 20, \"segment_focus\": 16},\n",
    "        ],\n",
    "        # tsfresh_fe.friedrich_coefficients: [\n",
    "        #     {\"m\": 3, \"r\": 30, \"coeff\": 2}, \n",
    "        #     {\"m\": 3, \"r\": 30, \"coeff\": 3}\n",
    "        # ],\n",
    "        tsfresh_fe.change_quantiles: [\n",
    "            # {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 1.0, \"ql\": 0.4},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 1.0, \"ql\": 0.2},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.6},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.8, \"ql\": 0.2},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.6, \"ql\": 0.4},\n",
    "            {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.6, \"ql\": 0.2},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": True,  \"qh\": 0.4, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 1.0, \"ql\": 0.4},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 1.0, \"ql\": 0.2},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.4},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.2},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.8, \"ql\": 0.0},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.6, \"ql\": 0.4},\n",
    "            # {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.6, \"ql\": 0.2},\n",
    "            {\"f_agg\": \"var\", \"isabs\": False, \"qh\": 0.4, \"ql\": 0.2},\n",
    "            # {\"f_agg\": \"mean\",\"isabs\": True,  \"qh\": 1.0, \"ql\": 0.4},\n",
    "            # {\"f_agg\": \"mean\",\"isabs\": True,  \"qh\": 0.6, \"ql\": 0.4},\n",
    "        ],\n",
    "        tsfresh_fe.agg_linear_trend: [\n",
    "            # {\"attr\": \"slope\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"slope\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"slope\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"rvalue\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"rvalue\", \"chunk_len\": 50, \"f_agg\": \"max\"},\n",
    "            # {\"attr\": \"rvalue\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"rvalue\", \"chunk_len\": 5,  \"f_agg\": \"max\"},\n",
    "            {\"attr\": \"rvalue\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"rvalue\", \"chunk_len\": 10, \"f_agg\": \"max\"},\n",
    "            # {\"attr\": \"intercept\", \"chunk_len\": 50, \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"intercept\", \"chunk_len\": 50, \"f_agg\": \"max\"},\n",
    "            # {\"attr\": \"intercept\", \"chunk_len\": 5,  \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"intercept\", \"chunk_len\": 5,  \"f_agg\": \"max\"},\n",
    "            # {\"attr\": \"intercept\", \"chunk_len\": 10, \"f_agg\": \"mean\"},\n",
    "            # {\"attr\": \"intercept\", \"chunk_len\": 10, \"f_agg\": \"max\"},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    def param_to_str(param):\n",
    "        if isinstance(param, dict):\n",
    "            return '_'.join([f\"{k}_{v}\" for k, v in param.items()])\n",
    "        else:\n",
    "            return str(param)\n",
    "\n",
    "    def calculate_stats_for_feature(func, param=None):\n",
    "        results = {}\n",
    "        base_name = func.__name__\n",
    "        if param is not None:\n",
    "            base_name += f\"_{param_to_str(param)}\"\n",
    "\n",
    "        try:\n",
    "            # Prepare arguments for each segment\n",
    "            args_s1 = [s1]\n",
    "            args_s2 = [s2]\n",
    "            args_s_whole = [s_whole]\n",
    "            is_combiner = False\n",
    "\n",
    "            if param is None: # Simple function, no params\n",
    "                pass\n",
    "            elif isinstance(param, dict):\n",
    "                # Check if it's a combiner function or a function with kwargs\n",
    "                sig = inspect.signature(func)\n",
    "                if 'param' in sig.parameters: # Combiner function\n",
    "                    is_combiner = True\n",
    "                    args_s1.append([param])\n",
    "                    args_s2.append([param])\n",
    "                    args_s_whole.append([param])\n",
    "                else: # Function with kwargs\n",
    "                    args_s1.append(param)\n",
    "                    args_s2.append(param)\n",
    "                    args_s_whole.append(param)\n",
    "            else: # Simple function with a single parameter\n",
    "                args_s1.append(param)\n",
    "                args_s2.append(param)\n",
    "                args_s_whole.append(param)\n",
    "\n",
    "            # Execute function for each segment\n",
    "            if is_combiner:\n",
    "                v1_dict = {k: v for k, v in func(*args_s1)}\n",
    "                v2_dict = {k: v for k, v in func(*args_s2)}\n",
    "                v_whole_dict = {k: v for k, v in func(*args_s_whole)}\n",
    "                \n",
    "                for key in v1_dict:\n",
    "                    v1, v2, v_whole = v1_dict[key], v2_dict[key], v_whole_dict[key]\n",
    "                    feat_name_base = f\"{func.__name__}_{key}\"\n",
    "                    results[f'{feat_name_base}_left'] = v1\n",
    "                    results[f'{feat_name_base}_right'] = v2\n",
    "                    results[f'{feat_name_base}_whole'] = v_whole\n",
    "                    _add_diff_ratio_feats(feats, feat_name_base, v1, v2)\n",
    "                    _add_contribution_ratio_feats(results, feat_name_base, v1, v2, v_whole)\n",
    "                return results\n",
    "\n",
    "            else:\n",
    "                if isinstance(param, dict) and not is_combiner:\n",
    "                    v1, v2, v_whole = func(args_s1[0], **args_s1[1]), func(args_s2[0], **args_s2[1]), func(args_s_whole[0], **args_s_whole[1])\n",
    "                else:\n",
    "                    v1, v2, v_whole = func(*args_s1), func(*args_s2), func(*args_s_whole)\n",
    "\n",
    "                results[f'{base_name}_left'] = v1\n",
    "                results[f'{base_name}_right'] = v2\n",
    "                results[f'{base_name}_whole'] = v_whole\n",
    "                _add_diff_ratio_feats(feats, base_name, v1, v2)\n",
    "                _add_contribution_ratio_feats(results, base_name, v1, v2, v_whole)\n",
    "        \n",
    "        except Exception:\n",
    "            # For combiner functions, need to know keys to create nulls\n",
    "            if 'param' in locals() and inspect.isfunction(func) and 'param' in inspect.signature(func).parameters:\n",
    "                 # It's a combiner, but we can't get keys without running it. Skip for now on error.\n",
    "                 pass\n",
    "            else:\n",
    "                results[f'{base_name}_left'] = np.nan\n",
    "                results[f'{base_name}_right'] = np.nan\n",
    "                results[f'{base_name}_whole'] = np.nan\n",
    "                results[f'{base_name}_diff'] = np.nan\n",
    "                results[f'{base_name}_ratio'] = np.nan\n",
    "                \n",
    "        return results\n",
    "\n",
    "\n",
    "    for func, params in funcs.items():\n",
    "        if params is None:\n",
    "            feats.update(calculate_stats_for_feature(func))\n",
    "        else:\n",
    "            for param in params:\n",
    "                feats.update(calculate_stats_for_feature(func, param))\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 9. 时间序列建模 ---\n",
    "@register_feature(func_id=\"9\")\n",
    "def ar_model_features(u: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    基于AR模型派生特征。\n",
    "    1. 在 period 0 上训练模型，预测 period 1，计算残差统计量。\n",
    "    2. 在 period 1 上训练模型，预测 period 0，计算残差统计量。\n",
    "    3. 分别在 period 0 和 1 上训练模型，比较模型参数、残差和信息准则(AIC/BIC)。\n",
    "    \"\"\"\n",
    "    s1 = u['value'][u['period'] == 0].to_numpy()\n",
    "    s2 = u['value'][u['period'] == 1].to_numpy()\n",
    "    s_whole = u['value'].to_numpy()\n",
    "    feats = {}\n",
    "    lags = 5 # 固定阶数以保证可比性\n",
    "\n",
    "    # # --- 特征组1: 用 s1 训练，预测 s2 ---\n",
    "    # if len(s1) > lags and len(s2) > 0:\n",
    "    #     try:\n",
    "    #         model1_fit = AutoReg(s1, lags=lags).fit()\n",
    "    #         predictions = model1_fit.predict(start=len(s1), end=len(s1) + len(s2) - 1, dynamic=True)\n",
    "    #         residuals = s2 - predictions\n",
    "    #         feats['ar_residuals_s2_pred_mean'] = np.mean(residuals)\n",
    "    #         feats['ar_residuals_s2_pred_std'] = np.std(residuals)\n",
    "    #         feats['ar_residuals_s2_pred_skew'] = pd.Series(residuals).skew()\n",
    "    #         feats['ar_residuals_s2_pred_kurt'] = pd.Series(residuals).kurt()\n",
    "    #     except Exception:\n",
    "    #         # 宽泛地捕获异常，防止因数值问题中断\n",
    "    #         feats.update({'ar_residuals_s2_pred_mean': 0, 'ar_residuals_s2_pred_std': 0, 'ar_residuals_s2_pred_skew': 0, 'ar_residuals_s2_pred_kurt': 0})\n",
    "    # else:\n",
    "    #     feats.update({'ar_residuals_s2_pred_mean': 0, 'ar_residuals_s2_pred_std': 0, 'ar_residuals_s2_pred_skew': 0, 'ar_residuals_s2_pred_kurt': 0})\n",
    "\n",
    "    # # --- 特征组2: 用 s2 训练，预测 s1 ---\n",
    "    # if len(s2) > lags and len(s1) > 0:\n",
    "    #     try:\n",
    "    #         model2_fit = AutoReg(s2, lags=lags).fit()\n",
    "    #         predictions_on_s1 = model2_fit.predict(start=len(s2), end=len(s2) + len(s1) - 1, dynamic=True)\n",
    "    #         residuals_s1_pred = s1 - predictions_on_s1\n",
    "    #         feats['ar_residuals_s1_pred_mean'] = np.mean(residuals_s1_pred)\n",
    "    #         feats['ar_residuals_s1_pred_std'] = np.std(residuals_s1_pred)\n",
    "    #         feats['ar_residuals_s1_pred_skew'] = pd.Series(residuals_s1_pred).skew()\n",
    "    #         feats['ar_residuals_s1_pred_kurt'] = pd.Series(residuals_s1_pred).kurt()\n",
    "    #     except Exception:\n",
    "    #         feats.update({'ar_residuals_s1_pred_mean': 0, 'ar_residuals_s1_pred_std': 0, 'ar_residuals_s1_pred_skew': 0, 'ar_residuals_s1_pred_kurt': 0})\n",
    "    # else:\n",
    "    #     feats.update({'ar_residuals_s1_pred_mean': 0, 'ar_residuals_s1_pred_std': 0, 'ar_residuals_s1_pred_skew': 0, 'ar_residuals_s1_pred_kurt': 0})\n",
    "\n",
    "\n",
    "    # --- 特征组3: 分别建模，比较差异 ---\n",
    "    # s1_resid_std, s1_params = np.nan, np.full(lags + 1, np.nan)\n",
    "    # # s1_aic, s1_bic = np.nan, np.nan\n",
    "    # if len(s1) > lags:\n",
    "    #     try:\n",
    "    #         fit1 = AutoReg(s1, lags=lags).fit()\n",
    "    #         # s1_resid_std = np.std(fit1.resid)\n",
    "    #         s1_params = fit1.params\n",
    "    #         # s1_aic = fit1.aic\n",
    "    #         # s1_bic = fit1.bic\n",
    "    #     except Exception:\n",
    "    #         pass\n",
    "\n",
    "    # s2_resid_std, s2_params = np.nan, np.full(lags + 1, np.nan)\n",
    "    # s2_aic, s2_bic = np.nan, np.nan\n",
    "    # if len(s2) > lags:\n",
    "    #     try:\n",
    "    #         fit2 = AutoReg(s2, lags=lags).fit()\n",
    "    #         # s2_resid_std = np.std(fit2.resid)\n",
    "    #         s2_params = fit2.params\n",
    "    #         # s2_aic = fit2.aic\n",
    "    #         # s2_bic = fit2.bic\n",
    "    #     except Exception:\n",
    "    #         pass\n",
    "\n",
    "    # swhole_resid_std, swhole_params = np.nan, np.full(lags + 1, np.nan)\n",
    "    # swhole_aic, swhole_bic = np.nan, np.nan\n",
    "    # if len(s_whole) > lags:\n",
    "    #     try:\n",
    "    #         fit_whole = AutoReg(s_whole, lags=lags).fit()\n",
    "    #         # swhole_resid_std = np.std(fit_whole.resid)\n",
    "    #         swhole_params = fit_whole.params\n",
    "    #         # swhole_aic = fit_whole.aic\n",
    "    #         # swhole_bic = fit_whole.bic\n",
    "    #     except Exception:\n",
    "    #         pass\n",
    "            \n",
    "    # feats['ar_resid_std_left'] = s1_resid_std\n",
    "    # feats['ar_resid_std_right'] = s2_resid_std\n",
    "    # feats['ar_resid_std_whole'] = swhole_resid_std\n",
    "    # _add_diff_ratio_feats(feats, 'ar_resid_std', s1_resid_std, s2_resid_std)\n",
    "    # _add_contribution_ratio_feats(feats, 'ar_resid_std', s1_resid_std, s2_resid_std, swhole_resid_std)\n",
    "    \n",
    "    # feats['ar_aic_left'] = s1_aic\n",
    "    # feats['ar_aic_right'] = s2_aic\n",
    "    # feats['ar_aic_whole'] = swhole_aic\n",
    "    # _add_diff_ratio_feats(feats, 'ar_aic', s1_aic, s2_aic)\n",
    "    # _add_contribution_ratio_feats(feats, 'ar_aic', s1_aic, s2_aic, swhole_aic)\n",
    "\n",
    "    # feats['ar_bic_left'] = s1_bic\n",
    "    # feats['ar_bic_right'] = s2_bic\n",
    "    # feats['ar_bic_whole'] = swhole_bic\n",
    "    # _add_diff_ratio_feats(feats, 'ar_bic', s1_bic, s2_bic)\n",
    "    # _add_contribution_ratio_feats(feats, 'ar_bic', s1_bic, s2_bic, swhole_bic)\n",
    "    \n",
    "    # 比较模型系数\n",
    "    # for i in range(lags + 1):\n",
    "        # feats[f'ar_param_{i}_left'] = s1_params[i]\n",
    "        # feats[f'ar_param_{i}_right'] = s2_params[i]\n",
    "        # feats[f'ar_param_{i}_whole'] = swhole_params[i]\n",
    "        # _add_diff_ratio_feats(feats, f'ar_param_{i}', s1_params[i], s2_params[i])\n",
    "        # _add_contribution_ratio_feats(feats, f'ar_param_{i}', s1_params[i], s2_params[i], swhole_params[i])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}\n",
    "\n",
    "# --- 10. 分段损失 ---\n",
    "class RPTFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        # 所有可用的cost类及其名称\n",
    "        self.cost_classes = {\n",
    "            # 'l1': rpt.costs.CostL1,               # 中位数\n",
    "            # 'l2': rpt.costs.CostL2,               # 均值\n",
    "            # 'clinear': rpt.costs.CostCLinear,     # 线性协方差\n",
    "            # 'rbf': rpt.costs.CostRbf,             # RBF核\n",
    "            # 'normal': rpt.costs.CostNormal,       # 协方差\n",
    "            # 'ar': rpt.costs.CostAR,               # 自回归\n",
    "            # 'mahalanobis': rpt.costs.CostMl,      # 马氏距离\n",
    "            # 'rank': rpt.costs.CostRank,           # 排名\n",
    "            'cosine': rpt.costs.CostCosine,       # 余弦距离\n",
    "        }\n",
    "\n",
    "    def calculate(self, cost, start, end):\n",
    "        result = cost.error(start, end)\n",
    "        if isinstance(result, (np.ndarray, list)) and np.array(result).size == 1:\n",
    "            return float(np.array(result).squeeze())\n",
    "        return result\n",
    "\n",
    "    def extract(self, signal, boundary):\n",
    "        \"\"\"\n",
    "        输入：\n",
    "            signal: 1D numpy array，单变量时间序列\n",
    "            boundary: int，分割点\n",
    "        输出：\n",
    "            result: dict，格式为 {cost_name: {'left': value, 'right': value}}\n",
    "        \"\"\"\n",
    "        signal = np.asarray(signal)\n",
    "        n = len(signal)\n",
    "        result = {}\n",
    "        for name, cls in self.cost_classes.items():\n",
    "            try:\n",
    "                if name == 'ar':\n",
    "                    cost = cls(order=4)\n",
    "                else:\n",
    "                    cost = cls()\n",
    "                cost.fit(signal)\n",
    "                left = self.calculate(cost, 0, boundary)\n",
    "                right = self.calculate(cost, boundary, n)\n",
    "                whole = self.calculate(cost, 0, n)\n",
    "                # diff = right - left if left is not None and right is not None else None\n",
    "                # ratio = right / (left + 1e-6) if left is not None and right is not None else None\n",
    "            except Exception:\n",
    "                left = None\n",
    "                right = None\n",
    "                whole = None\n",
    "                # diff = None\n",
    "                # ratio = None\n",
    "            # Move to _add_diff_ratio_feats, 'diff': diff, 'ratio': ratio\n",
    "            result[name] = {'left': left, 'right': right, 'whole': whole}\n",
    "        return result\n",
    "\n",
    "@register_feature(func_id=\"10\")\n",
    "def rupture_cost_features(u: pd.DataFrame) -> dict:\n",
    "    value = u['value'].values.astype(np.float32)\n",
    "    period = u['period'].values.astype(np.float32)\n",
    "    boundary = np.where(np.diff(period) != 0)[0].item()\n",
    "    feats = {}\n",
    "\n",
    "    extractor = RPTFeatureExtractor()\n",
    "    features = extractor.extract(value, boundary)\n",
    "\n",
    "    feats = {}\n",
    "    for k, v in features.items():\n",
    "        for seg, value in v.items():\n",
    "            feats[f'rpt_cost_{k}_{seg}'] = value\n",
    "        # _add_diff_ratio_feats(feats, f'rpt_cost_{k}', v['left'], v['right'])\n",
    "        # _add_contribution_ratio_feats(feats, f'rpt_cost_{k}', v['left'], v['right'], v['whole'])\n",
    "\n",
    "    return {k: float(v) if not np.isnan(v) else 0 for k, v in feats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3349f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 特征生成核心逻辑 ---\n",
    "def _apply_feature_func_sequential(func, X_df: pd.DataFrame, use_tqdm: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"顺序应用单个特征函数\"\"\"\n",
    "    all_ids = X_df.index.get_level_values(\"id\").unique()\n",
    "    iterator = (\n",
    "        tqdm(all_ids, desc=f\"Running {func.__name__} (sequentially)\")\n",
    "        if use_tqdm else all_ids\n",
    "    )\n",
    "    results = [\n",
    "        {**{'id': id_val}, **func(X_df.loc[id_val])}\n",
    "        for id_val in iterator\n",
    "    ]\n",
    "    return pd.DataFrame(results).set_index('id')\n",
    "\n",
    "def _apply_feature_func_parallel(func, X_df: pd.DataFrame, use_tqdm: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"并行应用单个特征函数\"\"\"\n",
    "    all_ids = X_df.index.get_level_values(\"id\").unique()\n",
    "    iterator = (\n",
    "        tqdm(all_ids, desc=f\"Running {func.__name__} (parallel)\")\n",
    "        if use_tqdm else all_ids\n",
    "    )\n",
    "    results = Parallel(n_jobs=config.N_JOBS)(\n",
    "        delayed(lambda df_id, id_val: {**{'id': id_val}, **func(df_id)})(X_df.loc[id_val], id_val)\n",
    "        for id_val in iterator\n",
    "    )\n",
    "    return pd.DataFrame(results).set_index('id')\n",
    "\n",
    "def _apply_transform_func(func, X_df: pd.DataFrame) -> List[pd.DataFrame]:\n",
    "    \"\"\"执行变换函数\"\"\"\n",
    "    return func(X_df)\n",
    "\n",
    "def apply_transformation(X_df: pd.DataFrame, transform_funcs: List[str] = None) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    应用时序变换\n",
    "    \n",
    "    Args:\n",
    "        X_df: 输入数据框\n",
    "        transform_funcs: 要应用的变换函数名称列表，如果为None则应用所有注册的变换函数\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, pd.DataFrame]: 键为模态名称，值为对应的数据框\n",
    "    \"\"\"\n",
    "    if transform_funcs is None:\n",
    "        transform_funcs = list(TRANSFORM_REGISTRY.keys())\n",
    "    \n",
    "    # 验证变换函数是否存在\n",
    "    valid_transform_funcs = []\n",
    "    for func_name in transform_funcs:\n",
    "        if func_name not in TRANSFORM_REGISTRY:\n",
    "            pass\n",
    "            # logger.warning(f\"变换函数 {func_name} 未在注册表中找到，已跳过。\")\n",
    "        else:\n",
    "            valid_transform_funcs.append(func_name)\n",
    "    \n",
    "    transform_funcs = valid_transform_funcs\n",
    "    \n",
    "    # 存储所有模态的数据框\n",
    "    transformed_data = {}\n",
    "    \n",
    "    for func_name in transform_funcs:\n",
    "        # logger.info(f\"--- 开始应用变换函数: {func_name} ---\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        transform_info = TRANSFORM_REGISTRY[func_name]\n",
    "        func = transform_info['func']\n",
    "        output_mode_names = transform_info['output_mode_names']\n",
    "        \n",
    "        # 执行变换\n",
    "        transformed_results = _apply_transform_func(func, X_df)\n",
    "        \n",
    "        # 存储结果\n",
    "        for mode_name, mode_df in zip(output_mode_names, transformed_results):\n",
    "            transformed_data[mode_name] = mode_df\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        # logger.info(f\"'{func_name}' 变换完毕，耗时: {duration:.2f} 秒，生成模态: {output_mode_names}\")\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "def clean_feature_names(df: pd.DataFrame, prefix: str = \"f\") -> pd.DataFrame:\n",
    "    \"\"\"清理特征名称，确保它们是合法的列名。\"\"\"\n",
    "    cleaned_columns = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        # 替换非法字符为 _\n",
    "        cleaned = re.sub(r'[^\\w]', '_', col)\n",
    "        # 防止开头是数字（如 \"123_feature\"）非法\n",
    "        if re.match(r'^\\d', cleaned):\n",
    "            cleaned = f\"{prefix}_{cleaned}\"\n",
    "        # 多个连续 _ 合并为一个\n",
    "        cleaned = re.sub(r'__+', '_', cleaned)\n",
    "        cleaned_columns.append(cleaned)\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a22aa431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 特征管理核心逻辑 ---\n",
    "def _get_latest_feature_file() -> Path | None:\n",
    "    \"\"\"查找并返回最新的特征文件路径\"\"\"\n",
    "    # 获取特征文件目录下的所有特征文件\n",
    "    feature_files = list(config.FEATURE_DIR.glob('features_*.parquet'))\n",
    "    # 如果没有特征文件，返回None\n",
    "    if not feature_files:\n",
    "        return None\n",
    "    return max(feature_files, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "def _load_feature_file(file_path: Path):\n",
    "    \"\"\"加载指定的特征文件及其元数据。\"\"\"\n",
    "    if not file_path or not file_path.exists():\n",
    "        return pd.DataFrame(), {}\n",
    "    try:\n",
    "        table = pd.read_parquet(file_path)\n",
    "        metadata_str = table.attrs.get('feature_metadata', '{}')\n",
    "        metadata = json.loads(metadata_str)\n",
    "        return table, metadata\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"无法加载特征文件 {file_path}: {e}。\")\n",
    "        return pd.DataFrame(), {}\n",
    "\n",
    "def _load_feature_dict_file(file_path: Path):\n",
    "    \"\"\"加载字典格式的特征文件及其元数据。\"\"\"\n",
    "    if not file_path or not file_path.exists():\n",
    "        return {}, {}\n",
    "    try:\n",
    "        # 加载主文件获取元数据\n",
    "        main_table = pd.read_parquet(file_path)\n",
    "        metadata_str = main_table.attrs.get('feature_metadata', '{}')\n",
    "        metadata = json.loads(metadata_str)\n",
    "        \n",
    "        # 加载字典格式的特征数据\n",
    "        feature_dict = {}\n",
    "        base_name = file_path.stem  # 去掉扩展名\n",
    "        \n",
    "        # 查找所有相关的特征文件\n",
    "        for data_id_file in file_path.parent.glob(f\"{base_name}_id_*.parquet\"):\n",
    "            # 从文件名提取数据ID\n",
    "            data_id = data_id_file.stem.split('_id_')[-1]\n",
    "            feature_dict[data_id] = pd.read_parquet(data_id_file)\n",
    "        \n",
    "        # 如果没有找到分离的文件，尝试从主文件加载（向后兼容）\n",
    "        if not feature_dict and not main_table.empty:\n",
    "            feature_dict[\"0\"] = main_table\n",
    "            \n",
    "        return feature_dict, metadata\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"无法加载字典格式特征文件 {file_path}: {e}。\")\n",
    "        return {}, {}\n",
    "\n",
    "def extract_raw_features(feat):\n",
    "    raw_parts = []\n",
    "    model_flags = ['RAW', 'CUMSUM', 'DIFF', 'ASINH']\n",
    "    \n",
    "    # 找到所有model_flag的位置\n",
    "    flag_positions = []\n",
    "    for flag in model_flags:\n",
    "        start = 0\n",
    "        while True:\n",
    "            pos = feat.find(f'_{flag}_', start)\n",
    "            if pos == -1:\n",
    "                break\n",
    "            flag_positions.append((pos + 1, flag))  # +1 to skip the leading underscore\n",
    "            start = pos + 1\n",
    "    \n",
    "    # 按位置排序\n",
    "    flag_positions.sort()\n",
    "    \n",
    "    # 根据位置切分特征\n",
    "    for i, (pos, flag) in enumerate(flag_positions):\n",
    "        # 确定当前特征的结束位置\n",
    "        if i + 1 < len(flag_positions):\n",
    "            end_pos = flag_positions[i + 1][0] - 1  # -1 to exclude the underscore\n",
    "            raw_part = feat[pos:end_pos]\n",
    "        else:\n",
    "            raw_part = feat[pos:]\n",
    "        raw_parts.append(raw_part)\n",
    "    \n",
    "    # print(raw_parts)\n",
    "    return raw_parts\n",
    "\n",
    "def extract_trans_funcs_dict(\n",
    "        trans_mode_to_run: list = None, \n",
    "    ):\n",
    "    if trans_mode_to_run is None:\n",
    "        trans_mode_to_run = []\n",
    "        for func_name in TRANSFORM_REGISTRY.keys():\n",
    "            trans_mode_to_run.extend(TRANSFORM_REGISTRY[func_name][\"output_mode_names\"])\n",
    "        print(trans_mode_to_run)\n",
    "\n",
    "    # 1. 提取trans-funcs对\n",
    "    trans_funcs_dict = {}\n",
    "    raw_feat_name = []\n",
    "    operator_flags = ['mul', 'sqmul', 'sub', 'add', 'div', 'sq', 'cross_mul']\n",
    "    for feat in config.REMAIN_FEATURES:\n",
    "        matched_flag = next((flag for flag in operator_flags if feat.startswith(flag)), None)\n",
    "        if matched_flag is not None:\n",
    "            raw_parts = extract_raw_features(feat)\n",
    "            raw_feat_name.extend(raw_parts)\n",
    "        else:\n",
    "            raw_feat_name.append(feat)\n",
    "\n",
    "    for feat in raw_feat_name:\n",
    "        parts = feat.split('_')\n",
    "        trans_mode, func_mode = parts[0], parts[1]\n",
    "        if trans_mode in trans_mode_to_run:\n",
    "            trans_funcs_dict.setdefault(trans_mode, set()).add(func_mode)\n",
    "    trans_funcs_dict = {k: sorted(list(v)) for k, v in trans_funcs_dict.items()}\n",
    "    logger.warning(f'变换-特征匹配: {trans_funcs_dict}')\n",
    "    return trans_funcs_dict\n",
    "\n",
    "def generate_features(\n",
    "        X_data, \n",
    "        funcs_to_run: list = None, \n",
    "        trans_to_run: list = None, \n",
    "        base_feature_file: str = None,\n",
    "        use_tqdm: bool = False,\n",
    "        parallel: bool = False,\n",
    "        trans_funcs_dict: dict = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    生成指定的特征，或者如果未指定，则生成所有已注册的特征。\n",
    "    可以基于一个现有的特征文件进行增量更新。\n",
    "    现在支持字典格式的输入数据和特征存储。\n",
    "\n",
    "    Args:\n",
    "        X_data: 输入数据，可以是:\n",
    "            - pd.DataFrame: 单个数据框（向后兼容）\n",
    "            - dict: 字典格式，键为数据ID（\"0\"表示原始数据，\"1\"、\"2\"等表示增强数据），值为对应的数据框\n",
    "        funcs_to_run (list, optional): 要运行的特征函数名称列表。\n",
    "            如果为 None，则运行所有在 `FEATURE_REGISTRY` 中注册的、且不在 `EXPERIMENTAL_FEATURES` 中的函数。\n",
    "        trans_to_run (list, optional): 要运行的变换函数名称列表。\n",
    "        base_feature_file (str, optional): 基础特征文件名。如果提供，\n",
    "            将加载此文件并在此基础上添加或更新特征。否则，将创建一个新的特征集。\n",
    "    \"\"\"\n",
    "    # utils.ensure_feature_dirs()\n",
    "    \n",
    "    # 处理输入数据格式\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        # 向后兼容：单个数据框转换为字典格式\n",
    "        X_data_dict = {\"0\": X_data}\n",
    "        logger.info(\"输入为单个数据框，已转换为字典格式（数据ID: '0'）\")\n",
    "    elif isinstance(X_data, dict):\n",
    "        X_data_dict = X_data\n",
    "        logger.info(f\"输入为字典格式，包含数据ID: {list(X_data_dict.keys())}\")\n",
    "    else:\n",
    "        raise ValueError(\"X_data必须是pd.DataFrame或dict类型\")\n",
    "    \n",
    "    if funcs_to_run is None:\n",
    "        # 如果未指定函数，则运行所有非实验性特征\n",
    "        funcs_to_run = [\n",
    "            f for f in FEATURE_REGISTRY.keys() \n",
    "            if f not in config.EXPERIMENTAL_FEATURES\n",
    "        ]\n",
    "        logger.info(f\"未指定特征函数，将运行所有 {len(funcs_to_run)} 个非实验性特征。\")\n",
    "    \n",
    "    # 验证请求的函数是否都已注册\n",
    "    valid_funcs_to_run = []\n",
    "    for func_name in funcs_to_run:\n",
    "        if func_name not in FEATURE_REGISTRY:\n",
    "            logger.warning(f\"函数 {func_name} 未在注册表中找到，已跳过。\")\n",
    "        else:\n",
    "            valid_funcs_to_run.append(func_name)\n",
    "    \n",
    "    funcs_to_run = valid_funcs_to_run\n",
    "\n",
    "    # 1. 确定基础特征文件\n",
    "    if base_feature_file:\n",
    "        base_path = config.FEATURE_DIR / base_feature_file\n",
    "    else:\n",
    "        base_path = _get_latest_feature_file()\n",
    "\n",
    "    # 2. 加载基础特征（字典格式）\n",
    "    if base_path and base_path.exists():\n",
    "        logger.info(f\"将基于特征文件进行更新: {base_path.name}\")\n",
    "        feature_dict, metadata = _load_feature_dict_file(base_path)\n",
    "        # 如果加载失败，尝试旧格式\n",
    "        if not feature_dict:\n",
    "            logger.info(\"尝试加载旧格式特征文件...\")\n",
    "            old_feature_df, metadata = _load_feature_file(base_path)\n",
    "            if not old_feature_df.empty:\n",
    "                feature_dict = {\"0\": old_feature_df}\n",
    "    else:\n",
    "        logger.info(\"未找到基础特征文件，将创建全新的特征集。\")\n",
    "        feature_dict, metadata = {}, {}\n",
    "    \n",
    "    # 确保每个数据ID都有对应的特征DataFrame\n",
    "    for data_id in X_data_dict.keys():\n",
    "        if data_id not in feature_dict:\n",
    "            # 获取该数据ID的唯一ID列表\n",
    "            unique_ids = X_data_dict[data_id].index.get_level_values('id').unique()\n",
    "            feature_dict[data_id] = pd.DataFrame(index=unique_ids)\n",
    "            logger.info(f\"为数据ID '{data_id}' 创建新的特征DataFrame，包含 {len(unique_ids)} 个样本\")\n",
    "    \n",
    "    logger.info(f\"基础特征字典包含数据ID: {list(feature_dict.keys())}\")\n",
    "    for data_id, df in feature_dict.items():\n",
    "        logger.info(f\"  数据ID '{data_id}': {df.shape}\")\n",
    "\n",
    "    if trans_funcs_dict is None:\n",
    "        trans_funcs_dict = extract_trans_funcs_dict()\n",
    "\n",
    "    # 3. 为每个数据ID生成特征\n",
    "    initial_feature_counts = {data_id: len(df.columns) for data_id, df in feature_dict.items()}\n",
    "    \n",
    "    for data_id, X_df in X_data_dict.items():\n",
    "        logger.info(f\"=== 开始为数据ID '{data_id}' 生成特征 ===\")\n",
    "        \n",
    "        # 时序分解\n",
    "        logger.info(f\"--- 开始时序分解（数据ID: {data_id}） ---\")\n",
    "        transformed_data = apply_transformation(X_df, trans_to_run)\n",
    "        logger.info(f\"分解完成，共生成 {len(transformed_data)} 个模态: {list(transformed_data.keys())}\")\n",
    "        \n",
    "        # 获取当前数据ID的特征DataFrame\n",
    "        current_feature_df = feature_dict[data_id]\n",
    "        loaded_features = current_feature_df.columns.tolist()\n",
    "        \n",
    "        # 逐个生成新特征并更新\n",
    "        for mode_name, mode_df in transformed_data.items():\n",
    "            logger.info(f\"=== 开始为数据ID '{data_id}' 的模态 '{mode_name}' 生成特征 ===\")\n",
    "            for func_name in funcs_to_run:\n",
    "                logger.info(f\"--- 开始生成特征: {func_name} ---\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                feature_info = FEATURE_REGISTRY[func_name]\n",
    "                func = feature_info['func']\n",
    "                is_parallelizable = feature_info['parallelizable']\n",
    "                func_id = feature_info['func_id']\n",
    "                if func_id not in trans_funcs_dict[mode_name]:\n",
    "                    logger.info(f\"函数 '{func_name}' 已跳过。\")\n",
    "                    continue\n",
    "                \n",
    "                if is_parallelizable and parallel:\n",
    "                    new_features_df = _apply_feature_func_parallel(func, mode_df, use_tqdm)\n",
    "                else:\n",
    "                    logger.info(f\"函数 '{func_name}' 不可并行化，将顺序执行。\")\n",
    "                    new_features_df = _apply_feature_func_sequential(func, mode_df, use_tqdm)\n",
    "                new_features_df.columns = [f\"{mode_name}_{func_id}_{col}\" for col in new_features_df.columns]\n",
    "                new_features_df = clean_feature_names(new_features_df)\n",
    "\n",
    "                # 记录日志\n",
    "                duration = time.time() - start_time\n",
    "                logger.info(f\"'{func_name}' 生成完毕，耗时: {duration:.2f} 秒。\")\n",
    "                logger.info(f\"  新生成特征列名: {new_features_df.columns.tolist()}\")\n",
    "                \n",
    "                for col in new_features_df.columns:\n",
    "                    null_ratio = new_features_df[col].isnull().sum() / len(new_features_df)\n",
    "                    zero_ratio = (new_features_df[col] == 0).sum() / len(new_features_df)\n",
    "                    logger.info(f\"    - '{col}': 空值比例={null_ratio:.2%}, 零值比例={zero_ratio:.2%}\")\n",
    "\n",
    "                # 删除旧版本特征（如果存在），然后合并\n",
    "                current_feature_df = current_feature_df.drop(columns=new_features_df.columns, errors='ignore')\n",
    "                current_feature_df = current_feature_df.merge(new_features_df, left_index=True, right_index=True, how='left')\n",
    "                loaded_features = current_feature_df.columns.tolist()\n",
    "        \n",
    "        # 更新特征字典\n",
    "        feature_dict[data_id] = current_feature_df\n",
    "        logger.info(f\"数据ID '{data_id}' 特征生成完成，最终特征数: {len(current_feature_df.columns)}\")\n",
    "\n",
    "    return feature_dict, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03461576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 特征交互核心逻辑 ---\n",
    "def extract_and_generate_interaction_features(\n",
    "        feature_dict: dict, \n",
    "    ):\n",
    "    \"\"\"\n",
    "    根据特征重要性文件生成交互特征。\n",
    "    支持字典格式的特征数据。\n",
    "\n",
    "    Args:\n",
    "        feature_dict (dict): 特征数据框。\n",
    "    \"\"\"\n",
    "    # 1. 提取交互对\n",
    "    raw_feat_name = []\n",
    "    interaction_pairs = {}\n",
    "    operator_flags = ['mul', 'sqmul', 'sub', 'add', 'div', 'sq', 'cross_mul']\n",
    "    for flag in operator_flags:\n",
    "        interaction_pairs[flag] = []\n",
    "    for feat in config.REMAIN_FEATURES:\n",
    "        matched_flag = next((flag for flag in operator_flags if feat.startswith(flag)), None)\n",
    "        if matched_flag is not None:\n",
    "            raw_parts = extract_raw_features(feat)\n",
    "            raw_feat_name.extend(raw_parts)\n",
    "            interaction_pairs[matched_flag].append(tuple(raw_parts))  # 转为元组\n",
    "        else:\n",
    "            raw_feat_name.append(feat)\n",
    "\n",
    "    # 2. 检查是否有特征缺失\n",
    "    for data_id, feature_df in feature_dict.items():\n",
    "        missing_features = [f for f in raw_feat_name if f not in feature_df.columns]\n",
    "        if missing_features:\n",
    "            logger.warning(f\"Missing 【RAW FEATURES】 in <{data_id}>: {missing_features}\")\n",
    "\n",
    "    # 3. 为每个数据ID生成交互特征\n",
    "    updated_feature_dict = {}\n",
    "    all_interaction_features = []\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for data_id, feature_df in feature_dict.items():\n",
    "        logger.info(f\"\\n为数据ID '{data_id}' 生成交互特征...\")\n",
    "        \n",
    "        # 创建交互特征 - 使用字典收集所有特征，避免DataFrame碎片化\n",
    "        interaction_features_dict = {}\n",
    "\n",
    "        # 根据提取的交互对进行高效特征交互\n",
    "        for operator, pairs in interaction_pairs.items():\n",
    "            for f1, f2 in pairs:\n",
    "                if operator == 'mul':\n",
    "                    interaction_features_dict[f'mul_{f1}_{f2}'] = feature_df[f1] * feature_df[f2]\n",
    "                elif operator == 'sqmul':\n",
    "                    interaction_features_dict[f'sqmul_{f1}_{f2}'] = feature_df[f1] * (feature_df[f2] ** 2)\n",
    "                elif operator == 'sub':\n",
    "                    interaction_features_dict[f'sub_{f1}_{f2}'] = feature_df[f1] - feature_df[f2]\n",
    "                elif operator == 'add':\n",
    "                    interaction_features_dict[f'add_{f1}_{f2}'] = feature_df[f1] + feature_df[f2]\n",
    "                elif operator == 'div':\n",
    "                    interaction_features_dict[f'div_{f1}_{f2}'] = feature_df[f1] / (feature_df[f2] + epsilon)\n",
    "                elif operator == 'sq':\n",
    "                    interaction_features_dict[f'sq_{f1}'] = feature_df[f1] ** 2\n",
    "                elif operator == 'cross_mul':\n",
    "                    interaction_features_dict[f'cross_mul_{f1}_{f2}'] = feature_df[f1] * feature_df[f2]\n",
    "                elif operator == 'cross_sqmul':\n",
    "                    interaction_features_dict[f'cross_sqmul_{f1}_{f2}'] = feature_df[f1] * (feature_df[f2] ** 2)\n",
    "                elif operator == 'cross_add':\n",
    "                    interaction_features_dict[f'cross_add_{f1}_{f2}'] = feature_df[f1] + feature_df[f2]\n",
    "                elif operator == 'cross_sub':\n",
    "                    interaction_features_dict[f'cross_sub_{f1}_{f2}'] = feature_df[f1] - feature_df[f2]\n",
    "                elif operator == 'cross_div':\n",
    "                    interaction_features_dict[f'cross_div_{f1}_{f2}'] = feature_df[f1] / (feature_df[f2] + epsilon)\n",
    "        \n",
    "        # 一次性创建DataFrame，避免碎片化\n",
    "        if interaction_features_dict:\n",
    "            interaction_features = pd.DataFrame(interaction_features_dict, index=feature_df.index)\n",
    "        else:\n",
    "            interaction_features = pd.DataFrame(index=feature_df.index)\n",
    "        \n",
    "        if interaction_features.empty:\n",
    "            logger.info(f\"数据ID '{data_id}' 没有选择任何交互项类型，跳过。\")\n",
    "            updated_feature_dict[data_id] = feature_df.copy()\n",
    "            continue\n",
    "        \n",
    "        logger.info(f\"  数据ID '{data_id}' 成功创建 {len(interaction_features.columns)} 个交互特征\")\n",
    "        \n",
    "        # 合并特征\n",
    "        updated_feature_df = feature_df.drop(columns=interaction_features.columns, errors='ignore')\n",
    "        updated_feature_df = updated_feature_df.merge(interaction_features, left_index=True, right_index=True, how='left')\n",
    "        updated_feature_df = clean_feature_names(updated_feature_df, prefix=\"f_inter\")\n",
    "        \n",
    "        updated_feature_dict[data_id] = updated_feature_df\n",
    "\n",
    "    return updated_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "020f6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_cv_splits(feature_df, y_train, data_ids, cv_params):\n",
    "    \"\"\"\n",
    "    创建增强数据的交叉验证分割策略。\n",
    "    \n",
    "    该函数确保：\n",
    "    1. 只使用原始数据（索引0-10000）创建CV分割\n",
    "    2. 验证集只包含原始数据\n",
    "    3. 训练集包含原始数据及其对应的增强数据\n",
    "    \n",
    "    Args:\n",
    "        feature_df: 包含所有数据（原始+增强）的特征DataFrame\n",
    "        y_train: 包含所有数据（原始+增强）的标签Series\n",
    "        data_ids: 数据增强ID列表，如[\"0\", \"1\", \"2\"]\n",
    "        cv_params: 交叉验证参数\n",
    "    \n",
    "    Returns:\n",
    "        generator: 生成器，每次yield (train_idx, val_idx)\n",
    "    \"\"\"\n",
    "    logger.info(\"创建增强数据交叉验证分割...\")\n",
    "    \n",
    "    # 1. 识别原始数据索引（0-10000）\n",
    "    original_indices = []\n",
    "    enhanced_indices = {}  # {original_id: [enhanced_id1, enhanced_id2, ...]}\n",
    "    \n",
    "    for idx in feature_df.index:\n",
    "        if idx <= 10000:  # 原始数据\n",
    "            original_indices.append(idx)\n",
    "            enhanced_indices[idx] = []\n",
    "        else:  # 增强数据\n",
    "            # 根据增强数据生成规律反推原始ID\n",
    "            # new_id = int(func_id) * 1000000 + i * 100000 + int(original_id)\n",
    "            original_id = idx % 100000  # 提取原始ID\n",
    "            if original_id in enhanced_indices:\n",
    "                enhanced_indices[original_id].append(idx)\n",
    "            else:\n",
    "                enhanced_indices[original_id] = [idx]\n",
    "    \n",
    "    original_indices = sorted(original_indices)\n",
    "    logger.info(f\"识别到 {len(original_indices)} 条原始数据\")\n",
    "    \n",
    "    # 统计增强数据\n",
    "    total_enhanced = sum(len(enhanced_list) for enhanced_list in enhanced_indices.values())\n",
    "    logger.info(f\"识别到 {total_enhanced} 条增强数据\")\n",
    "    \n",
    "    # 2. 使用原始数据创建CV分割\n",
    "    original_feature_df = feature_df.loc[original_indices]\n",
    "    original_y_train = y_train.loc[original_indices]\n",
    "    \n",
    "    skf = StratifiedKFold(**cv_params)\n",
    "    \n",
    "    # 3. 为每个fold生成训练集和验证集索引\n",
    "    for fold, (original_train_idx, original_val_idx) in enumerate(skf.split(original_feature_df, original_y_train)):\n",
    "        # 获取原始数据的实际索引\n",
    "        original_train_ids = [original_indices[i] for i in original_train_idx]\n",
    "        original_val_ids = [original_indices[i] for i in original_val_idx]\n",
    "        \n",
    "        # 验证集只包含原始数据\n",
    "        val_idx = original_val_ids\n",
    "        \n",
    "        # 训练集包含原始数据 + 对应的增强数据\n",
    "        train_idx = original_train_ids.copy()\n",
    "        \n",
    "        # 为训练集中的每个原始数据添加对应的增强数据\n",
    "        for original_id in original_train_ids:\n",
    "            if original_id in enhanced_indices:\n",
    "                train_idx.extend(enhanced_indices[original_id])\n",
    "        \n",
    "        # 转换为在feature_df中的位置索引\n",
    "        train_positions = [feature_df.index.get_loc(idx) for idx in train_idx if idx in feature_df.index]\n",
    "        val_positions = [feature_df.index.get_loc(idx) for idx in val_idx if idx in feature_df.index]\n",
    "        \n",
    "        logger.info(f\"Fold {fold+1}: 训练集 {len(train_positions)} 条 (原始: {len(original_train_ids)}, 增强: {len(train_positions)-len(original_train_ids)}), 验证集 {len(val_positions)} 条 (仅原始数据)\")\n",
    "        \n",
    "        yield train_positions, val_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3805e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        X_train: pd.DataFrame,\n",
    "        y_train: pd.Series,\n",
    "        model_directory_path: str,\n",
    "    ): \n",
    "    global logger, log_file_path\n",
    "    logger, log_file_path = get_logger('Train', Path(os.path.join(model_directory_path, 'train_logs')), verbose=False)\n",
    "    global config\n",
    "    config.PROJECT_ROOT = Path(model_directory_path)\n",
    "    config.FEATURE_DIR = config.PROJECT_ROOT / 'feature_dfs'\n",
    "    run_output_dir = Path(model_directory_path)\n",
    "    \n",
    "    # data.py\n",
    "    X_data = {}\n",
    "    y_data = {}\n",
    "    X_data[\"0\"] = X_train\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train = y_train.to_frame('structural_breakpoint')\n",
    "    y_data[\"0\"] = y_train\n",
    "\n",
    "    feature_dict, metadata = generate_features(X_data, use_tqdm=True, parallel=True)\n",
    "    feature_dict = extract_and_generate_interaction_features(feature_dict)\n",
    "    for data_id, feature_df in feature_dict.items():\n",
    "            missing_features = [f for f in config.REMAIN_FEATURES if f not in feature_df.columns]\n",
    "            if missing_features:\n",
    "                logger.warning(f\"Missing REMAIN_FEATURES in <{data_id}> before filter: {missing_features}\")\n",
    "\n",
    "    # 拼接特征数据\n",
    "    data_ids = list(feature_dict.keys())\n",
    "    feature_dfs = []\n",
    "    for data_id in data_ids:\n",
    "        df = feature_dict[data_id].copy()\n",
    "        feature_dfs.append(df)\n",
    "    if len(feature_dfs) == 1:\n",
    "        concatenated_df = feature_dfs[0]\n",
    "    else:\n",
    "        concatenated_df = pd.concat(feature_dfs, axis=0, ignore_index=False)\n",
    "    feature_df = concatenated_df[config.REMAIN_FEATURES]\n",
    "    logger.info(feature_df)\n",
    "    logger.info(\"--- 生成后完整特征列表 ---\")\n",
    "    logger.info(f\"{feature_df.columns.tolist()}\")\n",
    "    logger.info(\"-----------------------------\")\n",
    "    logger.info(f\"生成/更新完成。总特征数: {len(feature_df.columns)}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Starting training and evaluation pipeline...\")\n",
    "    logger.info(f\"Model Parameters: {json.dumps(config.LGBM_PARAMS, indent=4)}\")\n",
    "\n",
    "    # 1. 加载特征和标签\n",
    "    y_train = pd.concat(list(y_data.values()), axis=0, ignore_index=False)\n",
    "    # 确保对齐\n",
    "    common_index = feature_df.index.intersection(y_train.index)\n",
    "    feature_df = feature_df.loc[common_index]\n",
    "    y_train = y_train.loc[common_index]['structural_breakpoint'].astype(int)\n",
    "    logger.info(f\"训练数据已对齐. X shape: {feature_df.shape}, y shape: {y_train.shape}\")\n",
    "    \n",
    "    # # 2. \n",
    "    # 特征选择\n",
    "    if len(config.REMAIN_FEATURES) > 0:\n",
    "        feature_df = feature_df[config.REMAIN_FEATURES]\n",
    "    if feature_df is None:\n",
    "        logger.error(\"特征加载失败，训练中止。\")\n",
    "        return None, None\n",
    "\n",
    "    logger.info(f\"--- 使用的特征列表 (共 {len(feature_df.columns)} 个) ---\")\n",
    "    logger.info(feature_df.columns.tolist())\n",
    "    logger.info(\"-\" * min(50, len(str(feature_df.columns.tolist()))))\n",
    "    \n",
    "    # 3. 模型训练\n",
    "    if config.TRAIN_STRATEGY == 'cv':\n",
    "        # 3. 交叉验证\n",
    "        logger.info(\"Starting 5-fold cross-validation with enhanced data strategy...\")\n",
    "\n",
    "        oof_preds = np.zeros(len(feature_df[0:10001]))\n",
    "        models = []\n",
    "        feature_importances = pd.DataFrame(index=feature_df.columns)\n",
    "        permutation_results = pd.DataFrame(index=feature_df.columns)\n",
    "        fold_metrics = []\n",
    "        \n",
    "        # 使用增强数据交叉验证策略\n",
    "        cv_iterator = create_enhanced_cv_splits(feature_df, y_train, data_ids, config.CV_PARAMS)\n",
    "        for fold, (train_idx, val_idx) in enumerate(cv_iterator):\n",
    "            logger.info(f\"--- Fold {fold+1}/{config.CV_PARAMS['n_splits']} ---\")\n",
    "            fold_start_time = time.time()\n",
    "\n",
    "            X_train_fold, y_train_fold = feature_df.iloc[train_idx], y_train.iloc[train_idx]\n",
    "            X_val_fold, y_val_fold = feature_df.iloc[val_idx], y_train.iloc[val_idx]\n",
    "            logger.info(f\"训练数据: {X_train_fold.shape}, 验证数据: {X_val_fold.shape}\")\n",
    "\n",
    "            # 配置模型\n",
    "            if config.MODEL == 'LGB':\n",
    "                model = lgb.LGBMClassifier(**config.LGBM_PARAMS)\n",
    "                callbacks = []\n",
    "                if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0:\n",
    "                    callbacks.append(lgb.early_stopping(config.EARLY_STOPPING_ROUNDS, verbose=False))\n",
    "                model.fit(\n",
    "                    X_train_fold, y_train_fold,\n",
    "                    eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n",
    "                    eval_names=['train', 'valid'],\n",
    "                    eval_metric='auc',\n",
    "                    callbacks=callbacks\n",
    "                )\n",
    "                train_auc = model.best_score_['train']['auc']\n",
    "            elif config.MODEL == 'CAT':\n",
    "                model = cat.CatBoostClassifier(**config.CAT_PARAMS)\n",
    "                model.fit(\n",
    "                    X_train_fold, y_train_fold, \n",
    "                    eval_set=[(X_val_fold, y_val_fold)],\n",
    "                    early_stopping_rounds=(config.EARLY_STOPPING_ROUNDS if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0 else None),\n",
    "                    verbose=False\n",
    "                )\n",
    "                train_preds = model.predict_proba(X_train_fold)[:, 1]\n",
    "                # 确保y_train_fold是NumPy格式，兼容cuDF\n",
    "                y_train_fold_numpy = y_train_fold.to_numpy() if hasattr(y_train_fold, 'to_numpy') else y_train_fold\n",
    "                train_auc = roc_auc_score(y_train_fold_numpy, train_preds)\n",
    "            elif config.MODEL == 'XGB':\n",
    "                if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0:\n",
    "                    config.XGB_PARAMS['early_stopping_rounds'] = config.EARLY_STOPPING_ROUNDS\n",
    "                model = xgb.XGBClassifier(**config.XGB_PARAMS)\n",
    "                # 如果使用GPU且cudf可用，转换数据到GPU\n",
    "                if CUDF_AVAILABLE and config.XGB_PARAMS.get('device') == 'cuda':\n",
    "                    logger.info(f\"Fold {fold+1}: Using cuDF for GPU data processing\")\n",
    "                    X_train_fold_gpu = cudf.DataFrame(X_train_fold)\n",
    "                    y_train_fold_gpu = cudf.Series(y_train_fold)\n",
    "                    X_val_fold_gpu = cudf.DataFrame(X_val_fold)\n",
    "                    y_val_fold_gpu = cudf.Series(y_val_fold)\n",
    "                    model.fit(\n",
    "                        X_train_fold_gpu, y_train_fold_gpu, \n",
    "                        eval_set=[(X_train_fold_gpu, y_train_fold_gpu), (X_val_fold_gpu, y_val_fold_gpu)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "                else:\n",
    "                    model.fit(\n",
    "                        X_train_fold, y_train_fold, \n",
    "                        eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n",
    "                        verbose=False\n",
    "                    )\n",
    "                train_preds = model.predict_proba(X_train_fold)[:, 1]\n",
    "                train_auc = roc_auc_score(y_train_fold, train_preds)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown config.MODEL\")\n",
    "\n",
    "            # 预测验证集\n",
    "            preds = model.predict_proba(X_val_fold)[:, 1]\n",
    "            if hasattr(preds, 'get'):\n",
    "                preds = preds.get()\n",
    "\n",
    "            oof_preds[val_idx] = preds\n",
    "            models.append(model)\n",
    "            feature_importances[f'fold_{fold+1}'] = model.feature_importances_\n",
    "            \n",
    "            fold_auc = roc_auc_score(y_val_fold, preds)\n",
    "            logger.warning(f\"Fold {fold+1} Train AUC: {train_auc:.5f}, Val AUC: {fold_auc:.5f}\")\n",
    "\n",
    "            # 记录早停的 step（best_iteration）\n",
    "            best_iteration = None\n",
    "            if config.MODEL == 'LGB':\n",
    "                best_iteration = getattr(model, 'best_iteration_', None)\n",
    "            elif config.MODEL == 'CAT':\n",
    "                try:\n",
    "                    best_iteration = model.get_best_iteration()\n",
    "                except Exception:\n",
    "                    best_iteration = getattr(model, 'best_iteration_', None)\n",
    "            elif config.MODEL == 'XGB':\n",
    "                best_iteration = getattr(model, 'best_iteration', None)\n",
    "            logger.info(f\"Fold {fold+1} Early stopping step (best_iteration): {best_iteration}\")\n",
    "\n",
    "            # 保存到元数据结构中\n",
    "            fold_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'train_auc': float(train_auc),\n",
    "                'val_auc': float(fold_auc),\n",
    "                'best_iteration': int(best_iteration) if best_iteration is not None else None,\n",
    "            })\n",
    "\n",
    "            fold_duration = time.time() - fold_start_time\n",
    "            logger.warning(f\"Fold {fold+1} finished in {fold_duration:.2f}s\")\n",
    "\n",
    "        overall_oof_auc = roc_auc_score(y_train[0:10001], oof_preds)\n",
    "        logger.warning(f\"Overall OOF AUC: {overall_oof_auc:.5f}\")\n",
    "\n",
    "        # 6. 保存模型\n",
    "        for i, model in tqdm(enumerate(models), total=len(models), desc=\"Saving models\"):\n",
    "            joblib.dump(model, run_output_dir / f'online_{config.MODEL}_model_fold_{i+1}.pkl')\n",
    "        logger.info(\"Models saved.\")\n",
    "    \n",
    "    else:\n",
    "        # 3. 单模型\n",
    "        logger.info(\"Starting single model training...\")\n",
    "        \n",
    "        # 配置模型\n",
    "        if config.MODEL == 'LGB':\n",
    "            model = lgb.LGBMClassifier(**config.LGBM_PARAMS)\n",
    "            callbacks = []\n",
    "            if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0:\n",
    "                callbacks.append(lgb.early_stopping(config.EARLY_STOPPING_ROUNDS, verbose=False))\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n",
    "                eval_names=['train', 'valid'],\n",
    "                eval_metric='auc',\n",
    "                callbacks=callbacks\n",
    "            )\n",
    "            train_auc = model.best_score_['train']['auc']\n",
    "        elif config.MODEL == 'CAT':\n",
    "            model = cat.CatBoostClassifier(**config.CAT_PARAMS)\n",
    "            model.fit(\n",
    "                X_train_fold, y_train_fold, \n",
    "                eval_set=[(X_val_fold, y_val_fold)],\n",
    "                early_stopping_rounds=(config.EARLY_STOPPING_ROUNDS if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0 else None),\n",
    "                verbose=False\n",
    "            )\n",
    "            train_preds = model.predict_proba(X_train_fold)[:, 1]\n",
    "            # 确保y_train_fold是NumPy格式，兼容cuDF\n",
    "            y_train_fold_numpy = y_train_fold.to_numpy() if hasattr(y_train_fold, 'to_numpy') else y_train_fold\n",
    "            train_auc = roc_auc_score(y_train_fold_numpy, train_preds)\n",
    "        elif config.MODEL == 'XGB':\n",
    "            if getattr(config, 'EARLY_STOPPING_ROUNDS', 0) and config.EARLY_STOPPING_ROUNDS > 0:\n",
    "                config.XGB_PARAMS['early_stopping_rounds'] = config.EARLY_STOPPING_ROUNDS\n",
    "            model = xgb.XGBClassifier(**config.XGB_PARAMS)\n",
    "            # 如果使用GPU且cudf可用，转换数据到GPU\n",
    "            if CUDF_AVAILABLE and config.XGB_PARAMS.get('device') == 'cuda':\n",
    "                logger.info(f\"Fold {fold+1}: Using cuDF for GPU data processing\")\n",
    "                X_train_fold_gpu = cudf.DataFrame(X_train_fold)\n",
    "                y_train_fold_gpu = cudf.Series(y_train_fold)\n",
    "                X_val_fold_gpu = cudf.DataFrame(X_val_fold)\n",
    "                y_val_fold_gpu = cudf.Series(y_val_fold)\n",
    "                model.fit(\n",
    "                    X_train_fold_gpu, y_train_fold_gpu, \n",
    "                    eval_set=[(X_train_fold_gpu, y_train_fold_gpu), (X_val_fold_gpu, y_val_fold_gpu)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(\n",
    "                    X_train_fold, y_train_fold, \n",
    "                    eval_set=[(X_train_fold, y_train_fold), (X_val_fold, y_val_fold)],\n",
    "                    verbose=False\n",
    "                )\n",
    "            train_preds = model.predict_proba(X_train_fold)[:, 1]\n",
    "            train_auc = roc_auc_score(y_train_fold, train_preds)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown config.MODEL\")\n",
    "\n",
    "        logger.warning(f\"Train AUC: {train_auc:.5f}\")\n",
    "        \n",
    "        joblib.dump(model, run_output_dir / f'online_{config.MODEL}_model.pkl')\n",
    "        logger.info(\"Model saved.\")\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    logger.warning(f\"训练流程结束，总耗时: {duration:.2f} 秒。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fbad0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_directory_path):\n",
    "    \"\"\"Load all LightGBM model files saved with joblib and prepare them for ensemble\"\"\"\n",
    "    models = []\n",
    "    dirpath = Path(model_directory_path)\n",
    "    model_files = list(dirpath.glob('*.pkl'))\n",
    "    \n",
    "    if not model_files:\n",
    "        logger.warning(f\"Warning: No model files found under {model_directory_path}!\")\n",
    "        return models\n",
    "    logger.warning(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            logger.warning(f\"Loading model: {model_path}\")\n",
    "            model = joblib.load(model_path)\n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd2bf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_infer_parallel(\n",
    "        X_data,\n",
    "        funcs_to_run: list = None,\n",
    "        trans_to_run: list = None,\n",
    "        use_tqdm: bool = False,\n",
    "        trans_funcs_dict: dict = None,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    推理阶段：以“特征函数”为并行单元生成特征（一次只处理当前批的数据，通常只有一个样本）。\n",
    "    返回结构与 generate_features 保持一致：{data_id: feature_df}, metadata\n",
    "    \"\"\"\n",
    "    # 输入规范化\n",
    "    if isinstance(X_data, pd.DataFrame):\n",
    "        X_data_dict = {\"0\": X_data}\n",
    "    elif isinstance(X_data, dict):\n",
    "        X_data_dict = X_data\n",
    "    else:\n",
    "        raise ValueError(\"X_data必须是pd.DataFrame或dict类型\")\n",
    "\n",
    "    # 选择要运行的特征函数（默认：跳过实验性函数）\n",
    "    if funcs_to_run is None:\n",
    "        funcs_to_run = [\n",
    "            f for f in FEATURE_REGISTRY.keys()\n",
    "            if f not in config.EXPERIMENTAL_FEATURES\n",
    "        ]\n",
    "    valid_funcs_to_run = [f for f in funcs_to_run if f in FEATURE_REGISTRY]\n",
    "\n",
    "    # trans-funcs 对齐\n",
    "    if trans_funcs_dict is None:\n",
    "        trans_funcs_dict = extract_trans_funcs_dict()\n",
    "\n",
    "    feature_dict = {}\n",
    "    metadata = {}\n",
    "\n",
    "    for data_id, X_df in X_data_dict.items():\n",
    "        unique_ids = X_df.index.get_level_values('id').unique()\n",
    "        current_feature_df = pd.DataFrame(index=unique_ids)\n",
    "\n",
    "        # 先进行时序变换（按变换函数顺序执行，计算量主要在后续特征函数）\n",
    "        transformed_data = apply_transformation(X_df, trans_to_run)\n",
    "\n",
    "        # 按模态并行地“以特征函数为粒度”生成特征\n",
    "        for mode_name, mode_df in transformed_data.items():\n",
    "            allowed_func_ids = set(trans_funcs_dict.get(mode_name, []))\n",
    "            funcs_for_mode = [\n",
    "                fname for fname in valid_funcs_to_run\n",
    "                if FEATURE_REGISTRY[fname]['func_id'] in allowed_func_ids\n",
    "            ]\n",
    "            if not funcs_for_mode:\n",
    "                continue\n",
    "\n",
    "            from joblib import Parallel, delayed\n",
    "\n",
    "            def run_single_feature(func_name):\n",
    "                feature_info = FEATURE_REGISTRY[func_name]\n",
    "                func = feature_info['func']\n",
    "                func_id = feature_info['func_id']\n",
    "                try:\n",
    "                    # 逐 id 顺序计算，避免在小样本上产生额外进程/序列化开销\n",
    "                    df_res = _apply_feature_func_sequential(func, mode_df, use_tqdm=False)\n",
    "                    df_res.columns = [f\"{mode_name}_{func_id}_{col}\" for col in df_res.columns]\n",
    "                    df_res = clean_feature_names(df_res)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"特征函数 {func_name} 失败: {e}\")\n",
    "                    df_res = pd.DataFrame(index=mode_df.index.get_level_values('id').unique())\n",
    "                return df_res\n",
    "\n",
    "            new_feature_dfs = Parallel(n_jobs=config.N_JOBS, prefer=\"threads\")(\n",
    "                delayed(run_single_feature)(fname) for fname in funcs_for_mode\n",
    "            )\n",
    "\n",
    "            if len(new_feature_dfs) > 0:\n",
    "                merged_mode_df = pd.concat(new_feature_dfs, axis=1)\n",
    "                current_feature_df = current_feature_df.drop(columns=merged_mode_df.columns, errors='ignore')\n",
    "                current_feature_df = current_feature_df.merge(merged_mode_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "        feature_dict[data_id] = current_feature_df\n",
    "\n",
    "    return feature_dict, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dde3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(\n",
    "        X_test: typing.Iterable[pd.DataFrame],\n",
    "        model_directory_path: str,\n",
    "    ):\n",
    "    global logger, log_file_path\n",
    "    logger, log_file_path = get_logger('Inference', Path(os.path.join(model_directory_path, 'infer_logs')), verbose=False)\n",
    "    global config\n",
    "    config.PROJECT_ROOT = Path(model_directory_path)\n",
    "    config.FEATURE_DIR = config.PROJECT_ROOT / 'feature_dfs'\n",
    "    \n",
    "    # 加载模型\n",
    "    models = load_models(model_directory_path)\n",
    "    # 加载各变换应运行的函数映射\n",
    "    trans_funcs_dict = extract_trans_funcs_dict()\n",
    "\n",
    "    yield  # Ready\n",
    "\n",
    "    # X_test 只能迭代一次；拿到一条就立刻算、立刻推理\n",
    "    for X_df in tqdm(X_test, desc=\"Inference Progress\"):\n",
    "        X_data = {\"0\": X_df}\n",
    "\n",
    "        feature_dict, metadata = generate_features_infer_parallel(\n",
    "            X_data, use_tqdm=False, trans_funcs_dict=trans_funcs_dict\n",
    "        )\n",
    "        feature_dict = extract_and_generate_interaction_features(feature_dict)\n",
    "        for data_id, feature_df in feature_dict.items():\n",
    "            missing_features = [f for f in config.REMAIN_FEATURES if f not in feature_df.columns]\n",
    "            if missing_features:\n",
    "                logger.warning(f\"Missing REMAIN_FEATURES in <{data_id}> before filter: {missing_features}\")\n",
    "\n",
    "        # 拼接特征数据\n",
    "        data_ids = list(feature_dict.keys())\n",
    "        feature_dfs = [feature_dict[data_id].copy() for data_id in data_ids]\n",
    "        concatenated_df = feature_dfs[0] if len(feature_dfs) == 1 else pd.concat(feature_dfs, axis=0, ignore_index=False)\n",
    "        feature_df = concatenated_df[config.REMAIN_FEATURES]\n",
    "        logger.info(feature_df)\n",
    "        logger.info(\"--- 生成后完整特征列表 ---\")\n",
    "        logger.info(f\"{feature_df.columns.tolist()}\")\n",
    "        logger.info(\"-----------------------------\")\n",
    "        logger.info(f\"生成/更新完成。总特征数: {len(feature_df.columns)}\")\n",
    "\n",
    "        def ensemble_predict(models, X):\n",
    "            preds = [model.predict_proba(X)[:, 1] for model in models]\n",
    "            if len(preds) == 0:\n",
    "                logger.warning(\"No predictions generated, returning zeros.\")\n",
    "                return np.zeros(len(X))\n",
    "            return np.mean(preds, axis=0)\n",
    "\n",
    "        prediction = ensemble_predict(models, feature_df)\n",
    "        prediction = 1 - prediction\n",
    "        yield prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "196f3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m09:50:40\u001b[0m \u001b[33mno forbidden library found\u001b[0m\n",
      "\u001b[32m09:50:40\u001b[0m \u001b[33m\u001b[0m\n",
      "\u001b[32m09:50:43\u001b[0m started\n",
      "\u001b[32m09:50:43\u001b[0m running local test\n",
      "\u001b[32m09:50:43\u001b[0m \u001b[33minternet access isn't restricted, no check will be done\u001b[0m\n",
      "\u001b[32m09:50:43\u001b[0m \n",
      "\u001b[32m09:50:45\u001b[0m starting unstructured loop...\n",
      "\u001b[32m09:50:45\u001b[0m executing - command=infer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\X_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_train.parquet (204327238 bytes)\n",
      "data\\X_train.parquet: already exists, file length match\n",
      "data\\X_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/X_test.reduced.parquet (2380918 bytes)\n",
      "data\\X_test.reduced.parquet: already exists, file length match\n",
      "data\\y_train.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_train.parquet (61003 bytes)\n",
      "data\\y_train.parquet: already exists, file length match\n",
      "data\\y_test.reduced.parquet: download from https:crunchdao--competition--production.s3-accelerate.amazonaws.com/data-releases/146/y_test.reduced.parquet (2655 bytes)\n",
      "data\\y_test.reduced.parquet: already exists, file length match\n",
      "WARNING: Found a total of 5 model files.\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_5.pkl\n",
      "['RAW', 'CUMSUM', 'DIFF', 'ASINH']\n",
      "WARNING: 变换-特征匹配: {'RAW': ['1', '10', '2', '4', '7', '8'], 'ASINH': ['1', '8'], 'CUMSUM': ['1', '2', '3', '8'], 'DIFF': ['2', '7', '8']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 101it [05:16,  3.13s/it]\n",
      "\u001b[32m09:56:02\u001b[0m checking determinism by executing the inference again with 30% of the data (tolerance: 1e-08)\n",
      "\u001b[32m09:56:02\u001b[0m executing - command=infer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Found a total of 5 model files.\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_1.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_2.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_3.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_4.pkl\n",
      "WARNING: Loading model: resources\\local_LGB_model_fold_5.pkl\n",
      "['RAW', 'CUMSUM', 'DIFF', 'ASINH']\n",
      "WARNING: 变换-特征匹配: {'RAW': ['1', '10', '2', '4', '7', '8'], 'ASINH': ['1', '8'], 'CUMSUM': ['1', '2', '3', '8'], 'DIFF': ['2', '7', '8']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress: 30it [00:40,  1.34s/it]\n",
      "\u001b[32m09:56:43\u001b[0m determinism check: passed\n",
      "\u001b[32m09:56:43\u001b[0m \u001b[33msave prediction - path=data\\prediction.parquet\u001b[0m\n",
      "\u001b[32m09:56:43\u001b[0m ended\n",
      "\u001b[32m09:56:43\u001b[0m \u001b[33mduration - time=00:06:00\u001b[0m\n",
      "\u001b[32m09:56:43\u001b[0m \u001b[33mmemory - before=\"346.2 MB\" after=\"423.31 MB\" consumed=\"77.11 MB\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "crunch.test(\n",
    "    # Uncomment to disable the train\n",
    "    force_first_train=False,\n",
    "\n",
    "    # Uncomment to disable the determinism check\n",
    "    # no_determinism_check=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "caf19e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.833076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.921978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>0.784366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>0.869120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.375159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10097</th>\n",
       "      <td>0.683086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10098</th>\n",
       "      <td>0.977537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10099</th>\n",
       "      <td>0.750794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>0.988075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>0.961705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction\n",
       "id               \n",
       "10001    0.833076\n",
       "10002    0.921978\n",
       "10003    0.784366\n",
       "10004    0.869120\n",
       "10005    0.375159\n",
       "...           ...\n",
       "10097    0.683086\n",
       "10098    0.977537\n",
       "10099    0.750794\n",
       "10100    0.988075\n",
       "10101    0.961705\n",
       "\n",
       "[101 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = pd.read_parquet(\"data/prediction.parquet\")\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0c087a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0863849765258216)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the targets\n",
    "target = pd.read_parquet(\"data/y_test.reduced.parquet\")[\"structural_breakpoint\"]\n",
    "\n",
    "# Call the scoring function\n",
    "sklearn.metrics.roc_auc_score(\n",
    "    target,\n",
    "    prediction,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
